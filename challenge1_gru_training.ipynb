{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# **Pirate Pain Challenge â€” GRU Training**\n",
        "\n",
        "Modello GRU monodirezionale per la classificazione dei soggetti `high_pain`, `low_pain`, `no_pain`, seguendo le analisi esplorative e le linee guida precedenti. Il notebook Ã¨ pensato per Colab con GPU disponibile.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ðŸŒ Google Drive Connection\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## âš™ï¸ Setup & Imports\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import random\n",
        "from dataclasses import dataclass\n",
        "from pathlib import Path\n",
        "from typing import Dict, List, Tuple\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader, Dataset, WeightedRandomSampler\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from sklearn.model_selection import train_test_split\n",
        "from statsmodels.tsa.stattools import acf\n",
        "from scipy.signal import find_peaks\n",
        "\n",
        "SEED = 42\n",
        "os.environ['PYTHONHASHSEED'] = str(SEED)\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "torch.cuda.manual_seed_all(SEED)\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Running on device: {device}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## â³ Data Loading\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "DATA_DIR = Path('/content/drive/MyDrive/[2025-2026] AN2DL/Challenge')\n",
        "\n",
        "\n",
        "train_features_path = DATA_DIR / 'pirate_pain_train.csv'\n",
        "train_labels_path = DATA_DIR / 'pirate_pain_train_labels.csv'\n",
        "test_features_path = DATA_DIR / 'pirate_pain_test.csv'\n",
        "\n",
        "print(train_features_path)\n",
        "\n",
        "X_train_raw = pd.read_csv(train_features_path)\n",
        "y_train = pd.read_csv(train_labels_path)\n",
        "X_test_raw = pd.read_csv(test_features_path)\n",
        "\n",
        "print('Train features:', X_train_raw.shape)\n",
        "print('Train labels:', y_train.shape)\n",
        "print('Test features:', X_test_raw.shape)\n",
        "print('Label distribution:')\n",
        "print(y_train['label'].value_counts())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ðŸ§¼ Preprocessing\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def coerce_numeric_counts(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    word_to_num = {\n",
        "        'zero': 0,\n",
        "        'one': 1,\n",
        "        'two': 2,\n",
        "    }\n",
        "    for col in ['n_legs', 'n_hands', 'n_eyes']:\n",
        "        if col in df.columns:\n",
        "            col_series = df[col].astype(str).str.lower()\n",
        "            numeric_part = pd.to_numeric(col_series, errors='coerce')\n",
        "            word_part = col_series.str.extract('(zero|one|two|three|four|five|six|seven|eight|nine)', expand=False)\n",
        "            word_numeric = word_part.map(word_to_num)\n",
        "            df[col] = numeric_part.fillna(word_numeric)\n",
        "    return df\n",
        "\n",
        "X_train_raw = coerce_numeric_counts(X_train_raw.copy())\n",
        "X_test_raw = coerce_numeric_counts(X_test_raw.copy())\n",
        "\n",
        "for df in (X_train_raw, X_test_raw):\n",
        "    object_cols = df.select_dtypes(include='object').columns\n",
        "    for col in object_cols:\n",
        "        df[col] = pd.to_numeric(df[col], errors='coerce')\n",
        "\n",
        "numeric_cols = X_train_raw.select_dtypes(include=np.number).columns\n",
        "print('Numeric feature count:', len(numeric_cols))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Autocorrelation & Window Selection\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "acf_features = [f for f in ['joint_10', 'joint_28', 'pain_survey_1', 'pain_survey_2'] if f in selected_features]\n",
        "max_lag = 80\n",
        "acf_results = {}\n",
        "first_peak_lags = []\n",
        "\n",
        "ordered_df = X_train_raw.sort_values(['sample_index', 'time'])\n",
        "\n",
        "for feat in acf_features:\n",
        "    series = ordered_df[feat].values\n",
        "    feature_acf = acf(series, nlags=max_lag, fft=True)\n",
        "    acf_results[feat] = feature_acf\n",
        "    peaks, properties = find_peaks(feature_acf[1:], height=0.1)\n",
        "    if len(peaks) > 0:\n",
        "        first_peak = peaks[0] + 1\n",
        "        first_peak_lags.append(first_peak)\n",
        "        print(f\"First significant ACF peak for {feat}: lag {first_peak} (height={properties['peak_heights'][0]:.3f})\")\n",
        "    else:\n",
        "        print(f\"No significant peaks found for {feat} (using default).\")\n",
        "\n",
        "if first_peak_lags:\n",
        "    WINDOW_LENGTH = int(np.median(first_peak_lags))\n",
        "else:\n",
        "    WINDOW_LENGTH = 60\n",
        "\n",
        "WINDOW_LENGTH = max(20, min(WINDOW_LENGTH, SEQUENCE_LENGTH))\n",
        "print(f\"Selected window length based on ACF: {WINDOW_LENGTH} timesteps\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "all_feature_cols = [col for col in X_train_raw.columns if col not in ['sample_index']]\n",
        "numeric_feature_cols = [col for col in all_feature_cols if np.issubdtype(X_train_raw[col].dtype, np.number)]\n",
        "\n",
        "zero_var_cols = [col for col in numeric_feature_cols if np.isclose(X_train_raw[col].var(), 0.0)]\n",
        "print(f'Dropping zero-variance features ({len(zero_var_cols)}):', zero_var_cols)\n",
        "\n",
        "agg_candidates = [col for col in numeric_feature_cols if col != 'time']\n",
        "sequence_summary = (\n",
        "    X_train_raw\n",
        "    .groupby('sample_index')[agg_candidates]\n",
        "    .agg(['mean'])\n",
        ")\n",
        "sequence_summary.columns = [f\"{col}_{stat}\" for col, stat in sequence_summary.columns]\n",
        "sequence_summary = sequence_summary.reset_index().merge(y_train[['sample_index', 'label']], on='sample_index', how='left')\n",
        "\n",
        "feature_mean_cols = [c for c in sequence_summary.columns if c.endswith('_mean') and c not in ['sample_index']]\n",
        "label_means = sequence_summary.groupby('label')[feature_mean_cols].mean(numeric_only=True)\n",
        "contrast = label_means.loc['high_pain'] - label_means.drop('high_pain').mean()\n",
        "\n",
        "TOP_K = 40\n",
        "important_stats = contrast.abs().sort_values(ascending=False).head(TOP_K).index\n",
        "important_features = sorted(set(name.rsplit('_', 1)[0] for name in important_stats))\n",
        "\n",
        "manual_keep = ['time']\n",
        "selected_features = [f for f in numeric_feature_cols if f in important_features or f in manual_keep]\n",
        "selected_features = [f for f in selected_features if f not in zero_var_cols]\n",
        "\n",
        "print(f'Selected {len(selected_features)} features (from {len(numeric_feature_cols)} total).')\n",
        "print('Key features:', selected_features[:20])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Sequence Assembly\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "FEATURES = selected_features\n",
        "TARGET_COLUMN = 'label'\n",
        "SEQUENCE_LENGTH = X_train_raw.groupby('sample_index').size().iloc[0]\n",
        "print('Sequence length (full):', SEQUENCE_LENGTH)\n",
        "print('Window length used:', WINDOW_LENGTH)\n",
        "print('Number of features (including time):', len(FEATURES))\n",
        "\n",
        "label_to_idx = {'no_pain': 0, 'low_pain': 1, 'high_pain': 2}\n",
        "y_train['target'] = y_train[TARGET_COLUMN].map(label_to_idx)\n",
        "\n",
        "assert not y_train['target'].isna().any(), 'Found unmapped labels.'\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def build_sequences(features_df: pd.DataFrame, label_df: pd.DataFrame | None = None,\n",
        "                    feature_columns: List[str] | None = None,\n",
        "                    sequence_length: int = SEQUENCE_LENGTH,\n",
        "                    window_length: int | None = None) -> Tuple[np.ndarray, np.ndarray | None, List[int]]:\n",
        "    feature_columns = feature_columns or FEATURES\n",
        "    sequences = []\n",
        "    labels = []\n",
        "    sample_ids = []\n",
        "\n",
        "    grouped = features_df.groupby('sample_index')\n",
        "    for sample_index, group in grouped:\n",
        "        group = group.sort_values('time')\n",
        "        values = group[feature_columns].values\n",
        "        if len(values) != sequence_length:\n",
        "            raise ValueError(f'Sequence {sample_index} has length {len(values)} != {sequence_length}')\n",
        "        if window_length is not None and window_length < sequence_length:\n",
        "            values = values[-window_length:]\n",
        "        sequences.append(values)\n",
        "        sample_ids.append(sample_index)\n",
        "        if label_df is not None:\n",
        "            target_value = label_df.loc[label_df['sample_index'] == sample_index, 'target'].iloc[0]\n",
        "            labels.append(target_value)\n",
        "\n",
        "    sequences = np.stack(sequences)\n",
        "    labels = np.array(labels) if labels else None\n",
        "    return sequences, labels, sample_ids\n",
        "\n",
        "X_sequences, y_sequences, sample_ids = build_sequences(X_train_raw, y_train, window_length=WINDOW_LENGTH)\n",
        "X_test_sequences, _, test_ids = build_sequences(X_test_raw, label_df=None, window_length=WINDOW_LENGTH)\n",
        "\n",
        "print('Sequences shape:', X_sequences.shape)\n",
        "print('Test sequences shape:', X_test_sequences.shape)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Train/Validation Split\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_idx, val_idx, y_train_split, y_val_split = train_test_split(\n",
        "    np.arange(len(X_sequences)),\n",
        "    y_sequences,\n",
        "    test_size=0.2,\n",
        "    random_state=SEED,\n",
        "    stratify=y_sequences,\n",
        ")\n",
        "\n",
        "X_train_seq = X_sequences[train_idx]\n",
        "y_train_seq = y_sequences[train_idx]\n",
        "X_val_seq = X_sequences[val_idx]\n",
        "y_val_seq = y_sequences[val_idx]\n",
        "\n",
        "print('Train sequences:', X_train_seq.shape)\n",
        "print('Validation sequences:', X_val_seq.shape)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Normalisation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_flat = X_train_seq.reshape(-1, X_train_seq.shape[-1])\n",
        "feature_mean = train_flat.mean(axis=0)\n",
        "feature_std = train_flat.std(axis=0) + 1e-8\n",
        "\n",
        "X_train_norm = (X_train_seq - feature_mean) / feature_std\n",
        "X_val_norm = (X_val_seq - feature_mean) / feature_std\n",
        "X_test_norm = (X_test_sequences - feature_mean) / feature_std\n",
        "\n",
        "print('Normalisation applied. Mean close to 0:', np.abs(X_train_norm.mean()).round(4))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ðŸ“¦ PyTorch Dataset & Dataloaders\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class SequenceDataset(Dataset):\n",
        "    def __init__(self, sequences: np.ndarray, labels: np.ndarray | None = None):\n",
        "        self.sequences = torch.tensor(sequences, dtype=torch.float32)\n",
        "        self.labels = torch.tensor(labels, dtype=torch.long) if labels is not None else None\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.sequences)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        x = self.sequences[idx]\n",
        "        if self.labels is None:\n",
        "            return x\n",
        "        y = self.labels[idx]\n",
        "        return x, y\n",
        "\n",
        "train_dataset = SequenceDataset(X_train_norm, y_train_seq)\n",
        "val_dataset = SequenceDataset(X_val_norm, y_val_seq)\n",
        "test_dataset = SequenceDataset(X_test_norm)\n",
        "\n",
        "class_counts = np.bincount(y_train_seq)\n",
        "class_weights = class_counts.sum() / (len(class_counts) * class_counts)\n",
        "print('Class counts:', class_counts)\n",
        "print('Class weights:', class_weights.round(3))\n",
        "\n",
        "sample_weights = class_weights[y_train_seq]\n",
        "weighted_sampler = WeightedRandomSampler(sample_weights, num_samples=len(sample_weights), replacement=True)\n",
        "\n",
        "BATCH_SIZE = 32\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, sampler=weighted_sampler, drop_last=False)\n",
        "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, drop_last=False)\n",
        "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, drop_last=False)\n",
        "\n",
        "input_features = X_train_norm.shape[-1]\n",
        "print('Input features for GRU:', input_features)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ðŸ§  GRU Model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class MonoGRU(nn.Module):\n",
        "    def __init__(self, input_size: int, hidden_size: int = 320, num_layers: int = 3, dropout: float = 0.35, num_classes: int = 3):\n",
        "        super().__init__()\n",
        "        self.gru = nn.GRU(\n",
        "            input_size=input_size,\n",
        "            hidden_size=hidden_size,\n",
        "            num_layers=num_layers,\n",
        "            dropout=dropout if num_layers > 1 else 0.0,\n",
        "            batch_first=True,\n",
        "            bidirectional=False,\n",
        "        )\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.fc = nn.Linear(hidden_size, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        gru_out, hidden = self.gru(x)\n",
        "        last_hidden = hidden[-1]\n",
        "        out = self.dropout(last_hidden)\n",
        "        logits = self.fc(out)\n",
        "        return logits\n",
        "\n",
        "model = MonoGRU(input_size=input_features, hidden_size=160, num_layers=2, dropout=0.3, num_classes=len(label_to_idx)).to(device)\n",
        "print(model)\n",
        "\n",
        "params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "print(f'Trainable parameters: {params/1e6:.2f}M')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ðŸ‹ï¸ Training Utilities\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "LABEL_SMOOTHING = 0.05\n",
        "\n",
        "@dataclass\n",
        "class TrainConfig:\n",
        "    epochs: int = 120\n",
        "    patience: int = 15\n",
        "    lr: float = 2e-3\n",
        "    weight_decay: float = 1e-4\n",
        "\n",
        "\n",
        "def compute_class_weights(labels: np.ndarray, base_weights: np.ndarray) -> torch.Tensor:\n",
        "    weights = torch.tensor(base_weights, dtype=torch.float32, device=device)\n",
        "    return weights\n",
        "\n",
        "\n",
        "def train_one_epoch(model: nn.Module, dataloader: DataLoader, criterion, optimizer) -> Tuple[float, float]:\n",
        "    model.train()\n",
        "    total_loss = 0.0\n",
        "    preds = []\n",
        "    targets = []\n",
        "\n",
        "    for batch in dataloader:\n",
        "        inputs, labels = batch\n",
        "        inputs = inputs.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad(set_to_none=True)\n",
        "        logits = model(inputs)\n",
        "        loss = criterion(logits, labels)\n",
        "        loss.backward()\n",
        "        nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item() * inputs.size(0)\n",
        "        preds.append(logits.argmax(dim=1).detach().cpu().numpy())\n",
        "        targets.append(labels.detach().cpu().numpy())\n",
        "\n",
        "    avg_loss = total_loss / len(dataloader.dataset)\n",
        "    preds = np.concatenate(preds)\n",
        "    targets = np.concatenate(targets)\n",
        "    macro_f1 = classification_report(targets, preds, output_dict=True)['macro avg']['f1-score']\n",
        "    return avg_loss, macro_f1\n",
        "\n",
        "\n",
        "def evaluate(model: nn.Module, dataloader: DataLoader, criterion) -> Tuple[float, float, np.ndarray, np.ndarray]:\n",
        "    model.eval()\n",
        "    total_loss = 0.0\n",
        "    preds = []\n",
        "    targets = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in dataloader:\n",
        "            inputs, labels = batch\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            logits = model(inputs)\n",
        "            loss = criterion(logits, labels)\n",
        "\n",
        "            total_loss += loss.item() * inputs.size(0)\n",
        "            preds.append(logits.argmax(dim=1).cpu().numpy())\n",
        "            targets.append(labels.cpu().numpy())\n",
        "\n",
        "    avg_loss = total_loss / len(dataloader.dataset)\n",
        "    preds = np.concatenate(preds)\n",
        "    targets = np.concatenate(targets)\n",
        "    macro_f1 = classification_report(targets, preds, output_dict=True)['macro avg']['f1-score']\n",
        "    return avg_loss, macro_f1, preds, targets\n",
        "\n",
        "\n",
        "def train_model(model: nn.Module, train_loader: DataLoader, val_loader: DataLoader, class_weights: torch.Tensor, config: TrainConfig):\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=config.lr, weight_decay=config.weight_decay)\n",
        "    criterion = nn.CrossEntropyLoss(weight=class_weights, label_smoothing=LABEL_SMOOTHING)\n",
        "\n",
        "    best_f1 = -np.inf\n",
        "    best_state = None\n",
        "    history = {'train_loss': [], 'val_loss': [], 'train_f1': [], 'val_f1': []}\n",
        "    patience_counter = 0\n",
        "\n",
        "    for epoch in range(1, config.epochs + 1):\n",
        "        train_loss, train_f1 = train_one_epoch(model, train_loader, criterion, optimizer)\n",
        "        val_loss, val_f1, _, _ = evaluate(model, val_loader, criterion)\n",
        "\n",
        "        history['train_loss'].append(train_loss)\n",
        "        history['val_loss'].append(val_loss)\n",
        "        history['train_f1'].append(train_f1)\n",
        "        history['val_f1'].append(val_f1)\n",
        "\n",
        "        print(f\"Epoch {epoch:03d} | Train Loss {train_loss:.4f} F1 {train_f1:.4f} || Val Loss {val_loss:.4f} F1 {val_f1:.4f}\")\n",
        "\n",
        "        if val_f1 > best_f1:\n",
        "            best_f1 = val_f1\n",
        "            best_state = model.state_dict()\n",
        "            patience_counter = 0\n",
        "        else:\n",
        "            patience_counter += 1\n",
        "            if patience_counter >= config.patience:\n",
        "                print('Early stopping triggered.')\n",
        "                break\n",
        "\n",
        "    if best_state is not None:\n",
        "        model.load_state_dict(best_state)\n",
        "\n",
        "    return history, best_f1\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ðŸš€ Training\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "config = TrainConfig(epochs=120, patience=20, lr=3e-4, weight_decay=1e-4)\n",
        "class_weight_tensor = compute_class_weights(y_train_seq, class_weights)\n",
        "\n",
        "history, best_val_f1 = train_model(model, train_loader, val_loader, class_weight_tensor, config)\n",
        "print(f'Best validation macro F1: {best_val_f1:.4f}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ðŸ“ˆ Validation Metrics\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "criterion = nn.CrossEntropyLoss(weight=class_weight_tensor, label_smoothing=LABEL_SMOOTHING)\n",
        "val_loss, val_f1, val_preds, val_targets = evaluate(model, val_loader, criterion)\n",
        "print(f'Validation loss: {val_loss:.4f} | macro F1: {val_f1:.4f}')\n",
        "\n",
        "idx_to_label = {v: k for k, v in label_to_idx.items()}\n",
        "print('\\nClassification report:')\n",
        "print(classification_report(val_targets, val_preds, target_names=[idx_to_label[i] for i in range(len(idx_to_label))]))\n",
        "\n",
        "cm = confusion_matrix(val_targets, val_preds)\n",
        "cm_df = pd.DataFrame(cm, index=[idx_to_label[i] for i in range(len(idx_to_label))], columns=[idx_to_label[i] for i in range(len(idx_to_label))])\n",
        "cm_df\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Loss & Macro-F1 Curves\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "fig, axes = plt.subplots(1, 2, figsize=(16, 5))\n",
        "axes[0].plot(history['train_loss'], label='Train')\n",
        "axes[0].plot(history['val_loss'], label='Validation')\n",
        "axes[0].set_title('Loss')\n",
        "axes[0].set_xlabel('Epoch')\n",
        "axes[0].legend()\n",
        "\n",
        "axes[1].plot(history['train_f1'], label='Train')\n",
        "axes[1].plot(history['val_f1'], label='Validation')\n",
        "axes[1].set_title('Macro F1')\n",
        "axes[1].set_xlabel('Epoch')\n",
        "axes[1].legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ðŸ’¾ Save Model & Artifacts\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "artifacts_dir = DATA_DIR / 'artifacts'\n",
        "artifacts_dir.mkdir(exist_ok=True)\n",
        "\n",
        "model_path = artifacts_dir / 'gru_monodirectional.pt'\n",
        "scaler_path = artifacts_dir / 'feature_normalisation.npz'\n",
        "label_map_path = artifacts_dir / 'label_mapping.json'\n",
        "\n",
        "torch.save({'state_dict': model.state_dict(), 'config': vars(config)}, model_path)\n",
        "np.savez(scaler_path, mean=feature_mean, std=feature_std, features=FEATURES)\n",
        "\n",
        "import json\n",
        "with open(label_map_path, 'w') as f:\n",
        "    json.dump({str(k): int(v) for k, v in label_to_idx.items()}, f)\n",
        "\n",
        "print('Saved model to:', model_path)\n",
        "print('Saved scaler to:', scaler_path)\n",
        "print('Saved label map to:', label_map_path)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ðŸ§ª Test Set Inference (Optional)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model.eval()\n",
        "test_preds = []\n",
        "with torch.no_grad():\n",
        "    for batch in test_loader:\n",
        "        inputs = batch.to(device)\n",
        "        logits = model(inputs)\n",
        "        test_preds.append(logits.argmax(dim=1).cpu().numpy())\n",
        "\n",
        "test_preds = np.concatenate(test_preds)\n",
        "submission = pd.DataFrame({\n",
        "    'sample_index': test_ids,\n",
        "    'label': [idx_to_label[int(idx)] for idx in test_preds]\n",
        "})\n",
        "submission_path = artifacts_dir / 'gru_predictions.csv'\n",
        "submission.to_csv(submission_path, index=False)\n",
        "print('Saved submission predictions to:', submission_path)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## âœ… Next Steps\n",
        "\n",
        "- Monitorare il recall della classe `high_pain`: se resta basso, valutare focal loss o oversampling mirato sui soggetti piÃ¹ difficili.\n",
        "- Introdurre augmentation temporale leggera (rumore gaussiano controllato, time warping moderato) per aumentare la varietÃ  senza distruggere i pattern utili.\n",
        "- Effettuare hyperparameter search su `hidden_size`, `num_layers` e `dropout` mantenendo il vincolo GRU monodirezionale.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## âœ… Next Steps\n",
        "\n",
        "- Monitor `high_pain` recall: se resta bassa, considerare focal loss o oversampling mirato sui soggetti con peggiore performance.\n",
        "- Integrare augmentation temporale (noise, time warping leggero) rispettando i pattern individuati nell'EDA.\n",
        "- Valutare fine-tuning di `hidden_size`, `num_layers` e `dropout` per migliorare la generalizzazione.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
