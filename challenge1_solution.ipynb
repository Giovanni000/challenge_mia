{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# üè¥‚Äç‚ò†Ô∏è AN2DL25 Challenge 1 ‚Äî Pirate Pain Classification\n",
        "\n",
        "This notebook implements a full deep-learning pipeline for multivariate time-series classification of the Pirate Pain dataset. It is inspired by the Lecture 4 notebook (`Timeseries Classification (1).ipynb`) but adapted for the competition setting, including data preparation, model training (RNN/GRU/LSTM variants), evaluation, and test-time inference.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# %%capture\n",
        "# !pip install -q -r requirements.txt\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import random\n",
        "import math\n",
        "import copy\n",
        "from pathlib import Path\n",
        "from typing import Tuple, Dict, Optional, List\n",
        "from datetime import datetime\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.metrics import (\n",
        "    f1_score,\n",
        "    classification_report,\n",
        "    confusion_matrix,\n",
        "    precision_recall_fscore_support,\n",
        ")\n",
        "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from torch.cuda.amp import autocast, GradScaler\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "SEED = 42\n",
        "\n",
        "try:\n",
        "    from google.colab import drive  # type: ignore\n",
        "    IN_COLAB = True\n",
        "except ImportError:  # pragma: no cover\n",
        "    IN_COLAB = False\n",
        "\n",
        "if IN_COLAB:\n",
        "    drive.mount('/content/drive', force_remount=False)\n",
        "    BASE_DIR = Path('/content/drive/MyDrive/[2025-2026]\\ A2NDL/Challenge')\n",
        "else:\n",
        "    BASE_DIR = Path('/Users/md101ta/Desktop/Pirates')\n",
        "\n",
        "DATA_DIR = (BASE_DIR / 'data').resolve()\n",
        "OUTPUT_DIR = (BASE_DIR / 'outputs').resolve()\n",
        "OUTPUT_DIR.mkdir(exist_ok=True, parents=True)\n",
        "\n",
        "# Reproducibility\n",
        "os.environ['PYTHONHASHSEED'] = str(SEED)\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(SEED)\n",
        "    DEVICE = torch.device('cuda')\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "else:\n",
        "    DEVICE = torch.device('cpu')\n",
        "\n",
        "print(f'Running in Colab: {IN_COLAB}')\n",
        "print(f'Device: {DEVICE}')\n",
        "print(f'Data dir: {DATA_DIR}')\n",
        "print(f'Output dir: {OUTPUT_DIR}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "LOG_DIR = (OUTPUT_DIR / 'logs').resolve()\n",
        "CHECKPOINT_DIR = (OUTPUT_DIR / 'checkpoints').resolve()\n",
        "LOG_DIR.mkdir(exist_ok=True, parents=True)\n",
        "CHECKPOINT_DIR.mkdir(exist_ok=True, parents=True)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_data(data_dir: Path) -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n",
        "    X_train = pd.read_csv(data_dir / 'pirate_pain_train.csv')\n",
        "    y_train = pd.read_csv(data_dir / 'pirate_pain_train_labels.csv')\n",
        "    X_test = pd.read_csv(data_dir / 'pirate_pain_test.csv')\n",
        "    return X_train, y_train, X_test\n",
        "\n",
        "\n",
        "X_train_raw, y_train, X_test_raw = load_data(DATA_DIR)\n",
        "print(X_train_raw.shape, y_train.shape, X_test_raw.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "CATEGORICAL_COLUMNS = ['n_legs', 'n_hands', 'n_eyes']\n",
        "CATEGORY_MAPPINGS: Dict[str, Dict[str, int]] = {}\n",
        "\n",
        "for col in CATEGORICAL_COLUMNS:\n",
        "    uniques = pd.concat([X_train_raw[col], X_test_raw[col]]).dropna().unique()\n",
        "    mapping = {value: idx for idx, value in enumerate(sorted(uniques))}\n",
        "    CATEGORY_MAPPINGS[col] = mapping\n",
        "    X_train_raw[col] = X_train_raw[col].map(mapping).astype(np.int32)\n",
        "    X_test_raw[col] = X_test_raw[col].map(mapping).astype(np.int32)\n",
        "\n",
        "FEATURE_COLUMNS = [col for col in X_train_raw.columns if col not in ['sample_index', 'time']]\n",
        "TIME_STEPS = X_train_raw['time'].nunique()\n",
        "NUM_FEATURES = len(FEATURE_COLUMNS)\n",
        "NUM_CLASSES = y_train['label'].nunique()\n",
        "print(f'Time steps: {TIME_STEPS} | Features: {NUM_FEATURES} | Classes: {NUM_CLASSES}')\n",
        "print('Category mappings:', CATEGORY_MAPPINGS)\n",
        "\n",
        "y_train['label'].value_counts().plot(kind='bar', title='Class distribution')\n",
        "plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "LABEL2IDX = {label: idx for idx, label in enumerate(sorted(y_train['label'].unique()))}\n",
        "IDX2LABEL = {idx: label for label, idx in LABEL2IDX.items()}\n",
        "print('Label mapping:', LABEL2IDX)\n",
        "\n",
        "\n",
        "def pivot_timeseries(df: pd.DataFrame) -> np.ndarray:\n",
        "    pivoted = (\n",
        "        df.pivot(index='sample_index', columns='time', values=FEATURE_COLUMNS)\n",
        "          .sort_index(axis=0)\n",
        "          .sort_index(axis=1, level=1)\n",
        "    )\n",
        "    data = pivoted.to_numpy().reshape(-1, TIME_STEPS, NUM_FEATURES)\n",
        "    return data\n",
        "\n",
        "\n",
        "X_train_np = pivot_timeseries(X_train_raw)\n",
        "X_test_np = pivot_timeseries(X_test_raw)\n",
        "y_train_idx = y_train.set_index('sample_index').loc[pd.unique(X_train_raw['sample_index'])]['label'].map(LABEL2IDX).to_numpy()\n",
        "\n",
        "print(X_train_np.shape, y_train_idx.shape, X_test_np.shape)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def compute_normalization_stats(data: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:\n",
        "    # data shape: (N, T, F)\n",
        "    mean = data.reshape(-1, NUM_FEATURES).mean(axis=0)\n",
        "    std = data.reshape(-1, NUM_FEATURES).std(axis=0) + 1e-6\n",
        "    return mean, std\n",
        "\n",
        "\n",
        "def normalize(data: np.ndarray, mean: np.ndarray, std: np.ndarray) -> np.ndarray:\n",
        "    return (data - mean) / std\n",
        "\n",
        "\n",
        "feat_mean, feat_std = compute_normalization_stats(X_train_np)\n",
        "X_train_np = normalize(X_train_np, feat_mean, feat_std)\n",
        "X_test_np = normalize(X_test_np, feat_mean, feat_std)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class TimeSeriesDataset(Dataset):\n",
        "    def __init__(self, data: np.ndarray, labels: Optional[np.ndarray] = None):\n",
        "        self.data = torch.tensor(data, dtype=torch.float32)\n",
        "        self.labels = None if labels is None else torch.tensor(labels, dtype=torch.long)\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        return self.data.shape[0]\n",
        "\n",
        "    def __getitem__(self, idx: int):\n",
        "        if self.labels is None:\n",
        "            return self.data[idx]\n",
        "        return self.data[idx], self.labels[idx]\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def create_dataloaders(X: np.ndarray, y: np.ndarray, valid_size: float = 0.2, batch_size: int = 64):\n",
        "    X_train, X_valid, y_train, y_valid = train_test_split(\n",
        "        X, y, test_size=valid_size, random_state=SEED, stratify=y\n",
        "    )\n",
        "\n",
        "    train_loader = DataLoader(TimeSeriesDataset(X_train, y_train), batch_size=batch_size, shuffle=True, drop_last=False)\n",
        "    valid_loader = DataLoader(TimeSeriesDataset(X_valid, y_valid), batch_size=batch_size, shuffle=False, drop_last=False)\n",
        "    return train_loader, valid_loader, (X_train, y_train, X_valid, y_valid)\n",
        "\n",
        "\n",
        "BATCH_SIZE = 64\n",
        "train_loader, valid_loader, (X_train_split, y_train_split, X_valid_split, y_valid_split) = create_dataloaders(X_train_np, y_train_idx, batch_size=BATCH_SIZE)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class RecurrentBackbone(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        input_size: int,\n",
        "        hidden_size: int = 128,\n",
        "        num_layers: int = 2,\n",
        "        dropout: float = 0.2,\n",
        "        rnn_type: str = 'lstm',\n",
        "        bidirectional: bool = True,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        rnn_cls = {\n",
        "            'rnn': nn.RNN,\n",
        "            'gru': nn.GRU,\n",
        "            'lstm': nn.LSTM,\n",
        "        }[rnn_type.lower()]\n",
        "        self.rnn_type = rnn_type.lower()\n",
        "        self.bidirectional = bidirectional\n",
        "        self.rnn = rnn_cls(\n",
        "            input_size=input_size,\n",
        "            hidden_size=hidden_size,\n",
        "            num_layers=num_layers,\n",
        "            dropout=dropout if num_layers > 1 else 0.0,\n",
        "            batch_first=True,\n",
        "            bidirectional=bidirectional,\n",
        "        )\n",
        "        proj_input = hidden_size * (2 if bidirectional else 1)\n",
        "        self.head = nn.Sequential(\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(proj_input, hidden_size),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(hidden_size, NUM_CLASSES),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out, _ = self.rnn(x)\n",
        "        # use last time-step hidden state\n",
        "        last = out[:, -1, :]\n",
        "        return self.head(last)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def compute_classification_metrics(preds: np.ndarray, targets: np.ndarray) -> Dict[str, float]:\n",
        "    accuracy = float((preds == targets).mean())\n",
        "    precision, recall, f1, _ = precision_recall_fscore_support(\n",
        "        targets,\n",
        "        preds,\n",
        "        average='macro',\n",
        "        zero_division=0,\n",
        "    )\n",
        "    return {\n",
        "        'accuracy': accuracy,\n",
        "        'precision': float(precision),\n",
        "        'recall': float(recall),\n",
        "        'f1': float(f1),\n",
        "    }\n",
        "\n",
        "\n",
        "def train_one_epoch(\n",
        "    model: nn.Module,\n",
        "    loader: DataLoader,\n",
        "    criterion: nn.Module,\n",
        "    optimizer: torch.optim.Optimizer,\n",
        "    scaler: GradScaler,\n",
        "    max_grad_norm: float = 5.0,\n",
        ") -> Tuple[float, Dict[str, float]]:\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    preds_all, targets_all = [], []\n",
        "\n",
        "    for inputs, targets in loader:\n",
        "        inputs = inputs.to(DEVICE)\n",
        "        targets = targets.to(DEVICE)\n",
        "        optimizer.zero_grad(set_to_none=True)\n",
        "\n",
        "        with autocast(device_type=DEVICE.type, enabled=(DEVICE.type == 'cuda')):\n",
        "            logits = model(inputs)\n",
        "            loss = criterion(logits, targets)\n",
        "\n",
        "        scaler.scale(loss).backward()\n",
        "        if max_grad_norm is not None:\n",
        "            scaler.unscale_(optimizer)\n",
        "            nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "\n",
        "        running_loss += loss.item() * inputs.size(0)\n",
        "        preds_all.append(torch.argmax(logits.detach(), dim=1).cpu())\n",
        "        targets_all.append(targets.cpu())\n",
        "\n",
        "    preds_np = torch.cat(preds_all).numpy()\n",
        "    targets_np = torch.cat(targets_all).numpy()\n",
        "    metrics = compute_classification_metrics(preds_np, targets_np)\n",
        "    avg_loss = running_loss / len(loader.dataset)\n",
        "    return avg_loss, metrics\n",
        "\n",
        "\n",
        "def evaluate_epoch(\n",
        "    model: nn.Module,\n",
        "    loader: DataLoader,\n",
        "    criterion: nn.Module,\n",
        ") -> Tuple[float, Dict[str, float], np.ndarray, np.ndarray]:\n",
        "    model.eval()\n",
        "    running_loss = 0.0\n",
        "    preds_all, targets_all = [], []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, targets in loader:\n",
        "            inputs = inputs.to(DEVICE)\n",
        "            targets = targets.to(DEVICE)\n",
        "            with autocast(device_type=DEVICE.type, enabled=(DEVICE.type == 'cuda')):\n",
        "                logits = model(inputs)\n",
        "                loss = criterion(logits, targets)\n",
        "\n",
        "            running_loss += loss.item() * inputs.size(0)\n",
        "            preds_all.append(torch.argmax(logits, dim=1).cpu())\n",
        "            targets_all.append(targets.cpu())\n",
        "\n",
        "    preds_np = torch.cat(preds_all).numpy()\n",
        "    targets_np = torch.cat(targets_all).numpy()\n",
        "    metrics = compute_classification_metrics(preds_np, targets_np)\n",
        "    avg_loss = running_loss / len(loader.dataset)\n",
        "    return avg_loss, metrics, preds_np, targets_np\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def fit_model(\n",
        "    config: Dict,\n",
        "    train_loader: DataLoader,\n",
        "    valid_loader: DataLoader,\n",
        "    run_name: str,\n",
        "    tensorboard: bool = True,\n",
        ") -> Tuple[nn.Module, Dict[str, List[float]], Dict]:\n",
        "    model = RecurrentBackbone(\n",
        "        input_size=NUM_FEATURES,\n",
        "        hidden_size=config['hidden_size'],\n",
        "        num_layers=config['num_layers'],\n",
        "        dropout=config['dropout'],\n",
        "        rnn_type=config['rnn_type'],\n",
        "        bidirectional=config.get('bidirectional', False),\n",
        "    ).to(DEVICE)\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = torch.optim.AdamW(\n",
        "        model.parameters(),\n",
        "        lr=config['lr'],\n",
        "        weight_decay=config.get('weight_decay', 0.0),\n",
        "    )\n",
        "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
        "        optimizer,\n",
        "        mode='max',\n",
        "        factor=config.get('scheduler_factor', 0.5),\n",
        "        patience=config.get('scheduler_patience', 3),\n",
        "    )\n",
        "    scaler = GradScaler(enabled=(DEVICE.type == 'cuda'))\n",
        "\n",
        "    history: Dict[str, List[float]] = {\n",
        "        'train_loss': [],\n",
        "        'train_accuracy': [],\n",
        "        'train_f1': [],\n",
        "        'train_precision': [],\n",
        "        'train_recall': [],\n",
        "        'valid_loss': [],\n",
        "        'valid_accuracy': [],\n",
        "        'valid_f1': [],\n",
        "        'valid_precision': [],\n",
        "        'valid_recall': [],\n",
        "        'lr': [],\n",
        "    }\n",
        "\n",
        "    run_log_dir = (LOG_DIR / run_name).resolve()\n",
        "    writer = SummaryWriter(run_log_dir.as_posix()) if tensorboard else None\n",
        "\n",
        "    best_metric = -np.inf\n",
        "    best_state: Optional[Dict] = None\n",
        "    patience = config.get('patience', 10)\n",
        "    patience_counter = 0\n",
        "    checkpoint_path = (CHECKPOINT_DIR / f'{run_name}.pt').resolve()\n",
        "\n",
        "    for epoch in range(1, config['epochs'] + 1):\n",
        "        train_loss, train_metrics = train_one_epoch(\n",
        "            model,\n",
        "            train_loader,\n",
        "            criterion,\n",
        "            optimizer,\n",
        "            scaler,\n",
        "            max_grad_norm=config.get('max_grad_norm', 5.0),\n",
        "        )\n",
        "        valid_loss, valid_metrics, preds, targets = evaluate_epoch(\n",
        "            model,\n",
        "            valid_loader,\n",
        "            criterion,\n",
        "        )\n",
        "        scheduler.step(valid_metrics['f1'])\n",
        "\n",
        "        current_lr = optimizer.param_groups[0]['lr']\n",
        "\n",
        "        history['train_loss'].append(train_loss)\n",
        "        history['train_accuracy'].append(train_metrics['accuracy'])\n",
        "        history['train_f1'].append(train_metrics['f1'])\n",
        "        history['train_precision'].append(train_metrics['precision'])\n",
        "        history['train_recall'].append(train_metrics['recall'])\n",
        "        history['valid_loss'].append(valid_loss)\n",
        "        history['valid_accuracy'].append(valid_metrics['accuracy'])\n",
        "        history['valid_f1'].append(valid_metrics['f1'])\n",
        "        history['valid_precision'].append(valid_metrics['precision'])\n",
        "        history['valid_recall'].append(valid_metrics['recall'])\n",
        "        history['lr'].append(current_lr)\n",
        "\n",
        "        if writer is not None:\n",
        "            writer.add_scalar('Loss/train', train_loss, epoch)\n",
        "            writer.add_scalar('Loss/valid', valid_loss, epoch)\n",
        "            writer.add_scalar('F1/train', train_metrics['f1'], epoch)\n",
        "            writer.add_scalar('F1/valid', valid_metrics['f1'], epoch)\n",
        "            writer.add_scalar('Accuracy/train', train_metrics['accuracy'], epoch)\n",
        "            writer.add_scalar('Accuracy/valid', valid_metrics['accuracy'], epoch)\n",
        "            writer.add_scalar('LearningRate', current_lr, epoch)\n",
        "\n",
        "        msg = (\n",
        "            f\"Epoch {epoch:03d} | \"\n",
        "            f\"train_loss={train_loss:.4f} acc={train_metrics['accuracy']:.3f} f1={train_metrics['f1']:.3f} | \"\n",
        "            f\"valid_loss={valid_loss:.4f} acc={valid_metrics['accuracy']:.3f} f1={valid_metrics['f1']:.3f} | \"\n",
        "            f\"lr={current_lr:.2e}\"\n",
        "        )\n",
        "        print(msg)\n",
        "\n",
        "        if valid_metrics['f1'] > best_metric + config.get('min_improvement', 0.0):\n",
        "            best_metric = valid_metrics['f1']\n",
        "            patience_counter = 0\n",
        "            best_state = {\n",
        "                'epoch': epoch,\n",
        "                'model_state': copy.deepcopy(model.state_dict()),\n",
        "                'optimizer_state': copy.deepcopy(optimizer.state_dict()),\n",
        "                'metrics': valid_metrics,\n",
        "                'train_metrics': train_metrics,\n",
        "                'preds': preds,\n",
        "                'targets': targets,\n",
        "            }\n",
        "            torch.save(best_state['model_state'], checkpoint_path)\n",
        "        else:\n",
        "            patience_counter += 1\n",
        "            if patience_counter >= patience:\n",
        "                print(f\"Early stopping triggered at epoch {epoch}. Best f1={best_metric:.4f}.\")\n",
        "                break\n",
        "\n",
        "    if writer is not None:\n",
        "        writer.close()\n",
        "\n",
        "    if best_state is None:\n",
        "        best_state = {\n",
        "            'epoch': config['epochs'],\n",
        "            'model_state': copy.deepcopy(model.state_dict()),\n",
        "            'optimizer_state': copy.deepcopy(optimizer.state_dict()),\n",
        "            'metrics': valid_metrics,\n",
        "            'train_metrics': train_metrics,\n",
        "            'preds': preds,\n",
        "            'targets': targets,\n",
        "        }\n",
        "        torch.save(best_state['model_state'], checkpoint_path)\n",
        "\n",
        "    model.load_state_dict(best_state['model_state'])\n",
        "    best_state.update(\n",
        "        {\n",
        "            'run_name': run_name,\n",
        "            'config': copy.deepcopy(config),\n",
        "            'history': history,\n",
        "            'checkpoint_path': checkpoint_path,\n",
        "            'best_f1': best_metric,\n",
        "        }\n",
        "    )\n",
        "    return model, history, best_state\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def prepare_config(name: str, overrides: Dict) -> Dict:\n",
        "    base_config = {\n",
        "        'run_name': name,\n",
        "        'rnn_type': 'lstm',\n",
        "        'hidden_size': 192,\n",
        "        'num_layers': 2,\n",
        "        'dropout': 0.4,\n",
        "        'bidirectional': True,\n",
        "        'lr': 2e-3,\n",
        "        'weight_decay': 1e-4,\n",
        "        'epochs': 80,\n",
        "        'batch_size': 128,\n",
        "        'valid_size': 0.2,\n",
        "        'patience': 10,\n",
        "        'max_grad_norm': 5.0,\n",
        "        'scheduler_factor': 0.5,\n",
        "        'scheduler_patience': 3,\n",
        "        'min_improvement': 1e-4,\n",
        "        'tensorboard': True,\n",
        "    }\n",
        "    config = copy.deepcopy(base_config)\n",
        "    config.update(overrides)\n",
        "    return config\n",
        "\n",
        "\n",
        "def run_experiment(config: Dict) -> Dict:\n",
        "    run_name = config.get('run_name') or f\"{config['rnn_type'].upper()}_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
        "    config = copy.deepcopy(config)\n",
        "    config['run_name'] = run_name\n",
        "\n",
        "    train_loader, valid_loader, (X_tr, y_tr, X_val, y_val) = create_dataloaders(\n",
        "        X_train_np,\n",
        "        y_train_idx,\n",
        "        valid_size=config.get('valid_size', 0.2),\n",
        "        batch_size=config['batch_size'],\n",
        "    )\n",
        "\n",
        "    model, history, best_state = fit_model(\n",
        "        config,\n",
        "        train_loader,\n",
        "        valid_loader,\n",
        "        run_name=run_name,\n",
        "        tensorboard=config.get('tensorboard', True),\n",
        "    )\n",
        "\n",
        "    best_state['data_split'] = {\n",
        "        'X_train': X_tr,\n",
        "        'y_train': y_tr,\n",
        "        'X_valid': X_val,\n",
        "        'y_valid': y_val,\n",
        "    }\n",
        "    best_state['model'] = model\n",
        "    best_state['history'] = history\n",
        "    return best_state\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "EXPERIMENT_CONFIGS = [\n",
        "    prepare_config('LSTM_BASE', {}),\n",
        "    prepare_config('GRU_BASE', {'rnn_type': 'gru', 'bidirectional': True}),\n",
        "]\n",
        "\n",
        "experiment_results: List[Dict] = []\n",
        "for cfg in EXPERIMENT_CONFIGS:\n",
        "    print(f\"\\n=== Running experiment: {cfg['run_name']} ({cfg['rnn_type'].upper()}) ===\")\n",
        "    result = run_experiment(cfg)\n",
        "    experiment_results.append(result)\n",
        "    print(\n",
        "        f\"Best validation F1: {result['best_f1']:.4f} at epoch {result['epoch']} | \"\n",
        "        f\"Accuracy: {result['metrics']['accuracy']:.4f}\"\n",
        "    )\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "summary_rows = []\n",
        "for res in experiment_results:\n",
        "    cfg = res['config']\n",
        "    metrics = res['metrics']\n",
        "    row = {\n",
        "        'run_name': res['run_name'],\n",
        "        'rnn_type': cfg['rnn_type'],\n",
        "        'bidirectional': cfg.get('bidirectional', False),\n",
        "        'hidden_size': cfg['hidden_size'],\n",
        "        'num_layers': cfg['num_layers'],\n",
        "        'dropout': cfg['dropout'],\n",
        "        'best_epoch': res['epoch'],\n",
        "        'best_f1': res['best_f1'],\n",
        "        'accuracy': metrics['accuracy'],\n",
        "        'precision': metrics['precision'],\n",
        "        'recall': metrics['recall'],\n",
        "        'checkpoint': str(res['checkpoint_path']),\n",
        "    }\n",
        "    summary_rows.append(row)\n",
        "\n",
        "summary_df = pd.DataFrame(summary_rows)\n",
        "summary_table = summary_df.sort_values(by='best_f1', ascending=False).reset_index(drop=True)\n",
        "summary_table\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if not experiment_results:\n",
        "    raise RuntimeError('No experiments have been executed. Run the experiment cell above first.')\n",
        "\n",
        "best_run = max(experiment_results, key=lambda x: x['best_f1'])\n",
        "best_model = best_run['model']\n",
        "best_history = best_run['history']\n",
        "print(\n",
        "    f\"Selected best run: {best_run['run_name']} | \"\n",
        "    f\"F1={best_run['best_f1']:.4f} | Accuracy={best_run['metrics']['accuracy']:.4f}\"\n",
        ")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def plot_history(history: Dict[str, List[float]], title: str = 'Learning Curves'):\n",
        "    epochs = range(1, len(history['train_loss']) + 1)\n",
        "    fig, axs = plt.subplots(1, 3, figsize=(18, 4))\n",
        "\n",
        "    axs[0].plot(epochs, history['train_loss'], label='Train')\n",
        "    axs[0].plot(epochs, history['valid_loss'], label='Valid')\n",
        "    axs[0].set_title('Loss')\n",
        "    axs[0].set_xlabel('Epoch')\n",
        "    axs[0].legend()\n",
        "\n",
        "    axs[1].plot(epochs, history['train_accuracy'], label='Train')\n",
        "    axs[1].plot(epochs, history['valid_accuracy'], label='Valid')\n",
        "    axs[1].set_title('Accuracy')\n",
        "    axs[1].set_xlabel('Epoch')\n",
        "    axs[1].legend()\n",
        "\n",
        "    axs[2].plot(epochs, history['train_f1'], label='Train F1')\n",
        "    axs[2].plot(epochs, history['valid_f1'], label='Valid F1')\n",
        "    axs[2].set_title('Macro F1')\n",
        "    axs[2].set_xlabel('Epoch')\n",
        "    axs[2].legend()\n",
        "\n",
        "    plt.suptitle(title)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "plot_history(best_history, title=f\"Learning Curves ‚Äî {best_run['run_name']}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "best_preds = best_run['preds']\n",
        "best_targets = best_run['targets']\n",
        "print(f\"Best validation macro F1: {best_run['best_f1']:.3f}\")\n",
        "print(\n",
        "    classification_report(\n",
        "        best_targets,\n",
        "        best_preds,\n",
        "        target_names=[IDX2LABEL[i] for i in range(NUM_CLASSES)],\n",
        "    )\n",
        ")\n",
        "\n",
        "cf = confusion_matrix(best_targets, best_preds)\n",
        "plt.figure(figsize=(6, 5))\n",
        "sns.heatmap(\n",
        "    cf,\n",
        "    annot=True,\n",
        "    fmt='d',\n",
        "    cmap='Blues',\n",
        "    xticklabels=[IDX2LABEL[i] for i in range(NUM_CLASSES)],\n",
        "    yticklabels=[IDX2LABEL[i] for i in range(NUM_CLASSES)],\n",
        ")\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('True')\n",
        "plt.title(f\"Validation Confusion Matrix ‚Äî {best_run['run_name']}\")\n",
        "plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "test_loader = DataLoader(TimeSeriesDataset(X_test_np), batch_size=256, shuffle=False)\n",
        "\n",
        "best_model.eval()\n",
        "test_preds = []\n",
        "with torch.no_grad():\n",
        "    for inputs in test_loader:\n",
        "        inputs = inputs.to(DEVICE)\n",
        "        logits = best_model(inputs)\n",
        "        test_preds.append(torch.argmax(logits, dim=1).cpu().numpy())\n",
        "\n",
        "test_preds = np.concatenate(test_preds)\n",
        "test_labels = [IDX2LABEL[idx] for idx in test_preds]\n",
        "\n",
        "submission = pd.DataFrame({\n",
        "    'sample_index': pd.unique(X_test_raw['sample_index']),\n",
        "    'label': test_labels,\n",
        "})\n",
        "submission_filename = OUTPUT_DIR / f\"submission_{best_run['run_name'].lower()}.csv\"\n",
        "submission.to_csv(submission_filename, index=False)\n",
        "print(f\"Saved submission to {submission_filename}\")\n",
        "submission.head()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Next Steps\n",
        "\n",
        "- Aggiungere nuovi esperimenti modificando `EXPERIMENT_CONFIGS` (es. diverse profondit√†, dropout, versioni mono/bidirezionali).\n",
        "- Integrare una procedura di `StratifiedKFold` o `GroupKFold` per mediare su pi√π split e ridurre la varianza della validazione.\n",
        "- Loggare e confrontare i risultati in `summary_table` (esportarli in CSV per il report finale).\n",
        "- Provare tecniche di regularizzazione aggiuntive (label smoothing, mixup temporale) o layer di attenzione per migliorare l'F1.\n",
        "- Consolidare un ensemble (media delle probabilit√† di pi√π checkpoint) prima della submission Kaggle definitiva.\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
