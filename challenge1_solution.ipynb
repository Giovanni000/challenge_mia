{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ðŸ´â€â˜ ï¸ AN2DL25 Challenge 1 â€” Pirate Pain Classification\n",
        "\n",
        "This notebook implements a full deep-learning pipeline for multivariate time-series classification of the Pirate Pain dataset. It is inspired by the Lecture 4 notebook (`Timeseries Classification (1).ipynb`) but adapted for the competition setting, including data preparation, model training (RNN/GRU/LSTM variants), evaluation, and test-time inference.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# %%capture\n",
        "# !pip install -q -r requirements.txt\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import random\n",
        "import math\n",
        "import copy\n",
        "from pathlib import Path\n",
        "from typing import Tuple, Dict, Optional, List\n",
        "from datetime import datetime\n",
        "from itertools import product\n",
        "import inspect\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.metrics import (\n",
        "    f1_score,\n",
        "    classification_report,\n",
        "    confusion_matrix,\n",
        "    precision_recall_fscore_support,\n",
        ")\n",
        "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "\n",
        "try:\n",
        "    from torch.amp import autocast, GradScaler\n",
        "except ImportError:  # pragma: no cover\n",
        "    from torch.cuda.amp import autocast, GradScaler\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "SEED = 42\n",
        "\n",
        "try:\n",
        "    from google.colab import drive  # type: ignore\n",
        "    IN_COLAB = True\n",
        "except ImportError:  # pragma: no cover\n",
        "    IN_COLAB = False\n",
        "\n",
        "if IN_COLAB:\n",
        "    drive.mount('/content/drive', force_remount=False)\n",
        "    BASE_DIR = Path('/content/drive/MyDrive/[2025-2026]\\ A2NDL/Challenge')\n",
        "else:\n",
        "    BASE_DIR = Path('/Users/md101ta/Desktop/Pirates')\n",
        "\n",
        "DATA_DIR = (BASE_DIR / 'data').resolve()\n",
        "OUTPUT_DIR = (BASE_DIR / 'outputs').resolve()\n",
        "OUTPUT_DIR.mkdir(exist_ok=True, parents=True)\n",
        "\n",
        "# Reproducibility\n",
        "os.environ['PYTHONHASHSEED'] = str(SEED)\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(SEED)\n",
        "    DEVICE = torch.device('cuda')\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "else:\n",
        "    DEVICE = torch.device('cpu')\n",
        "\n",
        "print(f'Running in Colab: {IN_COLAB}')\n",
        "print(f'Device: {DEVICE}')\n",
        "print(f'Data dir: {DATA_DIR}')\n",
        "print(f'Output dir: {OUTPUT_DIR}')\n",
        "\n",
        "_AUTocast_params = inspect.signature(autocast).parameters\n",
        "_GRADSCALER_PARAMS = inspect.signature(GradScaler).parameters\n",
        "\n",
        "def autocast_context():\n",
        "    enabled = DEVICE.type == 'cuda'\n",
        "    if 'device_type' in _AUTocast_params:\n",
        "        return autocast(device_type=DEVICE.type, enabled=enabled)\n",
        "    if 'device' in _AUTocast_params:\n",
        "        return autocast(DEVICE.type, enabled=enabled)\n",
        "    # fallback to legacy signature (enabled only)\n",
        "    return autocast(enabled=enabled)\n",
        "\n",
        "\n",
        "def create_grad_scaler():\n",
        "    enabled = DEVICE.type == 'cuda'\n",
        "    if 'device_type' in _GRADSCALER_PARAMS:\n",
        "        return GradScaler(device_type=DEVICE.type, enabled=enabled)\n",
        "    return GradScaler(enabled=enabled)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "LOG_DIR = (OUTPUT_DIR / 'logs').resolve()\n",
        "CHECKPOINT_DIR = (OUTPUT_DIR / 'checkpoints').resolve()\n",
        "LOG_DIR.mkdir(exist_ok=True, parents=True)\n",
        "CHECKPOINT_DIR.mkdir(exist_ok=True, parents=True)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "from typing import Tuple\n",
        "\n",
        "# Percorso al dataset su Google Drive\n",
        "DATA_DIR = Path('/content/drive/MyDrive/[2025-2026] AN2DL/Challenge')\n",
        "\n",
        "def load_data(data_dir: Path) -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n",
        "    X_train = pd.read_csv(data_dir / 'pirate_pain_train.csv')\n",
        "    y_train = pd.read_csv(data_dir / 'pirate_pain_train_labels.csv')\n",
        "    X_test  = pd.read_csv(data_dir / 'pirate_pain_test.csv')\n",
        "    return X_train, y_train, X_test\n",
        "\n",
        "# Carica i dati\n",
        "X_train_raw, y_train, X_test_raw = load_data(DATA_DIR)\n",
        "print(X_train_raw.shape, y_train.shape, X_test_raw.shape)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "CATEGORICAL_COLUMNS = ['n_legs', 'n_hands', 'n_eyes']\n",
        "CATEGORY_MAPPINGS: Dict[str, Dict[str, int]] = {}\n",
        "\n",
        "for col in CATEGORICAL_COLUMNS:\n",
        "    uniques = pd.concat([X_train_raw[col], X_test_raw[col]]).dropna().unique()\n",
        "    mapping = {value: idx for idx, value in enumerate(sorted(uniques))}\n",
        "    CATEGORY_MAPPINGS[col] = mapping\n",
        "    X_train_raw[col] = X_train_raw[col].map(mapping).astype(np.int32)\n",
        "    X_test_raw[col] = X_test_raw[col].map(mapping).astype(np.int32)\n",
        "\n",
        "FEATURE_COLUMNS = [col for col in X_train_raw.columns if col not in ['sample_index', 'time']]\n",
        "TIME_STEPS = X_train_raw['time'].nunique()\n",
        "NUM_FEATURES = len(FEATURE_COLUMNS)\n",
        "NUM_CLASSES = y_train['label'].nunique()\n",
        "print(f'Time steps: {TIME_STEPS} | Features: {NUM_FEATURES} | Classes: {NUM_CLASSES}')\n",
        "print('Category mappings:', CATEGORY_MAPPINGS)\n",
        "\n",
        "y_train['label'].value_counts().plot(kind='bar', title='Class distribution')\n",
        "plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Exploratory data analysis plots\n",
        "train_with_labels = X_train_raw.merge(y_train, on='sample_index')\n",
        "\n",
        "survey_columns = [col for col in X_train_raw.columns if col.startswith('pain_survey')]\n",
        "joint_columns = [col for col in X_train_raw.columns if col.startswith('joint_')]\n",
        "\n",
        "# Mean survey trajectory per class\n",
        "survey_time_mean = (\n",
        "    train_with_labels[['time', 'label'] + survey_columns]\n",
        "    .groupby(['label', 'time'])[survey_columns]\n",
        "    .mean()\n",
        "    .reset_index()\n",
        ")\n",
        "\n",
        "fig, axes = plt.subplots(1, 2, figsize=(16, 5))\n",
        "sns.lineplot(\n",
        "    data=survey_time_mean,\n",
        "    x='time',\n",
        "    y=survey_columns[0],\n",
        "    hue='label',\n",
        "    palette='Dark2',\n",
        "    ax=axes[0],\n",
        ")\n",
        "axes[0].set_title(f\"Average {survey_columns[0]} across time\")\n",
        "axes[0].set_ylabel('Survey score')\n",
        "axes[0].set_xlabel('Time step')\n",
        "\n",
        "# Distribution of joint activation per class (average across joints and time)\n",
        "sample_joint_mean = (\n",
        "    train_with_labels.groupby(['sample_index', 'label'])[joint_columns]\n",
        "    .mean()\n",
        "    .reset_index()\n",
        ")\n",
        "sample_joint_mean['mean_joint_activation'] = sample_joint_mean[joint_columns].mean(axis=1)\n",
        "\n",
        "sns.boxplot(\n",
        "    data=sample_joint_mean,\n",
        "    x='label',\n",
        "    y='mean_joint_activation',\n",
        "    palette='Set2',\n",
        "    ax=axes[1],\n",
        ")\n",
        "axes[1].set_title('Average joint activation per sample')\n",
        "axes[1].set_xlabel('Pain class')\n",
        "axes[1].set_ylabel('Mean joint value')\n",
        "\n",
        "plt.suptitle('Pirate Pain dataset â€” exploratory views', fontsize=14)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "LABEL2IDX = {label: idx for idx, label in enumerate(sorted(y_train['label'].unique()))}\n",
        "IDX2LABEL = {idx: label for label, idx in LABEL2IDX.items()}\n",
        "print('Label mapping:', LABEL2IDX)\n",
        "\n",
        "\n",
        "def pivot_timeseries(df: pd.DataFrame) -> np.ndarray:\n",
        "    pivoted = (\n",
        "        df.pivot(index='sample_index', columns='time', values=FEATURE_COLUMNS)\n",
        "          .sort_index(axis=0)\n",
        "          .sort_index(axis=1, level=1)\n",
        "    )\n",
        "    data = pivoted.to_numpy().reshape(-1, TIME_STEPS, NUM_FEATURES)\n",
        "    return data\n",
        "\n",
        "\n",
        "X_train_np = pivot_timeseries(X_train_raw)\n",
        "X_test_np = pivot_timeseries(X_test_raw)\n",
        "y_train_idx = y_train.set_index('sample_index').loc[pd.unique(X_train_raw['sample_index'])]['label'].map(LABEL2IDX).to_numpy()\n",
        "\n",
        "print(X_train_np.shape, y_train_idx.shape, X_test_np.shape)\n",
        "\n",
        "CLASS_COUNTS = np.bincount(y_train_idx, minlength=NUM_CLASSES)\n",
        "CLASS_WEIGHTS = (len(y_train_idx) / (NUM_CLASSES * CLASS_COUNTS)).astype(np.float32)\n",
        "CLASS_WEIGHT_DICT = {IDX2LABEL[idx]: weight for idx, weight in enumerate(CLASS_WEIGHTS)}\n",
        "print('Class counts:', dict(zip([IDX2LABEL[i] for i in range(NUM_CLASSES)], CLASS_COUNTS)))\n",
        "print('Class weights (balanced):', CLASS_WEIGHT_DICT)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def compute_normalization_stats(data: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:\n",
        "    # data shape: (N, T, F)\n",
        "    mean = data.reshape(-1, NUM_FEATURES).mean(axis=0)\n",
        "    std = data.reshape(-1, NUM_FEATURES).std(axis=0) + 1e-6\n",
        "    return mean, std\n",
        "\n",
        "\n",
        "def normalize(data: np.ndarray, mean: np.ndarray, std: np.ndarray) -> np.ndarray:\n",
        "    return (data - mean) / std\n",
        "\n",
        "\n",
        "feat_mean, feat_std = compute_normalization_stats(X_train_np)\n",
        "X_train_np = normalize(X_train_np, feat_mean, feat_std)\n",
        "X_test_np = normalize(X_test_np, feat_mean, feat_std)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def make_dataloader_from_arrays(\n",
        "    X: np.ndarray,\n",
        "    y: Optional[np.ndarray],\n",
        "    batch_size: int,\n",
        "    shuffle: bool,\n",
        "    *,\n",
        "    class_weights: Optional[np.ndarray] = None,\n",
        "    use_weighted_sampler: bool = False,\n",
        ") -> DataLoader:\n",
        "    dataset = TimeSeriesDataset(X, y)\n",
        "    if use_weighted_sampler and y is not None and class_weights is not None:\n",
        "        sample_weights = class_weights[y]\n",
        "        sampler = WeightedRandomSampler(\n",
        "            weights=torch.from_numpy(sample_weights).double(),\n",
        "            num_samples=len(sample_weights),\n",
        "            replacement=True,\n",
        "        )\n",
        "        return DataLoader(dataset, batch_size=batch_size, sampler=sampler, drop_last=False)\n",
        "    return DataLoader(dataset, batch_size=batch_size, shuffle=shuffle, drop_last=False)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class TimeSeriesDataset(Dataset):\n",
        "    def __init__(self, data: np.ndarray, labels: Optional[np.ndarray] = None):\n",
        "        self.data = torch.tensor(data, dtype=torch.float32)\n",
        "        self.labels = None if labels is None else torch.tensor(labels, dtype=torch.long)\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        return self.data.shape[0]\n",
        "\n",
        "    def __getitem__(self, idx: int):\n",
        "        if self.labels is None:\n",
        "            return self.data[idx]\n",
        "        return self.data[idx], self.labels[idx]\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def create_dataloaders(X: np.ndarray, y: np.ndarray, valid_size: float = 0.2, batch_size: int = 64):\n",
        "    X_train, X_valid, y_train, y_valid = train_test_split(\n",
        "        X, y, test_size=valid_size, random_state=SEED, stratify=y\n",
        "    )\n",
        "\n",
        "    train_loader = make_dataloader_from_arrays(\n",
        "        X_train,\n",
        "        y_train,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=False,\n",
        "        class_weights=CLASS_WEIGHTS,\n",
        "        use_weighted_sampler=True,\n",
        "    )\n",
        "    valid_loader = make_dataloader_from_arrays(\n",
        "        X_valid,\n",
        "        y_valid,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=False,\n",
        "    )\n",
        "    return train_loader, valid_loader, (X_train, y_train, X_valid, y_valid)\n",
        "\n",
        "\n",
        "BATCH_SIZE = 64\n",
        "train_loader, valid_loader, (X_train_split, y_train_split, X_valid_split, y_valid_split) = create_dataloaders(X_train_np, y_train_idx, batch_size=BATCH_SIZE)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class RecurrentBackbone(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        input_size: int,\n",
        "        hidden_size: int = 128,\n",
        "        num_layers: int = 2,\n",
        "        dropout: float = 0.2,\n",
        "        rnn_type: str = 'lstm',\n",
        "        bidirectional: bool = True,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        rnn_cls = {\n",
        "            'rnn': nn.RNN,\n",
        "            'gru': nn.GRU,\n",
        "            'lstm': nn.LSTM,\n",
        "        }[rnn_type.lower()]\n",
        "        self.rnn_type = rnn_type.lower()\n",
        "        self.bidirectional = bidirectional\n",
        "        self.rnn = rnn_cls(\n",
        "            input_size=input_size,\n",
        "            hidden_size=hidden_size,\n",
        "            num_layers=num_layers,\n",
        "            dropout=dropout if num_layers > 1 else 0.0,\n",
        "            batch_first=True,\n",
        "            bidirectional=bidirectional,\n",
        "        )\n",
        "        proj_input = hidden_size * (2 if bidirectional else 1)\n",
        "        self.head = nn.Sequential(\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(proj_input, hidden_size),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(hidden_size, NUM_CLASSES),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out, _ = self.rnn(x)\n",
        "        # use last time-step hidden state\n",
        "        last = out[:, -1, :]\n",
        "        return self.head(last)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def compute_classification_metrics(preds: np.ndarray, targets: np.ndarray) -> Dict[str, float]:\n",
        "    accuracy = float((preds == targets).mean())\n",
        "    precision, recall, f1, _ = precision_recall_fscore_support(\n",
        "        targets,\n",
        "        preds,\n",
        "        average='macro',\n",
        "        zero_division=0,\n",
        "    )\n",
        "    return {\n",
        "        'accuracy': accuracy,\n",
        "        'precision': float(precision),\n",
        "        'recall': float(recall),\n",
        "        'f1': float(f1),\n",
        "    }\n",
        "\n",
        "\n",
        "def train_one_epoch(\n",
        "    model: nn.Module,\n",
        "    loader: DataLoader,\n",
        "    criterion: nn.Module,\n",
        "    optimizer: torch.optim.Optimizer,\n",
        "    scaler: GradScaler,\n",
        "    max_grad_norm: float = 5.0,\n",
        ") -> Tuple[float, Dict[str, float]]:\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    preds_all, targets_all = [], []\n",
        "\n",
        "    for inputs, targets in loader:\n",
        "        inputs = inputs.to(DEVICE)\n",
        "        targets = targets.to(DEVICE)\n",
        "        optimizer.zero_grad(set_to_none=True)\n",
        "\n",
        "        with autocast_context():\n",
        "            logits = model(inputs)\n",
        "            loss = criterion(logits, targets)\n",
        "\n",
        "        scaler.scale(loss).backward()\n",
        "        if max_grad_norm is not None:\n",
        "            scaler.unscale_(optimizer)\n",
        "            nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "\n",
        "        running_loss += loss.item() * inputs.size(0)\n",
        "        preds_all.append(torch.argmax(logits.detach(), dim=1).cpu())\n",
        "        targets_all.append(targets.cpu())\n",
        "\n",
        "    preds_np = torch.cat(preds_all).numpy()\n",
        "    targets_np = torch.cat(targets_all).numpy()\n",
        "    metrics = compute_classification_metrics(preds_np, targets_np)\n",
        "    avg_loss = running_loss / len(loader.dataset)\n",
        "    return avg_loss, metrics\n",
        "\n",
        "\n",
        "def evaluate_epoch(\n",
        "    model: nn.Module,\n",
        "    loader: DataLoader,\n",
        "    criterion: nn.Module,\n",
        ") -> Tuple[float, Dict[str, float], np.ndarray, np.ndarray]:\n",
        "    model.eval()\n",
        "    running_loss = 0.0\n",
        "    preds_all, targets_all = [], []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, targets in loader:\n",
        "            inputs = inputs.to(DEVICE)\n",
        "            targets = targets.to(DEVICE)\n",
        "            with autocast_context():\n",
        "                logits = model(inputs)\n",
        "                loss = criterion(logits, targets)\n",
        "\n",
        "            running_loss += loss.item() * inputs.size(0)\n",
        "            preds_all.append(torch.argmax(logits, dim=1).cpu())\n",
        "            targets_all.append(targets.cpu())\n",
        "\n",
        "    preds_np = torch.cat(preds_all).numpy()\n",
        "    targets_np = torch.cat(targets_all).numpy()\n",
        "    metrics = compute_classification_metrics(preds_np, targets_np)\n",
        "    avg_loss = running_loss / len(loader.dataset)\n",
        "    return avg_loss, metrics, preds_np, targets_np\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def fit_model(\n",
        "    config: Dict,\n",
        "    train_loader: DataLoader,\n",
        "    valid_loader: DataLoader,\n",
        "    run_name: str,\n",
        "    tensorboard: bool = True,\n",
        ") -> Tuple[nn.Module, Dict[str, List[float]], Dict]:\n",
        "    model = RecurrentBackbone(\n",
        "        input_size=NUM_FEATURES,\n",
        "        hidden_size=config['hidden_size'],\n",
        "        num_layers=config['num_layers'],\n",
        "        dropout=config['dropout'],\n",
        "        rnn_type=config['rnn_type'],\n",
        "        bidirectional=config.get('bidirectional', False),\n",
        "    ).to(DEVICE)\n",
        "\n",
        "    class_weight_tensor = torch.tensor(CLASS_WEIGHTS, dtype=torch.float32, device=DEVICE)\n",
        "    criterion = nn.CrossEntropyLoss(weight=class_weight_tensor)\n",
        "    optimizer = torch.optim.AdamW(\n",
        "        model.parameters(),\n",
        "        lr=config['lr'],\n",
        "        weight_decay=config.get('weight_decay', 0.0),\n",
        "    )\n",
        "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
        "        optimizer,\n",
        "        mode='max',\n",
        "        factor=config.get('scheduler_factor', 0.5),\n",
        "        patience=config.get('scheduler_patience', 3),\n",
        "    )\n",
        "    scaler = create_grad_scaler()\n",
        "\n",
        "    history: Dict[str, List[float]] = {\n",
        "        'train_loss': [],\n",
        "        'train_accuracy': [],\n",
        "        'train_f1': [],\n",
        "        'train_precision': [],\n",
        "        'train_recall': [],\n",
        "        'valid_loss': [],\n",
        "        'valid_accuracy': [],\n",
        "        'valid_f1': [],\n",
        "        'valid_precision': [],\n",
        "        'valid_recall': [],\n",
        "        'lr': [],\n",
        "    }\n",
        "\n",
        "    run_log_dir = (LOG_DIR / run_name).resolve()\n",
        "    writer = SummaryWriter(run_log_dir.as_posix()) if tensorboard else None\n",
        "\n",
        "    best_metric = -np.inf\n",
        "    best_state: Optional[Dict] = None\n",
        "    patience = config.get('patience', 10)\n",
        "    patience_counter = 0\n",
        "    checkpoint_path = (CHECKPOINT_DIR / f'{run_name}.pt').resolve()\n",
        "\n",
        "    for epoch in range(1, config['epochs'] + 1):\n",
        "        train_loss, train_metrics = train_one_epoch(\n",
        "            model,\n",
        "            train_loader,\n",
        "            criterion,\n",
        "            optimizer,\n",
        "            scaler,\n",
        "            max_grad_norm=config.get('max_grad_norm', 5.0),\n",
        "        )\n",
        "        valid_loss, valid_metrics, preds, targets = evaluate_epoch(\n",
        "            model,\n",
        "            valid_loader,\n",
        "            criterion,\n",
        "        )\n",
        "        scheduler.step(valid_metrics['f1'])\n",
        "\n",
        "        current_lr = optimizer.param_groups[0]['lr']\n",
        "\n",
        "        history['train_loss'].append(train_loss)\n",
        "        history['train_accuracy'].append(train_metrics['accuracy'])\n",
        "        history['train_f1'].append(train_metrics['f1'])\n",
        "        history['train_precision'].append(train_metrics['precision'])\n",
        "        history['train_recall'].append(train_metrics['recall'])\n",
        "        history['valid_loss'].append(valid_loss)\n",
        "        history['valid_accuracy'].append(valid_metrics['accuracy'])\n",
        "        history['valid_f1'].append(valid_metrics['f1'])\n",
        "        history['valid_precision'].append(valid_metrics['precision'])\n",
        "        history['valid_recall'].append(valid_metrics['recall'])\n",
        "        history['lr'].append(current_lr)\n",
        "\n",
        "        if writer is not None:\n",
        "            writer.add_scalar('Loss/train', train_loss, epoch)\n",
        "            writer.add_scalar('Loss/valid', valid_loss, epoch)\n",
        "            writer.add_scalar('F1/train', train_metrics['f1'], epoch)\n",
        "            writer.add_scalar('F1/valid', valid_metrics['f1'], epoch)\n",
        "            writer.add_scalar('Accuracy/train', train_metrics['accuracy'], epoch)\n",
        "            writer.add_scalar('Accuracy/valid', valid_metrics['accuracy'], epoch)\n",
        "            writer.add_scalar('LearningRate', current_lr, epoch)\n",
        "\n",
        "        msg = (\n",
        "            f\"Epoch {epoch:03d} | \"\n",
        "            f\"train_loss={train_loss:.4f} acc={train_metrics['accuracy']:.3f} f1={train_metrics['f1']:.3f} | \"\n",
        "            f\"valid_loss={valid_loss:.4f} acc={valid_metrics['accuracy']:.3f} f1={valid_metrics['f1']:.3f} | \"\n",
        "            f\"lr={current_lr:.2e}\"\n",
        "        )\n",
        "        print(msg)\n",
        "\n",
        "        if valid_metrics['f1'] > best_metric + config.get('min_improvement', 0.0):\n",
        "            best_metric = valid_metrics['f1']\n",
        "            patience_counter = 0\n",
        "            best_state = {\n",
        "                'epoch': epoch,\n",
        "                'model_state': copy.deepcopy(model.state_dict()),\n",
        "                'optimizer_state': copy.deepcopy(optimizer.state_dict()),\n",
        "                'metrics': valid_metrics,\n",
        "                'train_metrics': train_metrics,\n",
        "                'preds': preds,\n",
        "                'targets': targets,\n",
        "            }\n",
        "            torch.save(best_state['model_state'], checkpoint_path)\n",
        "        else:\n",
        "            patience_counter += 1\n",
        "            if patience_counter >= patience:\n",
        "                print(f\"Early stopping triggered at epoch {epoch}. Best f1={best_metric:.4f}.\")\n",
        "                break\n",
        "\n",
        "    if writer is not None:\n",
        "        writer.close()\n",
        "\n",
        "    if best_state is None:\n",
        "        best_state = {\n",
        "            'epoch': config['epochs'],\n",
        "            'model_state': copy.deepcopy(model.state_dict()),\n",
        "            'optimizer_state': copy.deepcopy(optimizer.state_dict()),\n",
        "            'metrics': valid_metrics,\n",
        "            'train_metrics': train_metrics,\n",
        "            'preds': preds,\n",
        "            'targets': targets,\n",
        "        }\n",
        "        torch.save(best_state['model_state'], checkpoint_path)\n",
        "\n",
        "    model.load_state_dict(best_state['model_state'])\n",
        "    best_state.update(\n",
        "        {\n",
        "            'run_name': run_name,\n",
        "            'config': copy.deepcopy(config),\n",
        "            'history': history,\n",
        "            'checkpoint_path': checkpoint_path,\n",
        "            'best_f1': best_metric,\n",
        "        }\n",
        "    )\n",
        "    return model, history, best_state\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def run_cross_validation(\n",
        "    config: Dict,\n",
        "    n_splits: int = 5,\n",
        "    shuffle: bool = True,\n",
        "    random_state: int = SEED,\n",
        ") -> List[Dict]:\n",
        "    skf = StratifiedKFold(n_splits=n_splits, shuffle=shuffle, random_state=random_state)\n",
        "    fold_results: List[Dict] = []\n",
        "\n",
        "    print(f\"\\n>>> Starting {n_splits}-fold CV for {config['run_name']} ({config['rnn_type'].upper()})\")\n",
        "    for fold_idx, (train_idx, valid_idx) in enumerate(skf.split(X_train_np, y_train_idx), start=1):\n",
        "        fold_config = copy.deepcopy(config)\n",
        "        fold_config['run_name'] = f\"{config['run_name']}_fold{fold_idx}\"\n",
        "\n",
        "        X_tr, y_tr = X_train_np[train_idx], y_train_idx[train_idx]\n",
        "        X_val, y_val = X_train_np[valid_idx], y_train_idx[valid_idx]\n",
        "\n",
        "        train_loader = make_dataloader_from_arrays(\n",
        "            X_tr,\n",
        "            y_tr,\n",
        "            batch_size=fold_config['batch_size'],\n",
        "            shuffle=False,\n",
        "            class_weights=CLASS_WEIGHTS,\n",
        "            use_weighted_sampler=True,\n",
        "        )\n",
        "        valid_loader = make_dataloader_from_arrays(\n",
        "            X_val,\n",
        "            y_val,\n",
        "            batch_size=fold_config['batch_size'],\n",
        "            shuffle=False,\n",
        "        )\n",
        "\n",
        "        model, history, best_state = fit_model(\n",
        "            fold_config,\n",
        "            train_loader,\n",
        "            valid_loader,\n",
        "            run_name=fold_config['run_name'],\n",
        "            tensorboard=fold_config.get('tensorboard', True),\n",
        "        )\n",
        "        best_state['fold'] = fold_idx\n",
        "        best_state['model'] = model\n",
        "        best_state['history'] = history\n",
        "        fold_results.append(best_state)\n",
        "\n",
        "        metrics = best_state['metrics']\n",
        "        print(\n",
        "            f\"Fold {fold_idx}/{n_splits} | best F1={metrics['f1']:.4f} | \"\n",
        "            f\"Accuracy={metrics['accuracy']:.4f}\"\n",
        "        )\n",
        "\n",
        "    return fold_results\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def summarize_cv_results(cv_results: List[Dict]) -> pd.DataFrame:\n",
        "    rows = []\n",
        "    for res in cv_results:\n",
        "        cfg = res['config']\n",
        "        metrics = res['metrics']\n",
        "        rows.append({\n",
        "            'run_name': res['run_name'],\n",
        "            'fold': res['fold'],\n",
        "            'rnn_type': cfg['rnn_type'],\n",
        "            'bidirectional': cfg.get('bidirectional', False),\n",
        "            'hidden_size': cfg['hidden_size'],\n",
        "            'num_layers': cfg['num_layers'],\n",
        "            'dropout': cfg['dropout'],\n",
        "            'f1': metrics['f1'],\n",
        "            'accuracy': metrics['accuracy'],\n",
        "            'precision': metrics['precision'],\n",
        "            'recall': metrics['recall'],\n",
        "            'checkpoint': str(res['checkpoint_path']),\n",
        "            'log_dir': str(LOG_DIR / res['run_name']),\n",
        "        })\n",
        "\n",
        "    df = pd.DataFrame(rows)\n",
        "    agg = df.groupby(['rnn_type', 'bidirectional'])[['f1', 'accuracy', 'precision', 'recall']].agg(['mean', 'std'])\n",
        "    agg.columns = ['_'.join(col) for col in agg.columns]\n",
        "    agg = agg.reset_index()\n",
        "    return df, agg\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def prepare_config(name: str, overrides: Dict) -> Dict:\n",
        "    base_config = {\n",
        "        'run_name': name,\n",
        "        'rnn_type': 'lstm',\n",
        "        'hidden_size': 256,\n",
        "        'num_layers': 3,\n",
        "        'dropout': 0.4,\n",
        "        'bidirectional': True,\n",
        "        'lr': 2e-3,\n",
        "        'weight_decay': 1e-4,\n",
        "        'epochs': 200,\n",
        "        'batch_size': 128,\n",
        "        'valid_size': 0.2,\n",
        "        'patience': 20,\n",
        "        'max_grad_norm': 5.0,\n",
        "        'scheduler_factor': 0.5,\n",
        "        'scheduler_patience': 5,\n",
        "        'min_improvement': 5e-4,\n",
        "        'tensorboard': True,\n",
        "    }\n",
        "    config = copy.deepcopy(base_config)\n",
        "    config.update(overrides)\n",
        "    return config\n",
        "\n",
        "\n",
        "def run_experiment(config: Dict) -> Dict:\n",
        "    run_name = config.get('run_name') or f\"{config['rnn_type'].upper()}_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
        "    config = copy.deepcopy(config)\n",
        "    config['run_name'] = run_name\n",
        "\n",
        "    train_loader, valid_loader, (X_tr, y_tr, X_val, y_val) = create_dataloaders(\n",
        "        X_train_np,\n",
        "        y_train_idx,\n",
        "        valid_size=config.get('valid_size', 0.2),\n",
        "        batch_size=config['batch_size'],\n",
        "    )\n",
        "\n",
        "    model, history, best_state = fit_model(\n",
        "        config,\n",
        "        train_loader,\n",
        "        valid_loader,\n",
        "        run_name=run_name,\n",
        "        tensorboard=config.get('tensorboard', True),\n",
        "    )\n",
        "\n",
        "    best_state['data_split'] = {\n",
        "        'X_train': X_tr,\n",
        "        'y_train': y_tr,\n",
        "        'X_valid': X_val,\n",
        "        'y_valid': y_val,\n",
        "    }\n",
        "    best_state['model'] = model\n",
        "    best_state['history'] = history\n",
        "    return best_state\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "EXPERIMENT_CONFIGS = [\n",
        "\n",
        "    prepare_config('GRU_SINGLE', {'rnn_type': 'gru', 'bidirectional': False}),\n",
        "\n",
        "]\n",
        "\n",
        "experiment_results: List[Dict] = []\n",
        "for cfg in EXPERIMENT_CONFIGS:\n",
        "    print(f\"\\n=== Running experiment: {cfg['run_name']} ({cfg['rnn_type'].upper()} - {'BI' if cfg['bidirectional'] else 'UNI'}) ===\")\n",
        "    result = run_experiment(cfg)\n",
        "    experiment_results.append(result)\n",
        "    print(\n",
        "        f\"Best validation F1: {result['best_f1']:.4f} at epoch {result['epoch']} | \"\n",
        "        f\"Accuracy: {result['metrics']['accuracy']:.4f}\"\n",
        "    )\n",
        "\n",
        "BEST_FOR_CV = prepare_config('GRU_SINGLE', {'bidirectional': False})\n",
        "cv_results = run_cross_validation(BEST_FOR_CV, n_splits=5)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "cv_folds_df, cv_summary = summarize_cv_results(cv_results)\n",
        "cv_summary\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "SWEEP_PARAM_SPACE = {\n",
        "    'hidden_size': [192, 256, 320],\n",
        "    'num_layers': [2, 3],\n",
        "    'dropout': [0.30, 0.45],\n",
        "    'bidirectional': [True, False],\n",
        "    'lr': [1e-3, 2e-3],\n",
        "}\n",
        "MAX_SWEEP_RUNS = 12\n",
        "SWEEP_SEED = 42\n",
        "\n",
        "param_grid = list(product(\n",
        "    SWEEP_PARAM_SPACE['hidden_size'],\n",
        "    SWEEP_PARAM_SPACE['num_layers'],\n",
        "    SWEEP_PARAM_SPACE['dropout'],\n",
        "    SWEEP_PARAM_SPACE['bidirectional'],\n",
        "    SWEEP_PARAM_SPACE['lr'],\n",
        "))\n",
        "random.Random(SWEEP_SEED).shuffle(param_grid)\n",
        "param_grid = param_grid[:MAX_SWEEP_RUNS]\n",
        "\n",
        "sweep_results: List[Dict] = []\n",
        "for idx, (hidden_size, num_layers, dropout, bidirectional, lr) in enumerate(param_grid, start=1):\n",
        "    run_name = (\n",
        "        f\"GRU_SWEEP_{idx:02d}_HS{hidden_size}_L{num_layers}_DO{int(dropout*100)}_\"\n",
        "        f\"{'BI' if bidirectional else 'UNI'}_LR{lr:.0e}\"\n",
        "    )\n",
        "    cfg = prepare_config(\n",
        "        run_name,\n",
        "        {\n",
        "            'rnn_type': 'gru',\n",
        "            'hidden_size': hidden_size,\n",
        "            'num_layers': num_layers,\n",
        "            'dropout': dropout,\n",
        "            'bidirectional': bidirectional,\n",
        "            'lr': lr,\n",
        "        },\n",
        "    )\n",
        "    print(f\"\\n>>> Sweep run {idx}/{len(param_grid)} â€” {run_name}\")\n",
        "    result = run_experiment(cfg)\n",
        "    sweep_results.append(result)\n",
        "\n",
        "print(f\"\\nCompleted {len(sweep_results)} GRU sweep runs.\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "for _, row in summary_table.iterrows():\n",
        "    print(\n",
        "        f\"Run: {row['run_name']} | Model: {row['rnn_type'].upper()} | \"\n",
        "        f\"Bidirectional: {row['bidirectional']} | F1: {row['best_f1']:.4f}\"\n",
        "    )\n",
        "    print(f\"  Logs: {row['log_dir']}\")\n",
        "\n",
        "# Per TensorBoard combinato (eseguire su Colab / locale):\n",
        "# %tensorboard --logdir \"{LOG_DIR.as_posix()}\"\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def gather_results(source_names: List[str]) -> List[Dict]:\n",
        "    collected: List[Dict] = []\n",
        "    for name in source_names:\n",
        "        if name in globals():\n",
        "            value = globals()[name]\n",
        "            if isinstance(value, list) and value:\n",
        "                collected.extend(value)\n",
        "    return collected\n",
        "\n",
        "\n",
        "def build_summary_table(result_sources: List[str]) -> Tuple[pd.DataFrame, List[Dict]]:\n",
        "    collected_results = gather_results(result_sources)\n",
        "    if not collected_results:\n",
        "        raise RuntimeError('No experiment results available. Run the training/sweep cells first.')\n",
        "\n",
        "    summary_rows = []\n",
        "    for res in collected_results:\n",
        "        cfg = res['config']\n",
        "        metrics = res['metrics']\n",
        "        summary_rows.append({\n",
        "            'run_name': res['run_name'],\n",
        "            'fold': res.get('fold'),\n",
        "            'rnn_type': cfg['rnn_type'],\n",
        "            'bidirectional': cfg.get('bidirectional', False),\n",
        "            'hidden_size': cfg['hidden_size'],\n",
        "            'num_layers': cfg['num_layers'],\n",
        "            'dropout': cfg['dropout'],\n",
        "            'lr': cfg['lr'],\n",
        "            'weight_decay': cfg.get('weight_decay', 0.0),\n",
        "            'best_epoch': res['epoch'],\n",
        "            'best_f1': res['best_f1'],\n",
        "            'accuracy': metrics['accuracy'],\n",
        "            'precision': metrics['precision'],\n",
        "            'recall': metrics['recall'],\n",
        "            'checkpoint': str(res['checkpoint_path']),\n",
        "            'log_dir': str(LOG_DIR / res['run_name']),\n",
        "            'is_cv': res.get('fold') is not None,\n",
        "        })\n",
        "\n",
        "    summary_df = pd.DataFrame(summary_rows)\n",
        "    summary_df = summary_df.sort_values(by='best_f1', ascending=False).reset_index(drop=True)\n",
        "    return summary_df, collected_results\n",
        "\n",
        "\n",
        "RESULT_SOURCES = ['experiment_results', 'sweep_results', 'cv_results', 'auto_cv_results']\n",
        "summary_table, all_results = build_summary_table(RESULT_SOURCES)\n",
        "summary_table\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "ENABLE_AUTO_CV = False\n",
        "AUTO_CV_TOP_K = 2\n",
        "AUTO_CV_SPLITS = 5\n",
        "\n",
        "if ENABLE_AUTO_CV:\n",
        "    auto_cv_results = []\n",
        "    for _, row in summary_table.head(AUTO_CV_TOP_K).iterrows():\n",
        "        overrides = {\n",
        "            'rnn_type': row['rnn_type'],\n",
        "            'hidden_size': row['hidden_size'],\n",
        "            'num_layers': row['num_layers'],\n",
        "            'dropout': row['dropout'],\n",
        "            'bidirectional': row['bidirectional'],\n",
        "            'lr': row['lr'],\n",
        "            'weight_decay': row['weight_decay'],\n",
        "        }\n",
        "        cfg = prepare_config(f\"{row['run_name']}_AUTO_CV\", overrides)\n",
        "        cv_out = run_cross_validation(cfg, n_splits=AUTO_CV_SPLITS)\n",
        "        auto_cv_results.extend(cv_out)\n",
        "\n",
        "    globals()['auto_cv_results'] = auto_cv_results\n",
        "    print(f\"Completed auto CV for top {min(AUTO_CV_TOP_K, len(summary_table))} runs.\")\n",
        "    RESULT_SOURCES = ['experiment_results', 'sweep_results', 'cv_results', 'auto_cv_results']\n",
        "    summary_table, all_results = build_summary_table(RESULT_SOURCES)\n",
        "    summary_table\n",
        "else:\n",
        "    print('Auto CV disabled. Set ENABLE_AUTO_CV=True to enable automatic cross-validation.')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if 'all_results' not in globals() or not all_results:\n",
        "    raise RuntimeError('No experiment results available. Run the training/sweep cells first.')\n",
        "\n",
        "best_run = max(all_results, key=lambda x: x['best_f1'])\n",
        "best_model = best_run['model']\n",
        "best_history = best_run['history']\n",
        "print(\n",
        "    f\"Selected best run: {best_run['run_name']} | \"\n",
        "    f\"F1={best_run['best_f1']:.4f} | Accuracy={best_run['metrics']['accuracy']:.4f}\"\n",
        ")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def plot_history(history: Dict[str, List[float]], title: str = 'Learning Curves'):\n",
        "    epochs = range(1, len(history['train_loss']) + 1)\n",
        "    fig, axs = plt.subplots(1, 3, figsize=(18, 4))\n",
        "\n",
        "    axs[0].plot(epochs, history['train_loss'], label='Train')\n",
        "    axs[0].plot(epochs, history['valid_loss'], label='Valid')\n",
        "    axs[0].set_title('Loss')\n",
        "    axs[0].set_xlabel('Epoch')\n",
        "    axs[0].legend()\n",
        "\n",
        "    axs[1].plot(epochs, history['train_accuracy'], label='Train')\n",
        "    axs[1].plot(epochs, history['valid_accuracy'], label='Valid')\n",
        "    axs[1].set_title('Accuracy')\n",
        "    axs[1].set_xlabel('Epoch')\n",
        "    axs[1].legend()\n",
        "\n",
        "    axs[2].plot(epochs, history['train_f1'], label='Train F1')\n",
        "    axs[2].plot(epochs, history['valid_f1'], label='Valid F1')\n",
        "    axs[2].set_title('Macro F1')\n",
        "    axs[2].set_xlabel('Epoch')\n",
        "    axs[2].legend()\n",
        "\n",
        "    plt.suptitle(title)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "plot_history(best_history, title=f\"Learning Curves â€” {best_run['run_name']}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "best_preds = best_run['preds']\n",
        "best_targets = best_run['targets']\n",
        "print(f\"Best validation macro F1: {best_run['best_f1']:.3f}\")\n",
        "print(\n",
        "    classification_report(\n",
        "        best_targets,\n",
        "        best_preds,\n",
        "        target_names=[IDX2LABEL[i] for i in range(NUM_CLASSES)],\n",
        "    )\n",
        ")\n",
        "\n",
        "cf = confusion_matrix(best_targets, best_preds)\n",
        "plt.figure(figsize=(6, 5))\n",
        "sns.heatmap(\n",
        "    cf,\n",
        "    annot=True,\n",
        "    fmt='d',\n",
        "    cmap='Blues',\n",
        "    xticklabels=[IDX2LABEL[i] for i in range(NUM_CLASSES)],\n",
        "    yticklabels=[IDX2LABEL[i] for i in range(NUM_CLASSES)],\n",
        ")\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('True')\n",
        "plt.title(f\"Validation Confusion Matrix â€” {best_run['run_name']}\")\n",
        "plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "test_loader = DataLoader(TimeSeriesDataset(X_test_np), batch_size=256, shuffle=False)\n",
        "\n",
        "best_model.eval()\n",
        "test_preds = []\n",
        "with torch.no_grad():\n",
        "    for inputs in test_loader:\n",
        "        inputs = inputs.to(DEVICE)\n",
        "        logits = best_model(inputs)\n",
        "        test_preds.append(torch.argmax(logits, dim=1).cpu().numpy())\n",
        "\n",
        "test_preds = np.concatenate(test_preds)\n",
        "test_labels = [IDX2LABEL[idx] for idx in test_preds]\n",
        "\n",
        "submission = pd.DataFrame({\n",
        "    'sample_index': pd.unique(X_test_raw['sample_index']),\n",
        "    'label': test_labels,\n",
        "})\n",
        "submission_filename = OUTPUT_DIR / f\"submission_{best_run['run_name'].lower()}.csv\"\n",
        "submission.to_csv(submission_filename, index=False)\n",
        "print(f\"Saved submission to {submission_filename}\")\n",
        "submission.head()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%tensorboard --logdir \"/content/drive/MyDrive/[2025-2026]\\ AN2DL/Challenge/outputs/logs\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Next Steps\n",
        "\n",
        "- Espandere `EXPERIMENT_CONFIGS` con ricerche random/grid su hidden size, depth, dropout, learning rate e scheduler per automatizzare l'hyperparameter tuning.\n",
        "- Utilizzare `run_cross_validation` su piÃ¹ configurazioni e confrontare le metriche aggregate in `cv_summary`, esportando i risultati (CSV/LaTeX) per il report finale.\n",
        "- Monitorare tutti i run con `%tensorboard --logdir outputs/logs`, salvando screenshot delle curve principali e confrontando tempi/risorse.\n",
        "- Integrare tecniche di regularizzazione avanzate (label smoothing, mixup temporale, stochastic weight averaging) o layer di attention/pooling.\n",
        "- Costruire ensemble sui checkpoint migliori (media delle probabilitÃ  o voting) prima della submission Kaggle definitiva.\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
