{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ðŸ´â€â˜ ï¸ AN2DL25 Challenge 1 â€” Pirate Pain Classification\n",
        "\n",
        "This notebook implements a full deep-learning pipeline for multivariate time-series classification of the Pirate Pain dataset. It is inspired by the Lecture 4 notebook (`Timeseries Classification (1).ipynb`) but adapted for the competition setting, including data preparation, model training (RNN/GRU/LSTM variants), evaluation, and test-time inference.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# %%capture\n",
        "# !pip install -q -r requirements.txt\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import random\n",
        "import math\n",
        "from pathlib import Path\n",
        "from typing import Tuple, Dict, Optional\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.metrics import f1_score, classification_report, confusion_matrix\n",
        "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "SEED = 42\n",
        "DATA_DIR = Path('/Users/md101ta/Desktop/Pirates/data')\n",
        "OUTPUT_DIR = Path('/Users/md101ta/Desktop/Pirates/outputs')\n",
        "OUTPUT_DIR.mkdir(exist_ok=True)\n",
        "\n",
        "# Reproducibility\n",
        "os.environ['PYTHONHASHSEED'] = str(SEED)\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(SEED)\n",
        "    DEVICE = torch.device('cuda')\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "else:\n",
        "    DEVICE = torch.device('cpu')\n",
        "\n",
        "print(f'Device: {DEVICE}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_data(data_dir: Path) -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n",
        "    X_train = pd.read_csv(data_dir / 'pirate_pain_train.csv')\n",
        "    y_train = pd.read_csv(data_dir / 'pirate_pain_train_labels.csv')\n",
        "    X_test = pd.read_csv(data_dir / 'pirate_pain_test.csv')\n",
        "    return X_train, y_train, X_test\n",
        "\n",
        "\n",
        "X_train_raw, y_train, X_test_raw = load_data(DATA_DIR)\n",
        "print(X_train_raw.shape, y_train.shape, X_test_raw.shape)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "FEATURE_COLUMNS = [col for col in X_train_raw.columns if col not in ['sample_index', 'time']]\n",
        "TIME_STEPS = X_train_raw['time'].nunique()\n",
        "NUM_FEATURES = len(FEATURE_COLUMNS)\n",
        "NUM_CLASSES = y_train['label'].nunique()\n",
        "print(f'Time steps: {TIME_STEPS} | Features: {NUM_FEATURES} | Classes: {NUM_CLASSES}')\n",
        "\n",
        "y_train['label'].value_counts().plot(kind='bar', title='Class distribution')\n",
        "plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "LABEL2IDX = {label: idx for idx, label in enumerate(sorted(y_train['label'].unique()))}\n",
        "IDX2LABEL = {idx: label for label, idx in LABEL2IDX.items()}\n",
        "print('Label mapping:', LABEL2IDX)\n",
        "\n",
        "\n",
        "def pivot_timeseries(df: pd.DataFrame) -> np.ndarray:\n",
        "    pivoted = (\n",
        "        df.pivot(index='sample_index', columns='time', values=FEATURE_COLUMNS)\n",
        "          .sort_index(axis=0)\n",
        "          .sort_index(axis=1, level=1)\n",
        "    )\n",
        "    data = pivoted.to_numpy().reshape(-1, TIME_STEPS, NUM_FEATURES)\n",
        "    return data\n",
        "\n",
        "\n",
        "X_train_np = pivot_timeseries(X_train_raw)\n",
        "X_test_np = pivot_timeseries(X_test_raw)\n",
        "y_train_idx = y_train.set_index('sample_index').loc[pd.unique(X_train_raw['sample_index'])]['label'].map(LABEL2IDX).to_numpy()\n",
        "\n",
        "print(X_train_np.shape, y_train_idx.shape, X_test_np.shape)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def compute_normalization_stats(data: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:\n",
        "    # data shape: (N, T, F)\n",
        "    mean = data.reshape(-1, NUM_FEATURES).mean(axis=0)\n",
        "    std = data.reshape(-1, NUM_FEATURES).std(axis=0) + 1e-6\n",
        "    return mean, std\n",
        "\n",
        "\n",
        "def normalize(data: np.ndarray, mean: np.ndarray, std: np.ndarray) -> np.ndarray:\n",
        "    return (data - mean) / std\n",
        "\n",
        "\n",
        "feat_mean, feat_std = compute_normalization_stats(X_train_np)\n",
        "X_train_np = normalize(X_train_np, feat_mean, feat_std)\n",
        "X_test_np = normalize(X_test_np, feat_mean, feat_std)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class TimeSeriesDataset(Dataset):\n",
        "    def __init__(self, data: np.ndarray, labels: Optional[np.ndarray] = None):\n",
        "        self.data = torch.tensor(data, dtype=torch.float32)\n",
        "        self.labels = None if labels is None else torch.tensor(labels, dtype=torch.long)\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        return self.data.shape[0]\n",
        "\n",
        "    def __getitem__(self, idx: int):\n",
        "        if self.labels is None:\n",
        "            return self.data[idx]\n",
        "        return self.data[idx], self.labels[idx]\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def create_dataloaders(X: np.ndarray, y: np.ndarray, valid_size: float = 0.2, batch_size: int = 64):\n",
        "    X_train, X_valid, y_train, y_valid = train_test_split(\n",
        "        X, y, test_size=valid_size, random_state=SEED, stratify=y\n",
        "    )\n",
        "\n",
        "    train_loader = DataLoader(TimeSeriesDataset(X_train, y_train), batch_size=batch_size, shuffle=True, drop_last=False)\n",
        "    valid_loader = DataLoader(TimeSeriesDataset(X_valid, y_valid), batch_size=batch_size, shuffle=False, drop_last=False)\n",
        "    return train_loader, valid_loader, (X_train, y_train, X_valid, y_valid)\n",
        "\n",
        "\n",
        "BATCH_SIZE = 64\n",
        "train_loader, valid_loader, (X_train_split, y_train_split, X_valid_split, y_valid_split) = create_dataloaders(X_train_np, y_train_idx, batch_size=BATCH_SIZE)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class RecurrentBackbone(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        input_size: int,\n",
        "        hidden_size: int = 128,\n",
        "        num_layers: int = 2,\n",
        "        dropout: float = 0.2,\n",
        "        rnn_type: str = 'lstm',\n",
        "        bidirectional: bool = True,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        rnn_cls = {\n",
        "            'rnn': nn.RNN,\n",
        "            'gru': nn.GRU,\n",
        "            'lstm': nn.LSTM,\n",
        "        }[rnn_type.lower()]\n",
        "        self.rnn_type = rnn_type.lower()\n",
        "        self.bidirectional = bidirectional\n",
        "        self.rnn = rnn_cls(\n",
        "            input_size=input_size,\n",
        "            hidden_size=hidden_size,\n",
        "            num_layers=num_layers,\n",
        "            dropout=dropout if num_layers > 1 else 0.0,\n",
        "            batch_first=True,\n",
        "            bidirectional=bidirectional,\n",
        "        )\n",
        "        proj_input = hidden_size * (2 if bidirectional else 1)\n",
        "        self.head = nn.Sequential(\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(proj_input, hidden_size),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(hidden_size, NUM_CLASSES),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out, _ = self.rnn(x)\n",
        "        # use last time-step hidden state\n",
        "        last = out[:, -1, :]\n",
        "        return self.head(last)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def accuracy_from_logits(logits: torch.Tensor, targets: torch.Tensor) -> float:\n",
        "    preds = torch.argmax(logits, dim=1)\n",
        "    return (preds == targets).float().mean().item()\n",
        "\n",
        "\n",
        "def train_one_epoch(model, loader, criterion, optimizer):\n",
        "    model.train()\n",
        "    total_loss, total_acc = 0.0, 0.0\n",
        "    for batch, (inputs, targets) in enumerate(loader):\n",
        "        inputs = inputs.to(DEVICE)\n",
        "        targets = targets.to(DEVICE)\n",
        "        optimizer.zero_grad()\n",
        "        logits = model(inputs)\n",
        "        loss = criterion(logits, targets)\n",
        "        loss.backward()\n",
        "        nn.utils.clip_grad_norm_(model.parameters(), max_norm=5.0)\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item() * inputs.size(0)\n",
        "        total_acc += accuracy_from_logits(logits.detach(), targets) * inputs.size(0)\n",
        "\n",
        "    total_loss /= len(loader.dataset)\n",
        "    total_acc /= len(loader.dataset)\n",
        "    return total_loss, total_acc\n",
        "\n",
        "\n",
        "def evaluate(model, loader, criterion):\n",
        "    model.eval()\n",
        "    total_loss, total_acc = 0.0, 0.0\n",
        "    preds, targets_all = [], []\n",
        "    with torch.no_grad():\n",
        "        for inputs, targets in loader:\n",
        "            inputs = inputs.to(DEVICE)\n",
        "            targets = targets.to(DEVICE)\n",
        "            logits = model(inputs)\n",
        "            loss = criterion(logits, targets)\n",
        "\n",
        "            total_loss += loss.item() * inputs.size(0)\n",
        "            total_acc += accuracy_from_logits(logits, targets) * inputs.size(0)\n",
        "\n",
        "            preds.append(torch.argmax(logits, dim=1).cpu())\n",
        "            targets_all.append(targets.cpu())\n",
        "\n",
        "    total_loss /= len(loader.dataset)\n",
        "    total_acc /= len(loader.dataset)\n",
        "    preds = torch.cat(preds)\n",
        "    targets_all = torch.cat(targets_all)\n",
        "    f1 = f1_score(targets_all, preds, average='macro')\n",
        "    return total_loss, total_acc, f1, preds.numpy(), targets_all.numpy()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train_model(\n",
        "    rnn_type: str = 'lstm',\n",
        "    hidden_size: int = 128,\n",
        "    num_layers: int = 2,\n",
        "    dropout: float = 0.3,\n",
        "    lr: float = 1e-3,\n",
        "    weight_decay: float = 1e-4,\n",
        "    epochs: int = 30,\n",
        "    batch_size: int = 64,\n",
        "):\n",
        "    train_loader, valid_loader, _ = create_dataloaders(X_train_np, y_train_idx, batch_size=batch_size)\n",
        "\n",
        "    model = RecurrentBackbone(\n",
        "        input_size=NUM_FEATURES,\n",
        "        hidden_size=hidden_size,\n",
        "        num_layers=num_layers,\n",
        "        dropout=dropout,\n",
        "        rnn_type=rnn_type,\n",
        "    ).to(DEVICE)\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
        "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=3, verbose=True)\n",
        "\n",
        "    history = {\n",
        "        'train_loss': [],\n",
        "        'train_acc': [],\n",
        "        'valid_loss': [],\n",
        "        'valid_acc': [],\n",
        "        'valid_f1': [],\n",
        "    }\n",
        "\n",
        "    best_f1 = -np.inf\n",
        "    best_state = None\n",
        "\n",
        "    for epoch in range(1, epochs + 1):\n",
        "        train_loss, train_acc = train_one_epoch(model, train_loader, criterion, optimizer)\n",
        "        valid_loss, valid_acc, valid_f1, preds, targets = evaluate(model, valid_loader, criterion)\n",
        "        scheduler.step(valid_f1)\n",
        "\n",
        "        history['train_loss'].append(train_loss)\n",
        "        history['train_acc'].append(train_acc)\n",
        "        history['valid_loss'].append(valid_loss)\n",
        "        history['valid_acc'].append(valid_acc)\n",
        "        history['valid_f1'].append(valid_f1)\n",
        "\n",
        "        if valid_f1 > best_f1:\n",
        "            best_f1 = valid_f1\n",
        "            best_state = {\n",
        "                'model_state': model.state_dict(),\n",
        "                'history': history.copy(),\n",
        "                'epoch': epoch,\n",
        "                'preds': preds,\n",
        "                'targets': targets,\n",
        "                'config': {\n",
        "                    'rnn_type': rnn_type,\n",
        "                    'hidden_size': hidden_size,\n",
        "                    'num_layers': num_layers,\n",
        "                    'dropout': dropout,\n",
        "                    'lr': lr,\n",
        "                    'weight_decay': weight_decay,\n",
        "                    'epochs': epochs,\n",
        "                    'batch_size': batch_size,\n",
        "                }\n",
        "            }\n",
        "            torch.save(best_state, OUTPUT_DIR / f'best_{rnn_type}.pt')\n",
        "\n",
        "        print(\n",
        "            f\"Epoch {epoch:02d} | Train Loss {train_loss:.4f} Acc {train_acc:.3f} | \"\n",
        "            f\"Valid Loss {valid_loss:.4f} Acc {valid_acc:.3f} F1 {valid_f1:.3f}\"\n",
        "        )\n",
        "\n",
        "    model.load_state_dict(best_state['model_state'])\n",
        "    return model, history, best_state\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example: train baseline LSTM (adjust hyperparameters as needed)\n",
        "lstm_model, lstm_history, lstm_state = train_model(\n",
        "    rnn_type='lstm',\n",
        "    hidden_size=192,\n",
        "    num_layers=2,\n",
        "    dropout=0.4,\n",
        "    lr=2e-3,\n",
        "    weight_decay=1e-4,\n",
        "    epochs=30,\n",
        "    batch_size=128,\n",
        ")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def plot_history(history: Dict[str, list]):\n",
        "    epochs = range(1, len(history['train_loss']) + 1)\n",
        "    fig, axs = plt.subplots(1, 3, figsize=(18, 4))\n",
        "\n",
        "    axs[0].plot(epochs, history['train_loss'], label='Train')\n",
        "    axs[0].plot(epochs, history['valid_loss'], label='Valid')\n",
        "    axs[0].set_title('Loss')\n",
        "    axs[0].set_xlabel('Epoch')\n",
        "    axs[0].legend()\n",
        "\n",
        "    axs[1].plot(epochs, history['train_acc'], label='Train')\n",
        "    axs[1].plot(epochs, history['valid_acc'], label='Valid')\n",
        "    axs[1].set_title('Accuracy')\n",
        "    axs[1].set_xlabel('Epoch')\n",
        "    axs[1].legend()\n",
        "\n",
        "    axs[2].plot(epochs, history['valid_f1'], label='Valid F1', color='green')\n",
        "    axs[2].set_title('Macro F1')\n",
        "    axs[2].set_xlabel('Epoch')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "plot_history(lstm_history)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "valid_loader_final = DataLoader(TimeSeriesDataset(X_valid_split, y_valid_split), batch_size=128, shuffle=False)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "_, _, f1_valid, preds_valid, targets_valid = evaluate(lstm_model, valid_loader_final, criterion)\n",
        "print(f'Validation macro F1: {f1_valid:.3f}')\n",
        "print(classification_report(targets_valid, preds_valid, target_names=[IDX2LABEL[i] for i in range(NUM_CLASSES)]))\n",
        "\n",
        "cf = confusion_matrix(targets_valid, preds_valid)\n",
        "plt.figure(figsize=(6, 5))\n",
        "sns.heatmap(cf, annot=True, fmt='d', cmap='Blues', xticklabels=[IDX2LABEL[i] for i in range(NUM_CLASSES)], yticklabels=[IDX2LABEL[i] for i in range(NUM_CLASSES)])\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('True')\n",
        "plt.title('Validation Confusion Matrix')\n",
        "plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "test_loader = DataLoader(TimeSeriesDataset(X_test_np), batch_size=256, shuffle=False)\n",
        "\n",
        "lstm_model.eval()\n",
        "test_preds = []\n",
        "with torch.no_grad():\n",
        "    for inputs in test_loader:\n",
        "        inputs = inputs.to(DEVICE)\n",
        "        logits = lstm_model(inputs)\n",
        "        test_preds.append(torch.argmax(logits, dim=1).cpu().numpy())\n",
        "\n",
        "test_preds = np.concatenate(test_preds)\n",
        "test_labels = [IDX2LABEL[idx] for idx in test_preds]\n",
        "\n",
        "submission = pd.DataFrame({\n",
        "    'sample_index': pd.unique(X_test_raw['sample_index']),\n",
        "    'label': test_labels,\n",
        "})\n",
        "submission.to_csv(OUTPUT_DIR / 'submission_lstm.csv', index=False)\n",
        "submission.head()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Next Steps\n",
        "\n",
        "- Tune hyperparameters (hidden size, depth, dropout, learning rate) and evaluate on validation splits.\n",
        "- Experiment with different recurrent cells by calling `train_model(rnn_type='gru')` or `train_model(rnn_type='rnn')`.\n",
        "- Implement k-fold cross-validation by adapting the `create_dataloaders` helper to loop over `StratifiedKFold` splits.\n",
        "- Add ensembling and/or temporal pooling strategies (attention, mean pooling across time) for further gains.\n",
        "- Document results inside this notebook so it is fully reproducible for the final submission package.\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
