{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d0iHUb-QTBGM"
      },
      "source": [
        "# üè¥‚Äç‚ò†Ô∏è AN2DL25 Challenge 1 ‚Äî Pirate Pain Classification\n",
        "\n",
        "This notebook implements a full deep-learning pipeline for multivariate time-series classification of the Pirate Pain dataset. It is inspired by the Lecture 4 notebook (`Timeseries Classification (1).ipynb`) but adapted for the competition setting, including data preparation, model training (RNN/GRU/LSTM variants), evaluation, and test-time inference.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 707,
      "metadata": {
        "id": "yD8eSdFgTBGP"
      },
      "outputs": [],
      "source": [
        "# %%capture\n",
        "# !pip install -q -r requirements.txt\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 708,
      "metadata": {
        "id": "TNB0xa3YTBGR"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import random\n",
        "import math\n",
        "import copy\n",
        "from pathlib import Path\n",
        "from typing import Tuple, Dict, Optional, List\n",
        "from datetime import datetime\n",
        "from itertools import product\n",
        "import inspect\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.metrics import (\n",
        "    f1_score,\n",
        "    classification_report,\n",
        "    confusion_matrix,\n",
        "    precision_recall_fscore_support,\n",
        ")\n",
        "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "import torch.nn.functional as F\n",
        "\n",
        "try:\n",
        "    from torch.amp import autocast, GradScaler\n",
        "except ImportError:  # pragma: no cover\n",
        "    from torch.cuda.amp import autocast, GradScaler\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 709,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JReu2QJbTBGR",
        "outputId": "66cb8304-c05d-4800-ab37-2d85638d4fa9"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<>:11: SyntaxWarning: invalid escape sequence '\\ '\n",
            "<>:11: SyntaxWarning: invalid escape sequence '\\ '\n",
            "/tmp/ipython-input-2583850707.py:11: SyntaxWarning: invalid escape sequence '\\ '\n",
            "  BASE_DIR = Path('/content/drive/MyDrive/[2025-2026]\\ A2NDL/Challenge')\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Running in Colab: True\n",
            "Device: cuda\n",
            "Data dir: /content/drive/MyDrive/[2025-2026]\\ A2NDL/Challenge/data\n",
            "Output dir: /content/drive/MyDrive/[2025-2026]\\ A2NDL/Challenge/outputs\n"
          ]
        }
      ],
      "source": [
        "SEED = 42\n",
        "\n",
        "try:\n",
        "    from google.colab import drive  # type: ignore\n",
        "    IN_COLAB = True\n",
        "except ImportError:  # pragma: no cover\n",
        "    IN_COLAB = False\n",
        "\n",
        "if IN_COLAB:\n",
        "    drive.mount('/content/drive', force_remount=False)\n",
        "    BASE_DIR = Path('/content/drive/MyDrive/[2025-2026]\\ A2NDL/Challenge')\n",
        "else:\n",
        "    BASE_DIR = Path('/Users/md101ta/Desktop/Pirates')\n",
        "\n",
        "DATA_DIR = (BASE_DIR / 'data').resolve()\n",
        "OUTPUT_DIR = (BASE_DIR / 'outputs').resolve()\n",
        "OUTPUT_DIR.mkdir(exist_ok=True, parents=True)\n",
        "\n",
        "# Reproducibility\n",
        "os.environ['PYTHONHASHSEED'] = str(SEED)\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(SEED)\n",
        "    DEVICE = torch.device('cuda')\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "else:\n",
        "    DEVICE = torch.device('cpu')\n",
        "\n",
        "print(f'Running in Colab: {IN_COLAB}')\n",
        "print(f'Device: {DEVICE}')\n",
        "print(f'Data dir: {DATA_DIR}')\n",
        "print(f'Output dir: {OUTPUT_DIR}')\n",
        "\n",
        "_AUTocast_params = inspect.signature(autocast).parameters\n",
        "_GRADSCALER_PARAMS = inspect.signature(GradScaler).parameters\n",
        "\n",
        "def autocast_context():\n",
        "    enabled = DEVICE.type == 'cuda'\n",
        "    if 'device_type' in _AUTocast_params:\n",
        "        return autocast(device_type=DEVICE.type, enabled=enabled)\n",
        "    if 'device' in _AUTocast_params:\n",
        "        return autocast(DEVICE.type, enabled=enabled)\n",
        "    # fallback to legacy signature (enabled only)\n",
        "    return autocast(enabled=enabled)\n",
        "\n",
        "\n",
        "def create_grad_scaler():\n",
        "    enabled = DEVICE.type == 'cuda'\n",
        "    if 'device_type' in _GRADSCALER_PARAMS:\n",
        "        return GradScaler(device_type=DEVICE.type, enabled=enabled)\n",
        "    return GradScaler(enabled=enabled)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 710,
      "metadata": {
        "id": "Us5CnwHwTBGS"
      },
      "outputs": [],
      "source": [
        "LOG_DIR = (OUTPUT_DIR / 'logs').resolve()\n",
        "CHECKPOINT_DIR = (OUTPUT_DIR / 'checkpoints').resolve()\n",
        "LOG_DIR.mkdir(exist_ok=True, parents=True)\n",
        "CHECKPOINT_DIR.mkdir(exist_ok=True, parents=True)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 711,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "-oigFvDtTBGS",
        "outputId": "128ba127-2cb6-447b-ff18-bcc9cfbdab45"
      },
      "outputs": [
        {
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/content/drive/MyDrive/[2025-2026] AN2DL/Challenge/pirate_pain_train.csv'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2697538878.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;31m# Carica i dati\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0mX_train_raw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test_raw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDATA_DIR\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_raw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test_raw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-2697538878.py\u001b[0m in \u001b[0;36mload_data\u001b[0;34m(data_dir)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mload_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_dir\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mPath\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTuple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mX_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_dir\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m'pirate_pain_train.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0my_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_dir\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m'pirate_pain_train_labels.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mX_test\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_dir\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m'pirate_pain_test.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1880\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1881\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    874\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/drive/MyDrive/[2025-2026] AN2DL/Challenge/pirate_pain_train.csv'"
          ]
        }
      ],
      "source": [
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "from typing import Tuple\n",
        "\n",
        "# Percorso al dataset su Google Drive\n",
        "DATA_DIR = Path('/content/drive/MyDrive/[2025-2026] AN2DL/Challenge')\n",
        "\n",
        "def load_data(data_dir: Path) -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n",
        "    X_train = pd.read_csv(data_dir / 'pirate_pain_train.csv')\n",
        "    y_train = pd.read_csv(data_dir / 'pirate_pain_train_labels.csv')\n",
        "    X_test  = pd.read_csv(data_dir / 'pirate_pain_test.csv')\n",
        "    return X_train, y_train, X_test\n",
        "\n",
        "# Carica i dati\n",
        "X_train_raw, y_train, X_test_raw = load_data(DATA_DIR)\n",
        "print(X_train_raw.shape, y_train.shape, X_test_raw.shape)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G1_qoIZ2TBGT"
      },
      "outputs": [],
      "source": [
        "CATEGORICAL_COLUMNS = ['n_legs', 'n_hands', 'n_eyes']\n",
        "CATEGORY_MAPPINGS: Dict[str, Dict[str, int]] = {}\n",
        "\n",
        "for col in CATEGORICAL_COLUMNS:\n",
        "    uniques = pd.concat([X_train_raw[col], X_test_raw[col]]).dropna().unique()\n",
        "    mapping = {value: idx for idx, value in enumerate(sorted(uniques))}\n",
        "    CATEGORY_MAPPINGS[col] = mapping\n",
        "    X_train_raw[col] = X_train_raw[col].map(mapping).astype(np.int32)\n",
        "    X_test_raw[col] = X_test_raw[col].map(mapping).astype(np.int32)\n",
        "\n",
        "FEATURE_COLUMNS = [col for col in X_train_raw.columns if col not in ['sample_index', 'time']]\n",
        "LOW_VARIANCE_THRESHOLD = 2e-3\n",
        "feature_std = X_train_raw[FEATURE_COLUMNS].std()\n",
        "low_variance_features = feature_std[feature_std <= LOW_VARIANCE_THRESHOLD].index.tolist()\n",
        "\n",
        "LOW_MAG_THRESHOLD = 1e-3\n",
        "feature_abs_max = X_train_raw[FEATURE_COLUMNS].abs().max()\n",
        "low_magnitude_features = feature_abs_max[feature_abs_max <= LOW_MAG_THRESHOLD].index.tolist()\n",
        "\n",
        "features_to_drop = sorted(set(low_variance_features) | set(low_magnitude_features))\n",
        "if features_to_drop:\n",
        "    print(\n",
        "        \"Removing features due to low variance / magnitude:\",\n",
        "        features_to_drop,\n",
        "    )\n",
        "    X_train_raw = X_train_raw.drop(columns=features_to_drop)\n",
        "    X_test_raw = X_test_raw.drop(columns=features_to_drop)\n",
        "    FEATURE_COLUMNS = [col for col in FEATURE_COLUMNS if col not in features_to_drop]\n",
        "else:\n",
        "    print(\n",
        "        f'No features met the pruning criteria (variance ‚â§{LOW_VARIANCE_THRESHOLD:.1e} and |max| ‚â§{LOW_MAG_THRESHOLD:.1e}).'\n",
        "    )\n",
        "\n",
        "FULL_TIME_STEPS = X_train_raw['time'].nunique()\n",
        "WINDOW_SIZE = 25\n",
        "WINDOW_STRIDE = 15\n",
        "EVAL_WINDOW_SIZE = WINDOW_SIZE\n",
        "EVAL_WINDOW_STRIDE = WINDOW_STRIDE\n",
        "EVAL_AGGREGATION = 'logsumexp'\n",
        "if WINDOW_SIZE > FULL_TIME_STEPS:\n",
        "    raise ValueError(\n",
        "        f'Requested window size ({WINDOW_SIZE}) exceeds the available time steps ({FULL_TIME_STEPS}).'\n",
        "    )\n",
        "TIME_STEPS = FULL_TIME_STEPS\n",
        "\n",
        "TIME_DIVISOR = max(FULL_TIME_STEPS - 1, 1)\n",
        "TIME_FEATURE_COLUMNS = ['time_fraction', 'time_sin', 'time_cos', 'time_is_start', 'time_is_end']\n",
        "for df in (X_train_raw, X_test_raw):\n",
        "    frac = (df['time'] / TIME_DIVISOR).astype(np.float32)\n",
        "    df['time_fraction'] = frac\n",
        "    df['time_sin'] = np.sin(2 * np.pi * frac).astype(np.float32)\n",
        "    df['time_cos'] = np.cos(2 * np.pi * frac).astype(np.float32)\n",
        "    df['time_is_start'] = (df['time'] == 0).astype(np.float32)\n",
        "    df['time_is_end'] = (df['time'] == FULL_TIME_STEPS - 1).astype(np.float32)\n",
        "\n",
        "for col in TIME_FEATURE_COLUMNS:\n",
        "    if col not in FEATURE_COLUMNS:\n",
        "        FEATURE_COLUMNS.append(col)\n",
        "\n",
        "NUM_FEATURES = len(FEATURE_COLUMNS)\n",
        "NUM_CLASSES = y_train['label'].nunique()\n",
        "print(\n",
        "    f'Time steps (raw): {FULL_TIME_STEPS} | Window size: {WINDOW_SIZE} | Features: {NUM_FEATURES} | Classes: {NUM_CLASSES}'\n",
        ")\n",
        "print('Category mappings:', CATEGORY_MAPPINGS)\n",
        "\n",
        "y_train['label'].value_counts().plot(kind='bar', title='Class distribution')\n",
        "plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m7ieHeHyTBGU"
      },
      "outputs": [],
      "source": [
        "LABEL2IDX = {label: idx for idx, label in enumerate(sorted(y_train['label'].unique()))}\n",
        "IDX2LABEL = {idx: label for label, idx in LABEL2IDX.items()}\n",
        "print('Label mapping:', LABEL2IDX)\n",
        "\n",
        "HIGH_PAIN_LABELS = [label for label in LABEL2IDX if 'high' in label.lower()]\n",
        "LOW_PAIN_LABELS = [label for label in LABEL2IDX if 'low' in label.lower()]\n",
        "NO_PAIN_LABELS = [label for label in LABEL2IDX if 'no' in label.lower()]\n",
        "HIGH_PAIN_IDX = {LABEL2IDX[label] for label in HIGH_PAIN_LABELS}\n",
        "LOW_PAIN_IDX = {LABEL2IDX[label] for label in LOW_PAIN_LABELS}\n",
        "NO_PAIN_IDX = {LABEL2IDX[label] for label in NO_PAIN_LABELS}\n",
        "print('High-pain classes for augmentation:', HIGH_PAIN_LABELS or 'None')\n",
        "print('Low-pain classes for augmentation:', LOW_PAIN_LABELS or 'None')\n",
        "print('No-pain classes for augmentation:', NO_PAIN_IDX or 'None')\n",
        "\n",
        "HIGH_PAIN_OVERSAMPLE = 2 if HIGH_PAIN_IDX else 1\n",
        "LOW_PAIN_OVERSAMPLE = 2 if LOW_PAIN_IDX else 1\n",
        "HIGH_PAIN_WEIGHT_SCALE = 0.6\n",
        "LOW_PAIN_WEIGHT_SCALE = 0.7\n",
        "NO_PAIN_WEIGHT_SCALE = 0.6\n",
        "HIGH_PAIN_AUGMENTATION_PARAMS = {\n",
        "    'jitter_std': 0.03,\n",
        "    'scale_range': (0.90, 1.10),\n",
        "    'time_mask_prob': 0.30,\n",
        "    'time_mask_ratio': 0.12,\n",
        "    'time_shift_range': 4,\n",
        "    'time_flip_prob': 0.10,\n",
        "}\n",
        "LOW_PAIN_AUGMENTATION_PARAMS = {\n",
        "    'jitter_std': 0.02,\n",
        "    'scale_range': (0.95, 1.05),\n",
        "    'time_mask_prob': 0.20,\n",
        "    'time_mask_ratio': 0.08,\n",
        "    'time_shift_range': 3,\n",
        "    'time_flip_prob': 0.05,\n",
        "}\n",
        "AUGMENTATION_PARAMS = HIGH_PAIN_AUGMENTATION_PARAMS\n",
        "\n",
        "\n",
        "def pivot_timeseries(df: pd.DataFrame) -> np.ndarray:\n",
        "    pivoted = (\n",
        "        df.pivot(index='sample_index', columns='time', values=FEATURE_COLUMNS)\n",
        "          .sort_index(axis=0)\n",
        "          .sort_index(axis=1, level=1)\n",
        "    )\n",
        "    data = pivoted.to_numpy().reshape(-1, TIME_STEPS, NUM_FEATURES)\n",
        "    return data\n",
        "\n",
        "\n",
        "X_train_np = pivot_timeseries(X_train_raw)\n",
        "X_test_np = pivot_timeseries(X_test_raw)\n",
        "y_train_idx = y_train.set_index('sample_index').loc[pd.unique(X_train_raw['sample_index'])]['label'].map(LABEL2IDX).to_numpy()\n",
        "\n",
        "print(X_train_np.shape, y_train_idx.shape, X_test_np.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "utrbBxCPTBGU"
      },
      "outputs": [],
      "source": [
        "class_counts = np.bincount(y_train_idx, minlength=NUM_CLASSES)\n",
        "class_weights = class_counts.sum() / (class_counts + 1e-6)\n",
        "class_weights = class_weights / class_weights.mean()\n",
        "if HIGH_PAIN_IDX:\n",
        "    for idx in HIGH_PAIN_IDX:\n",
        "        class_weights[idx] *= HIGH_PAIN_WEIGHT_SCALE\n",
        "if LOW_PAIN_IDX:\n",
        "    for idx in LOW_PAIN_IDX:\n",
        "        class_weights[idx] *= LOW_PAIN_WEIGHT_SCALE\n",
        "if NO_PAIN_IDX:\n",
        "    for idx in NO_PAIN_IDX:\n",
        "        class_weights[idx] *= NO_PAIN_WEIGHT_SCALE\n",
        "CLASS_COUNTS = class_counts\n",
        "CLASS_WEIGHTS = torch.tensor(class_weights, dtype=torch.float32)\n",
        "print('Class counts:', class_counts)\n",
        "print('Class weights (normalized):', np.round(class_weights, 3))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ufAfwBqxTBGV"
      },
      "outputs": [],
      "source": [
        "def compute_normalization_stats(data: np.ndarray, feature_columns: List[str]) -> Tuple[np.ndarray, np.ndarray, Dict[str, np.ndarray]]:\n",
        "    \"\"\"\n",
        "    Normalizza le features per gruppi:\n",
        "    - Joints: normalizzazione z-score (sono valori continui)\n",
        "    - Categoriche (n_legs, n_hands, n_eyes): NON normalizzare (gi√† 0/1)\n",
        "    - Pain survey: normalizzazione z-score\n",
        "    - Time features: gi√† normalizzate (0-1 o sin/cos)\n",
        "    \"\"\"\n",
        "    # data shape: (N, T, F)\n",
        "    data_flat = data.reshape(-1, NUM_FEATURES)\n",
        "    mean = np.zeros(NUM_FEATURES, dtype=np.float32)\n",
        "    std = np.ones(NUM_FEATURES, dtype=np.float32)\n",
        "    \n",
        "    # Identifica i gruppi di features\n",
        "    categorical_features = ['n_legs', 'n_hands', 'n_eyes']\n",
        "    time_features = ['time_fraction', 'time_sin', 'time_cos', 'time_is_start', 'time_is_end']\n",
        "    \n",
        "    # Indici delle features da normalizzare (tutto tranne categoriche e time)\n",
        "    normalize_indices = []\n",
        "    categorical_indices = []\n",
        "    time_indices = []\n",
        "    \n",
        "    for idx, col in enumerate(feature_columns):\n",
        "        if col in categorical_features:\n",
        "            categorical_indices.append(idx)\n",
        "        elif col in time_features:\n",
        "            time_indices.append(idx)\n",
        "        else:\n",
        "            normalize_indices.append(idx)\n",
        "    \n",
        "    # Normalizza solo le features continue (joints, pain survey, etc.)\n",
        "    if normalize_indices:\n",
        "        mean[normalize_indices] = data_flat[:, normalize_indices].mean(axis=0)\n",
        "        std[normalize_indices] = data_flat[:, normalize_indices].std(axis=0) + 1e-6\n",
        "    \n",
        "    # Le categoriche e time features non vengono normalizzate (std=1, mean=0 gi√† impostati)\n",
        "    feature_groups = {\n",
        "        'normalize': normalize_indices,\n",
        "        'categorical': categorical_indices,\n",
        "        'time': time_indices\n",
        "    }\n",
        "    \n",
        "    return mean, std, feature_groups\n",
        "\n",
        "\n",
        "def normalize(data: np.ndarray, mean: np.ndarray, std: np.ndarray) -> np.ndarray:\n",
        "    return (data - mean) / std\n",
        "\n",
        "\n",
        "feat_mean, feat_std, feature_groups = compute_normalization_stats(X_train_np, FEATURE_COLUMNS)\n",
        "print(f\"Features normalizzate (joints, pain survey): {len(feature_groups['normalize'])}\")\n",
        "print(f\"Features categoriche (n_legs, n_hands, n_eyes): {len(feature_groups['categorical'])}\")\n",
        "print(f\"Features temporali: {len(feature_groups['time'])}\")\n",
        "\n",
        "X_train_np = normalize(X_train_np, feat_mean, feat_std)\n",
        "X_test_np = normalize(X_test_np, feat_mean, feat_std)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cPxW7DukTBGV"
      },
      "outputs": [],
      "source": [
        "def make_dataloader_from_arrays(\n",
        "    X: np.ndarray,\n",
        "    y: Optional[np.ndarray],\n",
        "    batch_size: int,\n",
        "    shuffle: bool,\n",
        "    mode: str,\n",
        "    oversample_factor: int = 1,\n",
        "    augmentation_params: Optional[Dict[str, float]] = None,\n",
        "    window_size: Optional[int] = None,\n",
        "    window_stride: Optional[int] = None,\n",
        ") -> DataLoader:\n",
        "    dataset = TimeSeriesDataset(\n",
        "        X,\n",
        "        y,\n",
        "        window_size=window_size,\n",
        "        window_stride=window_stride,\n",
        "        mode=mode,\n",
        "        high_pain_targets=HIGH_PAIN_IDX,\n",
        "        oversample_factor=oversample_factor,\n",
        "        augmentation_params=augmentation_params,\n",
        "    )\n",
        "    return DataLoader(dataset, batch_size=batch_size, shuffle=shuffle, drop_last=False)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ruF9R0TzTBGV"
      },
      "outputs": [],
      "source": [
        "class TimeSeriesDataset(Dataset):\n",
        "    def __init__(\n",
        "        self,\n",
        "        data: np.ndarray,\n",
        "        labels: Optional[np.ndarray] = None,\n",
        "        *,\n",
        "        window_size: Optional[int] = None,\n",
        "        window_stride: Optional[int] = None,\n",
        "        mode: str = 'train',\n",
        "        high_pain_targets: Optional[set] = None,\n",
        "        oversample_factor: int = 1,\n",
        "        augmentation_params: Optional[Dict[str, float]] = None,\n",
        "    ):\n",
        "        self.data = torch.tensor(data, dtype=torch.float32)\n",
        "        self.labels = None if labels is None else torch.tensor(labels, dtype=torch.long)\n",
        "        self.window_size = window_size\n",
        "        self.window_stride = max(1, window_stride or 1)\n",
        "        self.mode = mode\n",
        "        self.high_pain_targets = set(high_pain_targets or [])\n",
        "        self.augmentation_params = augmentation_params or {}\n",
        "        self.time_steps = self.data.shape[1]\n",
        "\n",
        "        if mode not in {'train', 'valid', 'test'}:\n",
        "            raise ValueError(f\"Unsupported mode '{mode}'. Use 'train', 'valid', or 'test'.\")\n",
        "        if self.window_size is not None and self.window_size > self.time_steps:\n",
        "            raise ValueError(\n",
        "                f'Window size {self.window_size} exceeds series length {self.time_steps}.'\n",
        "            )\n",
        "\n",
        "        self.indices = self._build_indices(max(1, oversample_factor))\n",
        "\n",
        "    def _build_indices(self, oversample_factor: int) -> List[int]:\n",
        "        base_indices = list(range(len(self.data)))\n",
        "        if (\n",
        "            self.labels is None\n",
        "            or not self.high_pain_targets\n",
        "            or oversample_factor <= 1\n",
        "        ):\n",
        "            return base_indices\n",
        "\n",
        "        high_indices = [idx for idx in base_indices if int(self.labels[idx].item()) in self.high_pain_targets]\n",
        "        if not high_indices:\n",
        "            return base_indices\n",
        "\n",
        "        extra_indices: List[int] = []\n",
        "        for _ in range(oversample_factor - 1):\n",
        "            extra_indices.extend(high_indices)\n",
        "        return base_indices + extra_indices\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        return len(self.indices)\n",
        "\n",
        "    def __getitem__(self, idx: int):\n",
        "        real_idx = self.indices[idx]\n",
        "        series = self.data[real_idx]\n",
        "        label = None if self.labels is None else self.labels[real_idx]\n",
        "\n",
        "        if self.window_size is not None and self.mode == 'train':\n",
        "            series = self._select_window(series, random_selection=True)\n",
        "        elif self.window_size is not None and self.mode != 'train':\n",
        "            series = self._select_window(series, random_selection=False)\n",
        "        else:\n",
        "            series = series.clone()\n",
        "\n",
        "        if (\n",
        "            self.mode == 'train'\n",
        "            and label is not None\n",
        "            and int(label.item()) in self.high_pain_targets\n",
        "            and self.augmentation_params\n",
        "        ):\n",
        "            series = self._augment(series)\n",
        "\n",
        "        if label is None:\n",
        "            return series\n",
        "        return series, label\n",
        "\n",
        "    def _select_window(self, series: torch.Tensor, *, random_selection: bool) -> torch.Tensor:\n",
        "        window = self.window_size or series.shape[0]\n",
        "        if window >= series.shape[0]:\n",
        "            return series.clone()\n",
        "\n",
        "        max_start = series.shape[0] - window\n",
        "        if max_start <= 0:\n",
        "            start = 0\n",
        "        else:\n",
        "            stride = max(1, self.window_stride)\n",
        "            positions = list(range(0, max_start + 1, stride))\n",
        "            if positions[-1] != max_start:\n",
        "                positions.append(max_start)\n",
        "            if random_selection:\n",
        "                start = random.choice(positions)\n",
        "            else:\n",
        "                start = positions[len(positions) // 2]\n",
        "        end = start + window\n",
        "        return series[start:end].clone()\n",
        "\n",
        "    def _augment(self, series: torch.Tensor) -> torch.Tensor:\n",
        "        augmented = series.clone()\n",
        "        jitter_std = float(self.augmentation_params.get('jitter_std', 0.0))\n",
        "        if jitter_std > 0:\n",
        "            augmented = augmented + torch.randn_like(augmented) * jitter_std\n",
        "\n",
        "        scale_range = self.augmentation_params.get('scale_range')\n",
        "        if scale_range:\n",
        "            low, high = scale_range\n",
        "            scale = random.uniform(low, high)\n",
        "            augmented = augmented * scale\n",
        "\n",
        "        time_shift_range = int(self.augmentation_params.get('time_shift_range', 0))\n",
        "        if time_shift_range > 0:\n",
        "            shift = random.randint(-time_shift_range, time_shift_range)\n",
        "            if shift != 0:\n",
        "                augmented = torch.roll(augmented, shifts=shift, dims=0)\n",
        "\n",
        "        if random.random() < float(self.augmentation_params.get('time_flip_prob', 0.0)):\n",
        "            augmented = torch.flip(augmented, dims=[0])\n",
        "\n",
        "        time_mask_prob = float(self.augmentation_params.get('time_mask_prob', 0.0))\n",
        "        time_mask_ratio = float(self.augmentation_params.get('time_mask_ratio', 0.0))\n",
        "        if time_mask_prob > 0 and time_mask_ratio > 0 and random.random() < time_mask_prob:\n",
        "            mask_len = max(1, int(augmented.shape[0] * time_mask_ratio))\n",
        "            if mask_len < augmented.shape[0]:\n",
        "                start = random.randint(0, augmented.shape[0] - mask_len)\n",
        "                augmented[start:start + mask_len] = 0.0\n",
        "\n",
        "        return augmented\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sez_5vaUTBGW"
      },
      "outputs": [],
      "source": [
        "class FocalLoss(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        weight: Optional[torch.Tensor] = None,\n",
        "        gamma: float = 2.0,\n",
        "        reduction: str = 'mean',\n",
        "    ):\n",
        "        super().__init__()\n",
        "        if reduction not in {'none', 'mean', 'sum'}:\n",
        "            raise ValueError(f\"Unsupported reduction '{reduction}'.\")\n",
        "        if weight is not None:\n",
        "            self.register_buffer('weight', weight)\n",
        "            self._use_weight = True\n",
        "        else:\n",
        "            self.register_buffer('weight', torch.tensor([], dtype=torch.float32), persistent=False)\n",
        "            self._use_weight = False\n",
        "        self.gamma = gamma\n",
        "        self.reduction = reduction\n",
        "\n",
        "    def forward(self, logits: torch.Tensor, targets: torch.Tensor) -> torch.Tensor:\n",
        "        if logits.shape[0] != targets.shape[0]:\n",
        "            raise ValueError('Logits and targets must have matching batch dimension.')\n",
        "        targets = targets.long().unsqueeze(1)\n",
        "        log_probs = F.log_softmax(logits, dim=-1)\n",
        "        probs = log_probs.exp()\n",
        "        log_pt = log_probs.gather(1, targets)\n",
        "        pt = probs.gather(1, targets).clamp(min=1e-6, max=1.0)\n",
        "        focal_factor = (1.0 - pt) ** self.gamma\n",
        "        loss = -focal_factor * log_pt\n",
        "        if self._use_weight:\n",
        "            class_weights = self.weight[targets.squeeze(1)]\n",
        "            loss = loss * class_weights.unsqueeze(1)\n",
        "        loss = loss.squeeze(1)\n",
        "        if self.reduction == 'mean':\n",
        "            return loss.mean()\n",
        "        if self.reduction == 'sum':\n",
        "            return loss.sum()\n",
        "        return loss\n",
        "\n",
        "\n",
        "def build_loss_fn(config: Dict) -> nn.Module:\n",
        "    loss_type = config.get('loss_type', 'focal')\n",
        "    use_class_weights = config.get('use_class_weights', True)\n",
        "    weight_tensor = None\n",
        "    if use_class_weights and 'CLASS_WEIGHTS' in globals():\n",
        "        weight_tensor = CLASS_WEIGHTS.to(DEVICE)\n",
        "    if loss_type == 'focal':\n",
        "        gamma = config.get('focal_gamma', 2.0)\n",
        "        return FocalLoss(weight=weight_tensor, gamma=gamma)\n",
        "    if loss_type == 'weighted_ce':\n",
        "        return nn.CrossEntropyLoss(weight=weight_tensor)\n",
        "    if loss_type == 'cross_entropy':\n",
        "        return nn.CrossEntropyLoss()\n",
        "    raise ValueError(f\"Unsupported loss_type '{loss_type}'.\")\n",
        "\n",
        "\n",
        "def extract_sliding_windows(\n",
        "    series: torch.Tensor,\n",
        "    window_size: Optional[int],\n",
        "    stride: Optional[int],\n",
        ") -> torch.Tensor:\n",
        "    if window_size is None or window_size >= series.shape[0]:\n",
        "        return series.unsqueeze(0)\n",
        "    stride = max(1, stride or 1)\n",
        "    length = series.shape[0]\n",
        "    max_start = max(0, length - window_size)\n",
        "    positions = list(range(0, max_start + 1, stride))\n",
        "    if positions[-1] != max_start:\n",
        "        positions.append(max_start)\n",
        "    windows = [series[start:start + window_size] for start in positions]\n",
        "    return torch.stack(windows, dim=0)\n",
        "\n",
        "\n",
        "def forward_with_sliding_windows(\n",
        "    model: nn.Module,\n",
        "    inputs: torch.Tensor,\n",
        "    window_size: Optional[int],\n",
        "    stride: Optional[int],\n",
        "    aggregation: str = 'max',\n",
        ") -> torch.Tensor:\n",
        "    if window_size is None or window_size >= inputs.shape[1]:\n",
        "        return model(inputs)\n",
        "\n",
        "    all_windows: List[torch.Tensor] = []\n",
        "    window_counts: List[int] = []\n",
        "    for sample in inputs:\n",
        "        sample_windows = extract_sliding_windows(sample, window_size, stride)\n",
        "        all_windows.append(sample_windows)\n",
        "        window_counts.append(sample_windows.shape[0])\n",
        "\n",
        "    stacked_windows = torch.cat(all_windows, dim=0)\n",
        "    logits = model(stacked_windows)\n",
        "    chunks = logits.split(window_counts, dim=0)\n",
        "\n",
        "    aggregated_logits: List[torch.Tensor] = []\n",
        "    aggregation = aggregation.lower()\n",
        "    for chunk in chunks:\n",
        "        if aggregation == 'max':\n",
        "            aggregated_logits.append(chunk.max(dim=0).values)\n",
        "        elif aggregation == 'mean':\n",
        "            aggregated_logits.append(chunk.mean(dim=0))\n",
        "        elif aggregation == 'logsumexp':\n",
        "            aggregated_logits.append(torch.logsumexp(chunk, dim=0))\n",
        "        else:\n",
        "            raise ValueError(f\"Unsupported aggregation '{aggregation}'.\")\n",
        "\n",
        "    return torch.stack(aggregated_logits, dim=0)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hHmie09aTBGW"
      },
      "outputs": [],
      "source": [
        "def create_dataloaders(\n",
        "    X: np.ndarray,\n",
        "    y: np.ndarray,\n",
        "    valid_size: float = 0.2,\n",
        "    batch_size: int = 128,\n",
        "    oversample_factor: int = HIGH_PAIN_OVERSAMPLE,\n",
        "    window_size: Optional[int] = WINDOW_SIZE,\n",
        "    window_stride: Optional[int] = WINDOW_STRIDE,\n",
        "    augmentation_params: Optional[Dict[str, float]] = AUGMENTATION_PARAMS,\n",
        "):\n",
        "    X_train, X_valid, y_train, y_valid = train_test_split(\n",
        "        X, y, test_size=valid_size, random_state=SEED, stratify=y\n",
        "    )\n",
        "\n",
        "    train_loader = make_dataloader_from_arrays(\n",
        "        X_train,\n",
        "        y_train,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=True,\n",
        "        mode='train',\n",
        "        oversample_factor=oversample_factor,\n",
        "        augmentation_params=augmentation_params,\n",
        "        window_size=window_size,\n",
        "        window_stride=window_stride,\n",
        "    )\n",
        "    valid_loader = make_dataloader_from_arrays(\n",
        "        X_valid,\n",
        "        y_valid,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=False,\n",
        "        mode='valid',\n",
        "        window_size=None,\n",
        "        window_stride=None,\n",
        "    )\n",
        "    return train_loader, valid_loader, (X_train, y_train, X_valid, y_valid)\n",
        "\n",
        "\n",
        "BATCH_SIZE = 128\n",
        "train_loader, valid_loader, (X_train_split, y_train_split, X_valid_split, y_valid_split) = create_dataloaders(\n",
        "    X_train_np,\n",
        "    y_train_idx,\n",
        "    batch_size=BATCH_SIZE,\n",
        ")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EVRDa4oMTBGW"
      },
      "outputs": [],
      "source": [
        "class RecurrentBackbone(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        input_size: int,\n",
        "        hidden_size: int = 128,\n",
        "        num_layers: int = 2,\n",
        "        dropout: float = 0.2,\n",
        "        rnn_type: str = 'lstm',\n",
        "        bidirectional: bool = True,\n",
        "        conv_layers: int = 0,\n",
        "        conv_channels: Optional[int] = None,\n",
        "        conv_kernel_size: int = 3,\n",
        "        conv_dropout: float = 0.1,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        if conv_kernel_size % 2 == 0:\n",
        "            raise ValueError('conv_kernel_size should be odd to preserve temporal dimension.')\n",
        "        rnn_cls = {\n",
        "            'rnn': nn.RNN,\n",
        "            'gru': nn.GRU,\n",
        "            'lstm': nn.LSTM,\n",
        "        }[rnn_type.lower()]\n",
        "        self.rnn_type = rnn_type.lower()\n",
        "        self.bidirectional = bidirectional\n",
        "\n",
        "        self.conv_layers = conv_layers\n",
        "        self.conv = None\n",
        "        rnn_input_size = input_size\n",
        "        if conv_layers > 0:\n",
        "            conv_blocks: List[nn.Module] = []\n",
        "            in_channels = input_size\n",
        "            out_channels = conv_channels or input_size\n",
        "            for layer_idx in range(conv_layers):\n",
        "                conv_blocks.append(\n",
        "                    nn.Conv1d(\n",
        "                        in_channels,\n",
        "                        out_channels,\n",
        "                        kernel_size=conv_kernel_size,\n",
        "                        padding=conv_kernel_size // 2,\n",
        "                        bias=False,\n",
        "                    )\n",
        "                )\n",
        "                conv_blocks.append(nn.BatchNorm1d(out_channels))\n",
        "                conv_blocks.append(nn.ReLU())\n",
        "                if conv_dropout > 0:\n",
        "                    conv_blocks.append(nn.Dropout(conv_dropout))\n",
        "                in_channels = out_channels\n",
        "            self.conv = nn.Sequential(*conv_blocks)\n",
        "            rnn_input_size = out_channels\n",
        "\n",
        "        self.rnn = rnn_cls(\n",
        "            input_size=rnn_input_size,\n",
        "            hidden_size=hidden_size,\n",
        "            num_layers=num_layers,\n",
        "            dropout=dropout if num_layers > 1 else 0.0,\n",
        "            batch_first=True,\n",
        "            bidirectional=bidirectional,\n",
        "        )\n",
        "        proj_input = hidden_size * (2 if bidirectional else 1)\n",
        "        self.head = nn.Sequential(\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(proj_input, hidden_size),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(hidden_size, NUM_CLASSES),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        if self.conv is not None:\n",
        "            # (batch, time, feature) -> (batch, feature, time)\n",
        "            x = x.transpose(1, 2)\n",
        "            x = self.conv(x)\n",
        "            x = x.transpose(1, 2)\n",
        "        out, _ = self.rnn(x)\n",
        "        # use last time-step hidden state\n",
        "        last = out[:, -1, :]\n",
        "        return self.head(last)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uFlPWsSPTBGX"
      },
      "outputs": [],
      "source": [
        "def compute_classification_metrics(preds: np.ndarray, targets: np.ndarray) -> Dict[str, float]:\n",
        "    accuracy = float((preds == targets).mean())\n",
        "    precision, recall, f1, _ = precision_recall_fscore_support(\n",
        "        targets,\n",
        "        preds,\n",
        "        average='macro',\n",
        "        zero_division=0,\n",
        "    )\n",
        "    return {\n",
        "        'accuracy': accuracy,\n",
        "        'precision': float(precision),\n",
        "        'recall': float(recall),\n",
        "        'f1': float(f1),\n",
        "    }\n",
        "\n",
        "\n",
        "def train_one_epoch(\n",
        "    model: nn.Module,\n",
        "    loader: DataLoader,\n",
        "    criterion: nn.Module,\n",
        "    optimizer: torch.optim.Optimizer,\n",
        "    scaler: GradScaler,\n",
        "    max_grad_norm: float = 5.0,\n",
        ") -> Tuple[float, Dict[str, float]]:\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    preds_all, targets_all = [], []\n",
        "\n",
        "    for inputs, targets in loader:\n",
        "        inputs = inputs.to(DEVICE)\n",
        "        targets = targets.to(DEVICE)\n",
        "        optimizer.zero_grad(set_to_none=True)\n",
        "\n",
        "        with autocast_context():\n",
        "            logits = model(inputs)\n",
        "            loss = criterion(logits, targets)\n",
        "\n",
        "        scaler.scale(loss).backward()\n",
        "        if max_grad_norm is not None:\n",
        "            scaler.unscale_(optimizer)\n",
        "            nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "\n",
        "        running_loss += loss.item() * inputs.size(0)\n",
        "        preds_all.append(torch.argmax(logits.detach(), dim=1).cpu())\n",
        "        targets_all.append(targets.cpu())\n",
        "\n",
        "    preds_np = torch.cat(preds_all).numpy()\n",
        "    targets_np = torch.cat(targets_all).numpy()\n",
        "    metrics = compute_classification_metrics(preds_np, targets_np)\n",
        "    avg_loss = running_loss / len(loader.dataset)\n",
        "    return avg_loss, metrics\n",
        "\n",
        "\n",
        "def evaluate_epoch(\n",
        "    model: nn.Module,\n",
        "    loader: DataLoader,\n",
        "    criterion: nn.Module,\n",
        "    eval_window_size: Optional[int],\n",
        "    eval_window_stride: Optional[int],\n",
        "    aggregation: str,\n",
        ") -> Tuple[float, Dict[str, float], np.ndarray, np.ndarray]:\n",
        "    model.eval()\n",
        "    running_loss = 0.0\n",
        "    preds_all, targets_all = [], []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, targets in loader:\n",
        "            inputs = inputs.to(DEVICE)\n",
        "            targets = targets.to(DEVICE)\n",
        "            with autocast_context():\n",
        "                logits = forward_with_sliding_windows(\n",
        "                    model,\n",
        "                    inputs,\n",
        "                    eval_window_size,\n",
        "                    eval_window_stride,\n",
        "                    aggregation,\n",
        "                )\n",
        "                loss = criterion(logits, targets)\n",
        "\n",
        "            running_loss += loss.item() * targets.size(0)\n",
        "            preds_all.append(torch.argmax(logits, dim=1).cpu())\n",
        "            targets_all.append(targets.cpu())\n",
        "\n",
        "    preds_np = torch.cat(preds_all).numpy()\n",
        "    targets_np = torch.cat(targets_all).numpy()\n",
        "    metrics = compute_classification_metrics(preds_np, targets_np)\n",
        "    avg_loss = running_loss / len(loader.dataset)\n",
        "    return avg_loss, metrics, preds_np, targets_np\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i8CvXdcDTBGX"
      },
      "outputs": [],
      "source": [
        "def fit_model(\n",
        "    config: Dict,\n",
        "    train_loader: DataLoader,\n",
        "    valid_loader: DataLoader,\n",
        "    run_name: str,\n",
        "    tensorboard: bool = True,\n",
        ") -> Tuple[nn.Module, Dict[str, List[float]], Dict]:\n",
        "    model = RecurrentBackbone(\n",
        "        input_size=NUM_FEATURES,\n",
        "        hidden_size=config['hidden_size'],\n",
        "        num_layers=config['num_layers'],\n",
        "        dropout=config['dropout'],\n",
        "        rnn_type=config['rnn_type'],\n",
        "        bidirectional=config.get('bidirectional', False),\n",
        "        conv_layers=config.get('conv_layers', 0),\n",
        "        conv_channels=config.get('conv_channels'),\n",
        "        conv_kernel_size=config.get('conv_kernel_size', 3),\n",
        "        conv_dropout=config.get('conv_dropout', 0.1),\n",
        "    ).to(DEVICE)\n",
        "\n",
        "    criterion = build_loss_fn(config).to(DEVICE)\n",
        "    optimizer = torch.optim.AdamW(\n",
        "        model.parameters(),\n",
        "        lr=config['lr'],\n",
        "        weight_decay=config.get('weight_decay', 0.0),\n",
        "    )\n",
        "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
        "        optimizer,\n",
        "        mode='max',\n",
        "        factor=config.get('scheduler_factor', 0.5),\n",
        "        patience=config.get('scheduler_patience', 3),\n",
        "    )\n",
        "    scaler = create_grad_scaler()\n",
        "\n",
        "    history: Dict[str, List[float]] = {\n",
        "        'train_loss': [],\n",
        "        'train_accuracy': [],\n",
        "        'train_f1': [],\n",
        "        'train_precision': [],\n",
        "        'train_recall': [],\n",
        "        'valid_loss': [],\n",
        "        'valid_accuracy': [],\n",
        "        'valid_f1': [],\n",
        "        'valid_precision': [],\n",
        "        'valid_recall': [],\n",
        "        'lr': [],\n",
        "    }\n",
        "\n",
        "    run_log_dir = (LOG_DIR / run_name).resolve()\n",
        "    writer = SummaryWriter(run_log_dir.as_posix()) if tensorboard else None\n",
        "\n",
        "    best_metric = -np.inf\n",
        "    best_state: Optional[Dict] = None\n",
        "    patience = config.get('patience', 10)\n",
        "    patience_counter = 0\n",
        "    checkpoint_path = (CHECKPOINT_DIR / f'{run_name}.pt').resolve()\n",
        "\n",
        "    eval_window_size = config.get('eval_window_size', EVAL_WINDOW_SIZE)\n",
        "    eval_window_stride = config.get('eval_window_stride', EVAL_WINDOW_STRIDE)\n",
        "    eval_aggregation = config.get('eval_aggregation', EVAL_AGGREGATION)\n",
        "\n",
        "    for epoch in range(1, config['epochs'] + 1):\n",
        "        train_loss, train_metrics = train_one_epoch(\n",
        "            model,\n",
        "            train_loader,\n",
        "            criterion,\n",
        "            optimizer,\n",
        "            scaler,\n",
        "            max_grad_norm=config.get('max_grad_norm', 5.0),\n",
        "        )\n",
        "        valid_loss, valid_metrics, preds, targets = evaluate_epoch(\n",
        "            model,\n",
        "            valid_loader,\n",
        "            criterion,\n",
        "            eval_window_size,\n",
        "            eval_window_stride,\n",
        "            eval_aggregation,\n",
        "        )\n",
        "        scheduler.step(valid_metrics['f1'])\n",
        "\n",
        "        current_lr = optimizer.param_groups[0]['lr']\n",
        "\n",
        "        history['train_loss'].append(train_loss)\n",
        "        history['train_accuracy'].append(train_metrics['accuracy'])\n",
        "        history['train_f1'].append(train_metrics['f1'])\n",
        "        history['train_precision'].append(train_metrics['precision'])\n",
        "        history['train_recall'].append(train_metrics['recall'])\n",
        "        history['valid_loss'].append(valid_loss)\n",
        "        history['valid_accuracy'].append(valid_metrics['accuracy'])\n",
        "        history['valid_f1'].append(valid_metrics['f1'])\n",
        "        history['valid_precision'].append(valid_metrics['precision'])\n",
        "        history['valid_recall'].append(valid_metrics['recall'])\n",
        "        history['lr'].append(current_lr)\n",
        "\n",
        "        if writer is not None:\n",
        "            writer.add_scalar('Loss/train', train_loss, epoch)\n",
        "            writer.add_scalar('Loss/valid', valid_loss, epoch)\n",
        "            writer.add_scalar('F1/train', train_metrics['f1'], epoch)\n",
        "            writer.add_scalar('F1/valid', valid_metrics['f1'], epoch)\n",
        "            writer.add_scalar('Accuracy/train', train_metrics['accuracy'], epoch)\n",
        "            writer.add_scalar('Accuracy/valid', valid_metrics['accuracy'], epoch)\n",
        "            writer.add_scalar('LearningRate', current_lr, epoch)\n",
        "\n",
        "        msg = (\n",
        "            f\"Epoch {epoch:03d} | \"\n",
        "            f\"train_loss={train_loss:.4f} acc={train_metrics['accuracy']:.3f} f1={train_metrics['f1']:.3f} | \"\n",
        "            f\"valid_loss={valid_loss:.4f} acc={valid_metrics['accuracy']:.3f} f1={valid_metrics['f1']:.3f} | \"\n",
        "            f\"lr={current_lr:.2e}\"\n",
        "        )\n",
        "        print(msg)\n",
        "\n",
        "        if valid_metrics['f1'] > best_metric + config.get('min_improvement', 0.0):\n",
        "            best_metric = valid_metrics['f1']\n",
        "            patience_counter = 0\n",
        "            best_state = {\n",
        "                'epoch': epoch,\n",
        "                'model_state': copy.deepcopy(model.state_dict()),\n",
        "                'optimizer_state': copy.deepcopy(optimizer.state_dict()),\n",
        "                'metrics': valid_metrics,\n",
        "                'train_metrics': train_metrics,\n",
        "                'preds': preds,\n",
        "                'targets': targets,\n",
        "            }\n",
        "            torch.save(best_state['model_state'], checkpoint_path)\n",
        "        else:\n",
        "            patience_counter += 1\n",
        "            if patience_counter >= patience:\n",
        "                print(f\"Early stopping triggered at epoch {epoch}. Best f1={best_metric:.4f}.\")\n",
        "                break\n",
        "\n",
        "    if writer is not None:\n",
        "        writer.close()\n",
        "\n",
        "    if best_state is None:\n",
        "        best_state = {\n",
        "            'epoch': config['epochs'],\n",
        "            'model_state': copy.deepcopy(model.state_dict()),\n",
        "            'optimizer_state': copy.deepcopy(optimizer.state_dict()),\n",
        "            'metrics': valid_metrics,\n",
        "            'train_metrics': train_metrics,\n",
        "            'preds': preds,\n",
        "            'targets': targets,\n",
        "        }\n",
        "        torch.save(best_state['model_state'], checkpoint_path)\n",
        "\n",
        "    model.load_state_dict(best_state['model_state'])\n",
        "    best_state.update(\n",
        "        {\n",
        "            'run_name': run_name,\n",
        "            'config': copy.deepcopy(config),\n",
        "            'history': history,\n",
        "            'checkpoint_path': checkpoint_path,\n",
        "            'best_f1': best_metric,\n",
        "        }\n",
        "    )\n",
        "    return model, history, best_state\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ltbq2G3RTBGX"
      },
      "outputs": [],
      "source": [
        "def run_cross_validation(\n",
        "    config: Dict,\n",
        "    n_splits: int = 5,\n",
        "    shuffle: bool = True,\n",
        "    random_state: int = SEED,\n",
        ") -> List[Dict]:\n",
        "    skf = StratifiedKFold(n_splits=n_splits, shuffle=shuffle, random_state=random_state)\n",
        "    fold_results: List[Dict] = []\n",
        "\n",
        "    print(f\"\\n>>> Starting {n_splits}-fold CV for {config['run_name']} ({config['rnn_type'].upper()})\")\n",
        "    for fold_idx, (train_idx, valid_idx) in enumerate(skf.split(X_train_np, y_train_idx), start=1):\n",
        "        fold_config = copy.deepcopy(config)\n",
        "        fold_config['run_name'] = f\"{config['run_name']}_fold{fold_idx}\"\n",
        "\n",
        "        X_tr, y_tr = X_train_np[train_idx], y_train_idx[train_idx]\n",
        "        X_val, y_val = X_train_np[valid_idx], y_train_idx[valid_idx]\n",
        "\n",
        "        train_loader = make_dataloader_from_arrays(\n",
        "            X_tr,\n",
        "            y_tr,\n",
        "            batch_size=fold_config['batch_size'],\n",
        "            shuffle=True,\n",
        "            mode='train',\n",
        "            oversample_factor=fold_config.get('oversample_factor', HIGH_PAIN_OVERSAMPLE),\n",
        "            augmentation_params=fold_config.get('augmentation_params', AUGMENTATION_PARAMS),\n",
        "            window_size=fold_config.get('window_size', WINDOW_SIZE),\n",
        "            window_stride=fold_config.get('window_stride', WINDOW_STRIDE),\n",
        "        )\n",
        "        valid_loader = make_dataloader_from_arrays(\n",
        "            X_val,\n",
        "            y_val,\n",
        "            batch_size=fold_config['batch_size'],\n",
        "            shuffle=False,\n",
        "            mode='valid',\n",
        "            window_size=None,\n",
        "            window_stride=None,\n",
        "        )\n",
        "\n",
        "        model, history, best_state = fit_model(\n",
        "            fold_config,\n",
        "            train_loader,\n",
        "            valid_loader,\n",
        "            run_name=fold_config['run_name'],\n",
        "            tensorboard=fold_config.get('tensorboard', True),\n",
        "        )\n",
        "        best_state['fold'] = fold_idx\n",
        "        best_state['model'] = model\n",
        "        best_state['history'] = history\n",
        "        fold_results.append(best_state)\n",
        "\n",
        "        metrics = best_state['metrics']\n",
        "        print(\n",
        "            f\"Fold {fold_idx}/{n_splits} | best F1={metrics['f1']:.4f} | \"\n",
        "            f\"Accuracy={metrics['accuracy']:.4f}\"\n",
        "        )\n",
        "\n",
        "    return fold_results\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4MCuWUYeTBGX"
      },
      "outputs": [],
      "source": [
        "def summarize_cv_results(cv_results: List[Dict]) -> pd.DataFrame:\n",
        "    rows = []\n",
        "    for res in cv_results:\n",
        "        cfg = res['config']\n",
        "        metrics = res['metrics']\n",
        "        rows.append({\n",
        "            'run_name': res['run_name'],\n",
        "            'fold': res['fold'],\n",
        "            'rnn_type': cfg['rnn_type'],\n",
        "            'bidirectional': cfg.get('bidirectional', False),\n",
        "            'hidden_size': cfg['hidden_size'],\n",
        "            'num_layers': cfg['num_layers'],\n",
        "            'dropout': cfg['dropout'],\n",
        "            'f1': metrics['f1'],\n",
        "            'accuracy': metrics['accuracy'],\n",
        "            'precision': metrics['precision'],\n",
        "            'recall': metrics['recall'],\n",
        "            'checkpoint': str(res['checkpoint_path']),\n",
        "            'log_dir': str(LOG_DIR / res['run_name']),\n",
        "        })\n",
        "\n",
        "    df = pd.DataFrame(rows)\n",
        "    agg = df.groupby(['rnn_type', 'bidirectional'])[['f1', 'accuracy', 'precision', 'recall']].agg(['mean', 'std'])\n",
        "    agg.columns = ['_'.join(col) for col in agg.columns]\n",
        "    agg = agg.reset_index()\n",
        "    return df, agg\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "plM-wtoUTBGX"
      },
      "outputs": [],
      "source": [
        "def prepare_config(name: str, overrides: Dict) -> Dict:\n",
        "    base_config = {\n",
        "        'run_name': name,\n",
        "        'rnn_type': 'gru',\n",
        "        'hidden_size': 128,\n",
        "        'num_layers': 2,\n",
        "        'dropout': 0.4,\n",
        "        'bidirectional': True,\n",
        "        'lr': 2e-3,\n",
        "        'weight_decay': 1e-4,\n",
        "        'epochs': 200,\n",
        "        'batch_size': 64,\n",
        "        'valid_size': 0.1,\n",
        "        'patience': 20,\n",
        "        'max_grad_norm': 5.0,\n",
        "        'scheduler_factor': 0.5,\n",
        "        'scheduler_patience': 5,\n",
        "        'min_improvement': 5e-4,\n",
        "        'tensorboard': True,\n",
        "        'oversample_factor': HIGH_PAIN_OVERSAMPLE,\n",
        "        'window_size': WINDOW_SIZE,\n",
        "        'window_stride': WINDOW_STRIDE,\n",
        "        'eval_window_size': EVAL_WINDOW_SIZE,\n",
        "        'eval_window_stride': EVAL_WINDOW_STRIDE,\n",
        "        'eval_aggregation': EVAL_AGGREGATION,\n",
        "        'loss_type': 'focal',\n",
        "        'use_class_weights': True,\n",
        "        'focal_gamma': 1.3,\n",
        "        'augmentation_params': AUGMENTATION_PARAMS,\n",
        "        'conv_layers': 2,\n",
        "        'conv_channels': min(NUM_FEATURES, 128),\n",
        "        'conv_kernel_size': 5,\n",
        "        'conv_dropout': 0.1,\n",
        "    }\n",
        "    config = copy.deepcopy(base_config)\n",
        "    config.update(overrides)\n",
        "    return config\n",
        "\n",
        "\n",
        "def run_experiment(config: Dict) -> Dict:\n",
        "    run_name = config.get('run_name') or f\"{config['rnn_type'].upper()}_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
        "    config = copy.deepcopy(config)\n",
        "    config['run_name'] = run_name\n",
        "\n",
        "    train_loader, valid_loader, (X_tr, y_tr, X_val, y_val) = create_dataloaders(\n",
        "        X_train_np,\n",
        "        y_train_idx,\n",
        "        valid_size=config.get('valid_size', 0.2),\n",
        "        batch_size=config['batch_size'],\n",
        "        oversample_factor=config.get('oversample_factor', HIGH_PAIN_OVERSAMPLE),\n",
        "        window_size=config.get('window_size', WINDOW_SIZE),\n",
        "        window_stride=config.get('window_stride', WINDOW_STRIDE),\n",
        "        augmentation_params=config.get('augmentation_params', AUGMENTATION_PARAMS),\n",
        "    )\n",
        "\n",
        "    model, history, best_state = fit_model(\n",
        "        config,\n",
        "        train_loader,\n",
        "        valid_loader,\n",
        "        run_name=run_name,\n",
        "        tensorboard=config.get('tensorboard', True),\n",
        "    )\n",
        "\n",
        "    best_state['data_split'] = {\n",
        "        'X_train': X_tr,\n",
        "        'y_train': y_tr,\n",
        "        'X_valid': X_val,\n",
        "        'y_valid': y_val,\n",
        "    }\n",
        "    best_state['model'] = model\n",
        "    best_state['history'] = history\n",
        "    return best_state\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rCrkiQqCTBGY"
      },
      "outputs": [],
      "source": [
        "EXPERIMENT_CONFIGS = [\n",
        "\n",
        "    prepare_config('GRU_BI', {'rnn_type': 'gru', 'bidirectional': True}),\n",
        "    prepare_config('GRU_SINGLE', {'rnn_type': 'gru', 'bidirectional': False}),\n",
        "\n",
        "]\n",
        "\n",
        "experiment_results: List[Dict] = []\n",
        "for cfg in EXPERIMENT_CONFIGS:\n",
        "    print(f\"\\n=== Running experiment: {cfg['run_name']} ({cfg['rnn_type'].upper()} - {'BI' if cfg['bidirectional'] else 'UNI'}) ===\")\n",
        "    result = run_experiment(cfg)\n",
        "    experiment_results.append(result)\n",
        "    print(\n",
        "        f\"Best validation F1: {result['best_f1']:.4f} at epoch {result['epoch']} | \"\n",
        "        f\"Accuracy: {result['metrics']['accuracy']:.4f}\"\n",
        "    )\n",
        "\n",
        "BEST_FOR_CV = prepare_config('GRU_BI', {'bidirectional': True})\n",
        "# n_splits viene calcolato automaticamente da valid_size (default 0.1 -> n_splits=10 per 90-10 split)\n",
        "cv_results = run_cross_validation(BEST_FOR_CV)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yC993yBuTBGY"
      },
      "outputs": [],
      "source": [
        "for _, row in summary_table.iterrows():\n",
        "    print(\n",
        "        f\"Run: {row['run_name']} | Model: {row['rnn_type'].upper()} | \"\n",
        "        f\"Bidirectional: {row['bidirectional']} | F1: {row['best_f1']:.4f}\"\n",
        "    )\n",
        "    print(f\"  Logs: {row['log_dir']}\")\n",
        "\n",
        "# Per TensorBoard combinato (eseguire su Colab / locale):\n",
        "# %tensorboard --logdir \"{LOG_DIR.as_posix()}\"\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aE9xeS7dTBGY"
      },
      "outputs": [],
      "source": [
        "def gather_results(source_names: List[str]) -> List[Dict]:\n",
        "    collected: List[Dict] = []\n",
        "    for name in source_names:\n",
        "        if name in globals():\n",
        "            value = globals()[name]\n",
        "            if isinstance(value, list) and value:\n",
        "                collected.extend(value)\n",
        "    return collected\n",
        "\n",
        "\n",
        "def build_summary_table(result_sources: List[str]) -> Tuple[pd.DataFrame, List[Dict]]:\n",
        "    collected_results = gather_results(result_sources)\n",
        "    if not collected_results:\n",
        "        raise RuntimeError('No experiment results available. Run the training/sweep cells first.')\n",
        "\n",
        "    summary_rows = []\n",
        "    for res in collected_results:\n",
        "        cfg = res['config']\n",
        "        metrics = res['metrics']\n",
        "        summary_rows.append({\n",
        "            'run_name': res['run_name'],\n",
        "            'fold': res.get('fold'),\n",
        "            'rnn_type': cfg['rnn_type'],\n",
        "            'bidirectional': cfg.get('bidirectional', False),\n",
        "            'hidden_size': cfg['hidden_size'],\n",
        "            'num_layers': cfg['num_layers'],\n",
        "            'dropout': cfg['dropout'],\n",
        "            'lr': cfg['lr'],\n",
        "            'weight_decay': cfg.get('weight_decay', 0.0),\n",
        "            'best_epoch': res['epoch'],\n",
        "            'best_f1': res['best_f1'],\n",
        "            'accuracy': metrics['accuracy'],\n",
        "            'precision': metrics['precision'],\n",
        "            'recall': metrics['recall'],\n",
        "            'checkpoint': str(res['checkpoint_path']),\n",
        "            'log_dir': str(LOG_DIR / res['run_name']),\n",
        "            'is_cv': res.get('fold') is not None,\n",
        "        })\n",
        "\n",
        "    summary_df = pd.DataFrame(summary_rows)\n",
        "    summary_df = summary_df.sort_values(by='best_f1', ascending=False).reset_index(drop=True)\n",
        "    return summary_df, collected_results\n",
        "\n",
        "\n",
        "RESULT_SOURCES = ['experiment_results', 'sweep_results', 'cv_results', 'auto_cv_results']\n",
        "summary_table, all_results = build_summary_table(RESULT_SOURCES)\n",
        "summary_table\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pO9BAXqcTBGY"
      },
      "outputs": [],
      "source": [
        "cv_folds_df, cv_summary = summarize_cv_results(cv_results)\n",
        "cv_summary\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dyq9UQ2mTBGY"
      },
      "outputs": [],
      "source": [
        "ENABLE_AUTO_CV = False\n",
        "AUTO_CV_TOP_K = 2\n",
        "AUTO_CV_SPLITS = 5\n",
        "\n",
        "if ENABLE_AUTO_CV:\n",
        "    auto_cv_results = []\n",
        "    for _, row in summary_table.head(AUTO_CV_TOP_K).iterrows():\n",
        "        overrides = {\n",
        "            'rnn_type': row['rnn_type'],\n",
        "            'hidden_size': row['hidden_size'],\n",
        "            'num_layers': row['num_layers'],\n",
        "            'dropout': row['dropout'],\n",
        "            'bidirectional': row['bidirectional'],\n",
        "            'lr': row['lr'],\n",
        "            'weight_decay': row['weight_decay'],\n",
        "        }\n",
        "        cfg = prepare_config(f\"{row['run_name']}_AUTO_CV\", overrides)\n",
        "        cv_out = run_cross_validation(cfg, n_splits=AUTO_CV_SPLITS)\n",
        "        auto_cv_results.extend(cv_out)\n",
        "\n",
        "    globals()['auto_cv_results'] = auto_cv_results\n",
        "    print(f\"Completed auto CV for top {min(AUTO_CV_TOP_K, len(summary_table))} runs.\")\n",
        "    RESULT_SOURCES = ['experiment_results', 'sweep_results', 'cv_results', 'auto_cv_results']\n",
        "    summary_table, all_results = build_summary_table(RESULT_SOURCES)\n",
        "    summary_table\n",
        "else:\n",
        "    print('Auto CV disabled. Set ENABLE_AUTO_CV=True to enable automatic cross-validation.')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OR95BKT0TBGZ"
      },
      "outputs": [],
      "source": [
        "if 'all_results' not in globals() or not all_results:\n",
        "    raise RuntimeError('No experiment results available. Run the training/sweep cells first.')\n",
        "\n",
        "best_run = max(all_results, key=lambda x: x['best_f1'])\n",
        "best_model = best_run['model']\n",
        "best_history = best_run['history']\n",
        "print(\n",
        "    f\"Selected best run: {best_run['run_name']} | \"\n",
        "    f\"F1={best_run['best_f1']:.4f} | Accuracy={best_run['metrics']['accuracy']:.4f}\"\n",
        ")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Psxajjq9TBGZ"
      },
      "outputs": [],
      "source": [
        "def plot_history(history: Dict[str, List[float]], title: str = 'Learning Curves'):\n",
        "    epochs = range(1, len(history['train_loss']) + 1)\n",
        "    fig, axs = plt.subplots(1, 3, figsize=(18, 4))\n",
        "\n",
        "    axs[0].plot(epochs, history['train_loss'], label='Train')\n",
        "    axs[0].plot(epochs, history['valid_loss'], label='Valid')\n",
        "    axs[0].set_title('Loss')\n",
        "    axs[0].set_xlabel('Epoch')\n",
        "    axs[0].legend()\n",
        "\n",
        "    axs[1].plot(epochs, history['train_accuracy'], label='Train')\n",
        "    axs[1].plot(epochs, history['valid_accuracy'], label='Valid')\n",
        "    axs[1].set_title('Accuracy')\n",
        "    axs[1].set_xlabel('Epoch')\n",
        "    axs[1].legend()\n",
        "\n",
        "    axs[2].plot(epochs, history['train_f1'], label='Train F1')\n",
        "    axs[2].plot(epochs, history['valid_f1'], label='Valid F1')\n",
        "    axs[2].set_title('Macro F1')\n",
        "    axs[2].set_xlabel('Epoch')\n",
        "    axs[2].legend()\n",
        "\n",
        "    plt.suptitle(title)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "plot_history(best_history, title=f\"Learning Curves ‚Äî {best_run['run_name']}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3jnk70CJTBGZ"
      },
      "outputs": [],
      "source": [
        "best_preds = best_run['preds']\n",
        "best_targets = best_run['targets']\n",
        "print(f\"Best validation macro F1: {best_run['best_f1']:.3f}\")\n",
        "print(\n",
        "    classification_report(\n",
        "        best_targets,\n",
        "        best_preds,\n",
        "        target_names=[IDX2LABEL[i] for i in range(NUM_CLASSES)],\n",
        "    )\n",
        ")\n",
        "\n",
        "cf = confusion_matrix(best_targets, best_preds)\n",
        "plt.figure(figsize=(6, 5))\n",
        "sns.heatmap(\n",
        "    cf,\n",
        "    annot=True,\n",
        "    fmt='d',\n",
        "    cmap='Blues',\n",
        "    xticklabels=[IDX2LABEL[i] for i in range(NUM_CLASSES)],\n",
        "    yticklabels=[IDX2LABEL[i] for i in range(NUM_CLASSES)],\n",
        ")\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('True')\n",
        "plt.title(f\"Validation Confusion Matrix ‚Äî {best_run['run_name']}\")\n",
        "plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_Ip0Gs2ATBGZ"
      },
      "outputs": [],
      "source": [
        "%tensorboard --logdir \"/content/drive/MyDrive/[2025-2026]\\ AN2DL/Challenge/outputs/logs\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ALogtk9WTBGZ"
      },
      "outputs": [],
      "source": [
        "test_dataset = TimeSeriesDataset(\n",
        "    X_test_np,\n",
        "    labels=None,\n",
        "    window_size=None,\n",
        "    mode='test',\n",
        "    high_pain_targets=HIGH_PAIN_IDX,\n",
        ")\n",
        "test_loader = DataLoader(test_dataset, batch_size=256, shuffle=False, drop_last=False)\n",
        "\n",
        "best_model.eval()\n",
        "test_preds = []\n",
        "best_config = best_run['config']\n",
        "eval_window_size = best_config.get('eval_window_size', EVAL_WINDOW_SIZE)\n",
        "eval_window_stride = best_config.get('eval_window_stride', EVAL_WINDOW_STRIDE)\n",
        "eval_aggregation = best_config.get('eval_aggregation', EVAL_AGGREGATION)\n",
        "with torch.no_grad():\n",
        "    for inputs in test_loader:\n",
        "        inputs = inputs.to(DEVICE)\n",
        "        with autocast_context():\n",
        "            logits = forward_with_sliding_windows(\n",
        "                best_model,\n",
        "                inputs,\n",
        "                eval_window_size,\n",
        "                eval_window_stride,\n",
        "                eval_aggregation,\n",
        "            )\n",
        "        test_preds.append(torch.argmax(logits, dim=1).cpu().numpy())\n",
        "\n",
        "test_preds = np.concatenate(test_preds)\n",
        "test_labels = [IDX2LABEL[idx] for idx in test_preds]\n",
        "\n",
        "submission = pd.DataFrame({\n",
        "    'sample_index': pd.unique(X_test_raw['sample_index']),\n",
        "    'label': test_labels,\n",
        "})\n",
        "submission_filename = OUTPUT_DIR / f\"submission_{best_run['run_name'].lower()}.csv\"\n",
        "submission.to_csv(submission_filename, index=False)\n",
        "print(f\"Saved submission to {submission_filename}\")\n",
        "submission.head()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "64lgieJwTBGZ"
      },
      "source": [
        "## Next Steps\n",
        "\n",
        "- Espandere `EXPERIMENT_CONFIGS` con ricerche random/grid su hidden size, depth, dropout, learning rate e scheduler per automatizzare l'hyperparameter tuning.\n",
        "- Utilizzare `run_cross_validation` su pi√π configurazioni e confrontare le metriche aggregate in `cv_summary`, esportando i risultati (CSV/LaTeX) per il report finale.\n",
        "- Monitorare tutti i run con `%tensorboard --logdir outputs/logs`, salvando screenshot delle curve principali e confrontando tempi/risorse.\n",
        "- Integrare tecniche di regularizzazione avanzate (label smoothing, mixup temporale, stochastic weight averaging) o layer di attention/pooling.\n",
        "- Costruire ensemble sui checkpoint migliori (media delle probabilit√† o voting) prima della submission Kaggle definitiva.\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
