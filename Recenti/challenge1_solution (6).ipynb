{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d0iHUb-QTBGM"
      },
      "source": [
        "# üè¥‚Äç‚ò†Ô∏è AN2DL25 Challenge 1 ‚Äî Pirate Pain Classification\n",
        "\n",
        "This notebook implements a full deep-learning pipeline for multivariate time-series classification of the Pirate Pain dataset. It is inspired by the Lecture 4 notebook (`Timeseries Classification (1).ipynb`) but adapted for the competition setting, including data preparation, model training (RNN/GRU/LSTM variants), evaluation, and test-time inference.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 300,
      "metadata": {
        "id": "TNB0xa3YTBGR"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import random\n",
        "import math\n",
        "import copy\n",
        "from pathlib import Path\n",
        "from typing import Tuple, Dict, Optional, List, Any, Sequence\n",
        "from datetime import datetime\n",
        "from itertools import product\n",
        "import inspect\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.metrics import (\n",
        "    f1_score,\n",
        "    classification_report,\n",
        "    confusion_matrix,\n",
        "    precision_recall_fscore_support,\n",
        ")\n",
        "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "import torch.nn.functional as F\n",
        "\n",
        "try:\n",
        "    from torch.amp import autocast, GradScaler\n",
        "except ImportError:  # pragma: no cover\n",
        "    from torch.cuda.amp import autocast, GradScaler\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 301,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JReu2QJbTBGR",
        "outputId": "844f90a2-4e0f-490e-fef0-c11b4980d9cc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<>:11: SyntaxWarning: invalid escape sequence '\\ '\n",
            "<>:11: SyntaxWarning: invalid escape sequence '\\ '\n",
            "/tmp/ipython-input-1979095382.py:11: SyntaxWarning: invalid escape sequence '\\ '\n",
            "  BASE_DIR = Path('/content/drive/MyDrive/[2025-2026]\\ A2NDL/Challenge')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Usando DATA_DIR: /content/drive/MyDrive/[2025-2026] AN2DL/Challenge\n",
            "Running in Colab: True\n",
            "Device: cuda\n",
            "Data dir: /content/drive/MyDrive/[2025-2026] AN2DL/Challenge\n",
            "Output dir: /content/drive/MyDrive/[2025-2026]\\ A2NDL/Challenge/outputs\n"
          ]
        }
      ],
      "source": [
        "SEED = 9\n",
        "\n",
        "try:\n",
        "    from google.colab import drive  # type: ignore\n",
        "    IN_COLAB = True\n",
        "except ImportError:  # pragma: no cover\n",
        "    IN_COLAB = False\n",
        "\n",
        "if IN_COLAB:\n",
        "    drive.mount('/content/drive', force_remount=False)\n",
        "    BASE_DIR = Path('/content/drive/MyDrive/[2025-2026]\\ A2NDL/Challenge')\n",
        "else:\n",
        "    BASE_DIR = Path('/Users/md101ta/Desktop/Pirates')\n",
        "\n",
        "DATA_DIR = (BASE_DIR / 'data').resolve()\n",
        "DATA_DIR_CANDIDATES = [\n",
        "    Path('/content/drive/MyDrive/[2025-2026] AN2DL/Challenge'),\n",
        "    Path('/content/drive/MyDrive/[2025-2026]\\\\ A2NDL/Challenge'),\n",
        "    BASE_DIR / 'data'\n",
        "]\n",
        "\n",
        "for candidate in DATA_DIR_CANDIDATES:\n",
        "    if candidate.exists():\n",
        "        DATA_DIR = candidate\n",
        "        break\n",
        "else:\n",
        "    raise FileNotFoundError('Nessuna DATA_DIR valida trovata')\n",
        "\n",
        "print(f'Usando DATA_DIR: {DATA_DIR}')\n",
        "OUTPUT_DIR = (BASE_DIR / 'outputs').resolve()\n",
        "OUTPUT_DIR.mkdir(exist_ok=True, parents=True)\n",
        "\n",
        "# Reproducibility\n",
        "os.environ['PYTHONHASHSEED'] = str(SEED)\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(SEED)\n",
        "    DEVICE = torch.device('cuda')\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "else:\n",
        "    DEVICE = torch.device('cpu')\n",
        "\n",
        "print(f'Running in Colab: {IN_COLAB}')\n",
        "print(f'Device: {DEVICE}')\n",
        "print(f'Data dir: {DATA_DIR}')\n",
        "print(f'Output dir: {OUTPUT_DIR}')\n",
        "\n",
        "_AUTocast_params = inspect.signature(autocast).parameters\n",
        "_GRADSCALER_PARAMS = inspect.signature(GradScaler).parameters\n",
        "\n",
        "def autocast_context():\n",
        "    enabled = DEVICE.type == 'cuda'\n",
        "    if 'device_type' in _AUTocast_params:\n",
        "        return autocast(device_type=DEVICE.type, enabled=enabled)\n",
        "    if 'device' in _AUTocast_params:\n",
        "        return autocast(DEVICE.type, enabled=enabled)\n",
        "    # fallback to legacy signature (enabled only)\n",
        "    return autocast(enabled=enabled)\n",
        "\n",
        "\n",
        "def create_grad_scaler():\n",
        "    enabled = DEVICE.type == 'cuda'\n",
        "    if 'device_type' in _GRADSCALER_PARAMS:\n",
        "        return GradScaler(device_type=DEVICE.type, enabled=enabled)\n",
        "    return GradScaler(enabled=enabled)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 302,
      "metadata": {
        "id": "Us5CnwHwTBGS"
      },
      "outputs": [],
      "source": [
        "LOG_DIR = (OUTPUT_DIR / 'logs').resolve()\n",
        "CHECKPOINT_DIR = (OUTPUT_DIR / 'checkpoints').resolve()\n",
        "LOG_DIR.mkdir(exist_ok=True, parents=True)\n",
        "CHECKPOINT_DIR.mkdir(exist_ok=True, parents=True)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 303,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-oigFvDtTBGS",
        "outputId": "518912b5-66a6-4c7a-cc12-946d00653c42"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(105760, 40) (661, 2) (211840, 40)\n"
          ]
        }
      ],
      "source": [
        "from typing import Tuple\n",
        "\n",
        "assert DATA_DIR.exists(), f\"DATA_DIR non esiste: {DATA_DIR}\"\n",
        "\n",
        "def load_data(data_dir: Path) -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n",
        "    X_train = pd.read_csv(data_dir / 'pirate_pain_train.csv')\n",
        "    y_train = pd.read_csv(data_dir / 'pirate_pain_train_labels.csv')\n",
        "    X_test  = pd.read_csv(data_dir / 'pirate_pain_test.csv')\n",
        "    return X_train, y_train, X_test\n",
        "\n",
        "# Carica i dati\n",
        "X_train_raw, y_train, X_test_raw = load_data(DATA_DIR)\n",
        "print(X_train_raw.shape, y_train.shape, X_test_raw.shape)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 304,
      "metadata": {
        "id": "ZSUERSjmoTB4",
        "outputId": "6a551b41-e900-46c6-9560-51f490cbadb7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Salvato report: /content/drive/MyDrive/[2025-2026]\\ A2NDL/Challenge/outputs/reports/feature_describe.csv\n",
            "Salvato report: /content/drive/MyDrive/[2025-2026]\\ A2NDL/Challenge/outputs/reports/feature_missing.csv\n",
            "Salvato report: /content/drive/MyDrive/[2025-2026]\\ A2NDL/Challenge/outputs/reports/feature_variance.csv\n",
            "Salvato report: /content/drive/MyDrive/[2025-2026]\\ A2NDL/Challenge/outputs/reports/feature_correlation.csv\n",
            "Salvato report: /content/drive/MyDrive/[2025-2026]\\ A2NDL/Challenge/outputs/reports/feature_outlier_ratio.csv\n",
            "Profiling completato. Totale feature analizzate: 38\n"
          ]
        }
      ],
      "source": [
        "# -- Data profiling & feature diagnostics -------------------------------------------------\n",
        "REPORT_DIR = (OUTPUT_DIR / 'reports').resolve()\n",
        "REPORT_DIR.mkdir(exist_ok=True, parents=True)\n",
        "\n",
        "X_train_raw = X_train_raw.sort_values(['sample_index', 'time']).reset_index(drop=True)\n",
        "X_test_raw = X_test_raw.sort_values(['sample_index', 'time']).reset_index(drop=True)\n",
        "\n",
        "FEATURE_BASE_COLUMNS = [col for col in X_train_raw.columns if col not in ['sample_index', 'time']]\n",
        "NUMERIC_FEATURE_BASE_COLUMNS = X_train_raw[FEATURE_BASE_COLUMNS].select_dtypes(include=[np.number]).columns.tolist()\n",
        "feature_audit_entries: List[Dict[str, Any]] = []\n",
        "\n",
        "if not NUMERIC_FEATURE_BASE_COLUMNS:\n",
        "    raise RuntimeError('Nessuna feature numerica trovata per il profiling. Verifica il caricamento dati.')\n",
        "\n",
        "def save_report(df: pd.DataFrame, filename: str):\n",
        "    output_path = REPORT_DIR / filename\n",
        "    df.to_csv(output_path)\n",
        "    print(f\"Salvato report: {output_path}\")\n",
        "\n",
        "# Statistiche descrittive\n",
        "feature_describe = X_train_raw[NUMERIC_FEATURE_BASE_COLUMNS].describe().T\n",
        "save_report(feature_describe, 'feature_describe.csv')\n",
        "\n",
        "# Missing values\n",
        "missing_info = pd.DataFrame({\n",
        "    'missing_count': X_train_raw[FEATURE_BASE_COLUMNS].isna().sum(),\n",
        "    'missing_ratio': X_train_raw[FEATURE_BASE_COLUMNS].isna().mean(),\n",
        "})\n",
        "save_report(missing_info, 'feature_missing.csv')\n",
        "\n",
        "# Varianza\n",
        "variance_info = X_train_raw[NUMERIC_FEATURE_BASE_COLUMNS].var().to_frame(name='variance')\n",
        "save_report(variance_info, 'feature_variance.csv')\n",
        "\n",
        "# Correlazioni (assolute)\n",
        "corr_matrix = X_train_raw[NUMERIC_FEATURE_BASE_COLUMNS].corr().abs()\n",
        "save_report(corr_matrix, 'feature_correlation.csv')\n",
        "\n",
        "# Outlier ratio (valori oltre 5 sigma)\n",
        "feature_means = X_train_raw[NUMERIC_FEATURE_BASE_COLUMNS].mean()\n",
        "feature_stds = X_train_raw[NUMERIC_FEATURE_BASE_COLUMNS].std() + 1e-8\n",
        "z_scores = np.abs((X_train_raw[NUMERIC_FEATURE_BASE_COLUMNS] - feature_means) / feature_stds)\n",
        "outlier_ratio = (z_scores > 5).sum().div(len(X_train_raw)).to_frame(name='outlier_ratio')\n",
        "save_report(outlier_ratio, 'feature_outlier_ratio.csv')\n",
        "\n",
        "print('Profiling completato. Totale feature analizzate:', len(FEATURE_BASE_COLUMNS))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 305,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 594
        },
        "id": "G1_qoIZ2TBGT",
        "outputId": "95fe9206-d12d-4ade-d4fa-fe20057d2a87"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Removing features: ['joint_13', 'joint_14', 'joint_15', 'joint_16', 'joint_23', 'joint_24', 'joint_30', 'n_eyes', 'n_hands']\n",
            "Salvato report: /content/drive/MyDrive/[2025-2026]\\ A2NDL/Challenge/outputs/reports/feature_removal_log.csv\n",
            "Time steps (raw): 160 | Window size: 25 | Numeric features: 38 | Categorical embed dim: 5 | Classes: 3\n",
            "Category mappings: {'n_legs': {'one+peg_leg': 0, 'two': 1}, 'n_hands': {'one+hook_hand': 0, 'two': 1}, 'n_eyes': {'one+eye_patch': 0, 'two': 1}}\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "CATEGORICAL_COLUMNS = ['n_legs', 'n_hands', 'n_eyes']\n",
        "CATEGORY_MAPPINGS: Dict[str, Dict[str, int]] = {}\n",
        "\n",
        "for col in CATEGORICAL_COLUMNS:\n",
        "    uniques = pd.concat([X_train_raw[col], X_test_raw[col]]).dropna().unique()\n",
        "    mapping = {value: idx for idx, value in enumerate(sorted(uniques))}\n",
        "    CATEGORY_MAPPINGS[col] = mapping\n",
        "    X_train_raw[col] = X_train_raw[col].map(mapping).astype(np.int32)\n",
        "    X_test_raw[col] = X_test_raw[col].map(mapping).astype(np.int32)\n",
        "\n",
        "FEATURE_COLUMNS = [col for col in X_train_raw.columns if col not in ['sample_index', 'time']]\n",
        "\n",
        "LOW_VARIANCE_THRESHOLD = 2e-3\n",
        "LOW_MAG_THRESHOLD = 1e-3\n",
        "HIGH_CORR_THRESHOLD = 0.98\n",
        "\n",
        "feature_std = X_train_raw[FEATURE_COLUMNS].std()\n",
        "feature_abs_max = X_train_raw[FEATURE_COLUMNS].abs().max()\n",
        "\n",
        "# Helper per loggare le feature rimosse\n",
        "\n",
        "def log_feature_removal(feature: str, reason: str, metric_value: Optional[float] = None, notes: Optional[str] = None):\n",
        "    entry = {\n",
        "        'feature': feature,\n",
        "        'reason': reason,\n",
        "        'metric': metric_value,\n",
        "        'notes': notes,\n",
        "    }\n",
        "    feature_audit_entries.append(entry)\n",
        "\n",
        "# Colonne costanti\n",
        "constant_features = [col for col in FEATURE_COLUMNS if X_train_raw[col].nunique(dropna=False) <= 1]\n",
        "for feature in constant_features:\n",
        "    log_feature_removal(feature, 'constant_value', X_train_raw[feature].iloc[0])\n",
        "\n",
        "# Bassa varianza\n",
        "low_variance = feature_std[feature_std <= LOW_VARIANCE_THRESHOLD]\n",
        "for feature, value in low_variance.items():\n",
        "    log_feature_removal(feature, 'low_variance', float(value))\n",
        "\n",
        "# Bassa magnitudo\n",
        "low_magnitude = feature_abs_max[feature_abs_max <= LOW_MAG_THRESHOLD]\n",
        "for feature, value in low_magnitude.items():\n",
        "    log_feature_removal(feature, 'low_magnitude', float(value))\n",
        "\n",
        "# Alta correlazione\n",
        "corr_matrix = X_train_raw[FEATURE_COLUMNS].corr().abs()\n",
        "upper_mask = np.triu(np.ones_like(corr_matrix, dtype=bool), k=1)\n",
        "upper_corr = corr_matrix.where(upper_mask)\n",
        "high_corr_features = set()\n",
        "for col in upper_corr.columns:\n",
        "    partners = upper_corr.index[upper_corr[col] >= HIGH_CORR_THRESHOLD].tolist()\n",
        "    if partners:\n",
        "        high_corr_features.add(col)\n",
        "        joined_partners = ','.join(partners)\n",
        "        log_feature_removal(col, 'high_correlation', notes=f'correlated_with={joined_partners}')\n",
        "\n",
        "# Outlier-dominated features (opzionale, threshold conservativo)\n",
        "outlier_flagged = outlier_ratio[outlier_ratio['outlier_ratio'] >= 0.75].index.tolist()\n",
        "for feature in outlier_flagged:\n",
        "    log_feature_removal(feature, 'extreme_outliers', float(outlier_ratio.loc[feature, 'outlier_ratio']))\n",
        "\n",
        "features_to_drop = sorted(set(constant_features) | set(low_variance.index) | set(low_magnitude.index) | high_corr_features | set(outlier_flagged))\n",
        "if features_to_drop:\n",
        "    print(\"Removing features:\", features_to_drop)\n",
        "    X_train_raw = X_train_raw.drop(columns=features_to_drop)\n",
        "    X_test_raw = X_test_raw.drop(columns=features_to_drop)\n",
        "    FEATURE_COLUMNS = [col for col in FEATURE_COLUMNS if col not in features_to_drop]\n",
        "\n",
        "    removal_df = pd.DataFrame(feature_audit_entries)\n",
        "    if not removal_df.empty:\n",
        "        save_report(removal_df, 'feature_removal_log.csv')\n",
        "else:\n",
        "    print('Nessuna feature rimossa con i criteri impostati.')\n",
        "    removal_df = pd.DataFrame(feature_audit_entries)\n",
        "    save_report(removal_df, 'feature_removal_log.csv')\n",
        "\n",
        "FULL_TIME_STEPS = X_train_raw['time'].nunique()\n",
        "WINDOW_SIZE = 25\n",
        "WINDOW_STRIDE = 15\n",
        "EVAL_WINDOW_SIZE = WINDOW_SIZE\n",
        "EVAL_WINDOW_STRIDE = WINDOW_STRIDE\n",
        "EVAL_AGGREGATION = 'logsumexp'\n",
        "if WINDOW_SIZE > FULL_TIME_STEPS:\n",
        "    raise ValueError(\n",
        "        f'Requested window size ({WINDOW_SIZE}) exceeds the available time steps ({FULL_TIME_STEPS}).'\n",
        "    )\n",
        "TIME_STEPS = FULL_TIME_STEPS\n",
        "\n",
        "TIME_DIVISOR = max(FULL_TIME_STEPS - 1, 1)\n",
        "TIME_FEATURE_COLUMNS = ['time_fraction', 'time_sin', 'time_cos', 'time_is_start', 'time_is_end']\n",
        "for df in (X_train_raw, X_test_raw):\n",
        "    frac = (df['time'] / TIME_DIVISOR).astype(np.float32)\n",
        "    df['time_fraction'] = frac\n",
        "    df['time_sin'] = np.sin(2 * np.pi * frac).astype(np.float32)\n",
        "    df['time_cos'] = np.cos(2 * np.pi * frac).astype(np.float32)\n",
        "    df['time_is_start'] = (df['time'] == 0).astype(np.float32)\n",
        "    df['time_is_end'] = (df['time'] == FULL_TIME_STEPS - 1).astype(np.float32)\n",
        "\n",
        "for col in TIME_FEATURE_COLUMNS:\n",
        "    if col not in FEATURE_COLUMNS:\n",
        "        FEATURE_COLUMNS.append(col)\n",
        "\n",
        "TEMPORAL_WINDOW_SIZES = (5, 15)\n",
        "\n",
        "NUMERIC_FEATURE_COLUMNS = [col for col in FEATURE_COLUMNS if col not in CATEGORICAL_COLUMNS]\n",
        "VALUE_FEATURE_COLUMNS = [col for col in NUMERIC_FEATURE_COLUMNS if col not in TIME_FEATURE_COLUMNS]\n",
        "ACTIVE_CATEGORICAL_COLUMNS = [col for col in CATEGORICAL_COLUMNS if col in FEATURE_COLUMNS]\n",
        "\n",
        "CAT_CARDINALITIES = {col: len(CATEGORY_MAPPINGS[col]) for col in ACTIVE_CATEGORICAL_COLUMNS}\n",
        "\n",
        "def default_embed_dim(cardinality: int) -> int:\n",
        "    return int(max(2, min(16, np.ceil(cardinality ** 0.25 * 4))))\n",
        "\n",
        "CAT_EMBED_DIMS = {col: default_embed_dim(size) for col, size in CAT_CARDINALITIES.items()}\n",
        "CATEGORICAL_CARDINALITY_LIST = [CAT_CARDINALITIES[col] for col in ACTIVE_CATEGORICAL_COLUMNS]\n",
        "CATEGORICAL_EMBED_DIM_LIST = [CAT_EMBED_DIMS[col] for col in ACTIVE_CATEGORICAL_COLUMNS]\n",
        "TOTAL_EMBED_DIM = int(sum(CAT_EMBED_DIMS.values()))\n",
        "\n",
        "\n",
        "def add_temporal_statistics(df: pd.DataFrame):\n",
        "    if not VALUE_FEATURE_COLUMNS:\n",
        "        return\n",
        "    df_signal = df[VALUE_FEATURE_COLUMNS].abs().mean(axis=1)\n",
        "    df['signal_abs_mean'] = df_signal\n",
        "    if 'signal_abs_mean' not in FEATURE_COLUMNS:\n",
        "        FEATURE_COLUMNS.append('signal_abs_mean')\n",
        "    if 'signal_abs_mean' not in NUMERIC_FEATURE_COLUMNS:\n",
        "        NUMERIC_FEATURE_COLUMNS.append('signal_abs_mean')\n",
        "    for window in TEMPORAL_WINDOW_SIZES:\n",
        "        rolling_mean = (\n",
        "            df.groupby('sample_index')['signal_abs_mean']\n",
        "              .transform(lambda x: x.rolling(window, min_periods=1, center=True).mean())\n",
        "        )\n",
        "        rolling_std = (\n",
        "            df.groupby('sample_index')['signal_abs_mean']\n",
        "              .transform(lambda x: x.rolling(window, min_periods=1, center=True).std().fillna(0.0))\n",
        "        )\n",
        "        df[f'signal_mean_w{window}'] = rolling_mean\n",
        "        df[f'signal_std_w{window}'] = rolling_std\n",
        "        for feature in (f'signal_mean_w{window}', f'signal_std_w{window}'):\n",
        "            if feature not in FEATURE_COLUMNS:\n",
        "                FEATURE_COLUMNS.append(feature)\n",
        "            if feature not in NUMERIC_FEATURE_COLUMNS:\n",
        "                NUMERIC_FEATURE_COLUMNS.append(feature)\n",
        "\n",
        "for dataset in (X_train_raw, X_test_raw):\n",
        "    add_temporal_statistics(dataset)\n",
        "\n",
        "NUMERIC_FEATURE_COLUMNS = sorted(NUMERIC_FEATURE_COLUMNS)\n",
        "NUMERIC_FEATURE_DIM = len(NUMERIC_FEATURE_COLUMNS)\n",
        "TOTAL_EMBED_DIM = int(sum(CAT_EMBED_DIMS.values()))\n",
        "MODEL_INPUT_FEATURES = NUMERIC_FEATURE_DIM + TOTAL_EMBED_DIM\n",
        "NUM_FEATURES = NUMERIC_FEATURE_DIM  # legacy compatibility\n",
        "NUM_CLASSES = y_train['label'].nunique()\n",
        "print(\n",
        "    f\"Time steps (raw): {FULL_TIME_STEPS} | Window size: {WINDOW_SIZE} | Numeric features: {NUMERIC_FEATURE_DIM} | \"\n",
        "    f\"Categorical embed dim: {TOTAL_EMBED_DIM} | Classes: {NUM_CLASSES}\"\n",
        ")\n",
        "print('Category mappings:', CATEGORY_MAPPINGS)\n",
        "\n",
        "y_train['label'].value_counts().plot(kind='bar', title='Class distribution')\n",
        "plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 306,
      "metadata": {
        "id": "rF4CIcmeoTB6",
        "outputId": "ed8359dc-12fc-48a0-bff9-f1676aa29501",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sanity checks superati: dataset e feature set coerenti.\n"
          ]
        }
      ],
      "source": [
        "# -- Sanity checks per garantire coerenza pipeline ---------------------------\n",
        "def run_quick_sanity_checks():\n",
        "    train_cols = set(X_train_raw.columns)\n",
        "    test_cols = set(X_test_raw.columns)\n",
        "    assert train_cols == test_cols, 'Train/Test hanno colonne diverse'\n",
        "    assert len(FEATURE_COLUMNS) == len(set(FEATURE_COLUMNS)), 'FEATURE_COLUMNS contiene duplicati'\n",
        "    assert all(col in train_cols for col in FEATURE_COLUMNS), 'Colonne feature non presenti nel dataset'\n",
        "    assert set(NUMERIC_FEATURE_COLUMNS).issubset(train_cols), 'Feature numeriche mancanti nel dataset'\n",
        "    assert set(ACTIVE_CATEGORICAL_COLUMNS).issubset(train_cols), 'Feature categoriali mancanti nel dataset'\n",
        "    expected_input = len(NUMERIC_FEATURE_COLUMNS) + sum(CAT_EMBED_DIMS.values())\n",
        "    assert MODEL_INPUT_FEATURES == expected_input, 'MODEL_INPUT_FEATURES non coerente con le dimensioni calcolate'\n",
        "    print('Sanity checks superati: dataset e feature set coerenti.')\n",
        "\n",
        "run_quick_sanity_checks()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 307,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m7ieHeHyTBGU",
        "outputId": "72872eee-c613-41d3-89b7-28e9f1d257dc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Label mapping: {'high_pain': 0, 'low_pain': 1, 'no_pain': 2}\n",
            "High-pain classes for augmentation: ['high_pain']\n",
            "Low-pain classes for augmentation: ['low_pain']\n",
            "No-pain classes for augmentation: {2}\n",
            "Numeric tensors: (661, 160, 38) (1324, 160, 38)\n",
            "Categorical tensors: (661, 160, 1) (1324, 160, 1)\n",
            "Labels: (661,)\n"
          ]
        }
      ],
      "source": [
        "LABEL2IDX = {label: idx for idx, label in enumerate(sorted(y_train['label'].unique()))}\n",
        "IDX2LABEL = {idx: label for label, idx in LABEL2IDX.items()}\n",
        "print('Label mapping:', LABEL2IDX)\n",
        "\n",
        "HIGH_PAIN_LABELS = [label for label in LABEL2IDX if 'high' in label.lower()]\n",
        "LOW_PAIN_LABELS = [label for label in LABEL2IDX if 'low' in label.lower()]\n",
        "NO_PAIN_LABELS = [label for label in LABEL2IDX if 'no' in label.lower()]\n",
        "HIGH_PAIN_IDX = {LABEL2IDX[label] for label in HIGH_PAIN_LABELS}\n",
        "LOW_PAIN_IDX = {LABEL2IDX[label] for label in LOW_PAIN_LABELS}\n",
        "NO_PAIN_IDX = {LABEL2IDX[label] for label in NO_PAIN_LABELS}\n",
        "print('High-pain classes for augmentation:', HIGH_PAIN_LABELS or 'None')\n",
        "print('Low-pain classes for augmentation:', LOW_PAIN_LABELS or 'None')\n",
        "print('No-pain classes for augmentation:', NO_PAIN_IDX or 'None')\n",
        "\n",
        "HIGH_PAIN_OVERSAMPLE = 2 if HIGH_PAIN_IDX else 1\n",
        "LOW_PAIN_OVERSAMPLE = 2 if LOW_PAIN_IDX else 1\n",
        "HIGH_PAIN_WEIGHT_SCALE = 0.15\n",
        "LOW_PAIN_WEIGHT_SCALE = 0.465\n",
        "NO_PAIN_WEIGHT_SCALE = 2.3\n",
        "HIGH_PAIN_AUGMENTATION_PARAMS = {\n",
        "    'jitter_std': 0.03,\n",
        "    'scale_range': (0.90, 1.10),\n",
        "    'time_mask_prob': 0.30,\n",
        "    'time_mask_ratio': 0.12,\n",
        "    'time_shift_range': 4,\n",
        "    'time_flip_prob': 0.10,\n",
        "}\n",
        "LOW_PAIN_AUGMENTATION_PARAMS = {\n",
        "    'jitter_std': 0.02,\n",
        "    'scale_range': (0.95, 1.05),\n",
        "    'time_mask_prob': 0.20,\n",
        "    'time_mask_ratio': 0.08,\n",
        "    'time_shift_range': 3,\n",
        "    'time_flip_prob': 0.05,\n",
        "}\n",
        "AUGMENTATION_PARAMS = HIGH_PAIN_AUGMENTATION_PARAMS\n",
        "\n",
        "\n",
        "def pivot_timeseries(df: pd.DataFrame, columns: List[str], dtype: np.dtype) -> np.ndarray:\n",
        "    if not columns:\n",
        "        raise ValueError('Nessuna colonna fornita per il pivot timeseries.')\n",
        "    pivoted = (\n",
        "        df.pivot(index='sample_index', columns='time', values=columns)\n",
        "          .sort_index(axis=0)\n",
        "          .sort_index(axis=1, level=1)\n",
        "    )\n",
        "    data = pivoted.to_numpy().reshape(-1, TIME_STEPS, len(columns)).astype(dtype)\n",
        "    return data\n",
        "\n",
        "\n",
        "X_train_numeric = pivot_timeseries(X_train_raw, NUMERIC_FEATURE_COLUMNS, dtype=np.float32)\n",
        "X_test_numeric = pivot_timeseries(X_test_raw, NUMERIC_FEATURE_COLUMNS, dtype=np.float32)\n",
        "\n",
        "if ACTIVE_CATEGORICAL_COLUMNS:\n",
        "    X_train_categorical = pivot_timeseries(X_train_raw, ACTIVE_CATEGORICAL_COLUMNS, dtype=np.int64)\n",
        "    X_test_categorical = pivot_timeseries(X_test_raw, ACTIVE_CATEGORICAL_COLUMNS, dtype=np.int64)\n",
        "else:\n",
        "    X_train_categorical = None\n",
        "    X_test_categorical = None\n",
        "\n",
        "y_train_idx = (\n",
        "    y_train.set_index('sample_index')\n",
        "    .loc[pd.unique(X_train_raw['sample_index'])]['label']\n",
        "    .map(LABEL2IDX)\n",
        "    .to_numpy()\n",
        ")\n",
        "\n",
        "print('Numeric tensors:', X_train_numeric.shape, X_test_numeric.shape)\n",
        "if X_train_categorical is not None:\n",
        "    print('Categorical tensors:', X_train_categorical.shape, X_test_categorical.shape)\n",
        "print('Labels:', y_train_idx.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 308,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "utrbBxCPTBGU",
        "outputId": "e0282315-5755-4010-e539-2b9e291a3d56"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Class counts: [ 56  94 511]\n",
            "Class weights (normalized): [0.264 0.487 0.443]\n"
          ]
        }
      ],
      "source": [
        "class_counts = np.bincount(y_train_idx, minlength=NUM_CLASSES)\n",
        "class_weights = class_counts.sum() / (class_counts + 1e-6)\n",
        "class_weights = class_weights / class_weights.mean()\n",
        "if HIGH_PAIN_IDX:\n",
        "    for idx in HIGH_PAIN_IDX:\n",
        "        class_weights[idx] *= HIGH_PAIN_WEIGHT_SCALE\n",
        "if LOW_PAIN_IDX:\n",
        "    for idx in LOW_PAIN_IDX:\n",
        "        class_weights[idx] *= LOW_PAIN_WEIGHT_SCALE\n",
        "if NO_PAIN_IDX:\n",
        "    for idx in NO_PAIN_IDX:\n",
        "        class_weights[idx] *= NO_PAIN_WEIGHT_SCALE\n",
        "CLASS_COUNTS = class_counts\n",
        "CLASS_WEIGHTS = torch.tensor(class_weights, dtype=torch.float32)\n",
        "print('Class counts:', class_counts)\n",
        "print('Class weights (normalized):', np.round(class_weights, 3))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 309,
      "metadata": {
        "id": "RwZ3Y71eoTB7"
      },
      "outputs": [],
      "source": [
        "def set_class_weight_scales(high_scale: float, low_scale: float, no_scale: float) -> np.ndarray:\n",
        "    \"\"\"Aggiorna gli scale factor globali e ricalcola i pesi di classe normalizzati.\"\"\"\n",
        "    global HIGH_PAIN_WEIGHT_SCALE, LOW_PAIN_WEIGHT_SCALE, NO_PAIN_WEIGHT_SCALE, CLASS_WEIGHTS\n",
        "    HIGH_PAIN_WEIGHT_SCALE = float(high_scale)\n",
        "    LOW_PAIN_WEIGHT_SCALE = float(low_scale)\n",
        "    NO_PAIN_WEIGHT_SCALE = float(no_scale)\n",
        "\n",
        "    class_counts = CLASS_COUNTS\n",
        "    class_weights = class_counts.sum() / (class_counts + 1e-6)\n",
        "    class_weights = class_weights / class_weights.mean()\n",
        "    if HIGH_PAIN_IDX:\n",
        "        for idx in HIGH_PAIN_IDX:\n",
        "            class_weights[idx] *= HIGH_PAIN_WEIGHT_SCALE\n",
        "    if LOW_PAIN_IDX:\n",
        "        for idx in LOW_PAIN_IDX:\n",
        "            class_weights[idx] *= LOW_PAIN_WEIGHT_SCALE\n",
        "    if NO_PAIN_IDX:\n",
        "        for idx in NO_PAIN_IDX:\n",
        "            class_weights[idx] *= NO_PAIN_WEIGHT_SCALE\n",
        "    CLASS_WEIGHTS = torch.tensor(class_weights, dtype=torch.float32)\n",
        "    return class_weights\n",
        "\n",
        "\n",
        "def run_weight_scale_search(\n",
        "    high_values: Sequence[float],\n",
        "    low_values: Sequence[float],\n",
        "    no_values: Sequence[float],\n",
        "    gamma_values: Sequence[float],\n",
        "    *,\n",
        "    base_overrides: Optional[Dict[str, Any]] = None,\n",
        "    max_trials: Optional[int] = None,\n",
        "    folds: int = 10,\n",
        "    random_seed: int = SEED,\n",
        "    save_path: Optional[Path] = None,\n",
        "    verbose: bool = False,\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"Esegue una ricerca sistematica/sottocampionata sui weight scale e sul gamma della FocalLoss.\"\"\"\n",
        "    combos = list(product(high_values, low_values, no_values, gamma_values))\n",
        "    rng = random.Random(random_seed)\n",
        "    rng.shuffle(combos)\n",
        "    if max_trials is not None:\n",
        "        combos = combos[:max_trials]\n",
        "\n",
        "    search_records: List[Dict[str, Any]] = []\n",
        "    for trial_idx, (high_scale, low_scale, no_scale, gamma) in enumerate(combos, start=1):\n",
        "        updated_weights = set_class_weight_scales(high_scale, low_scale, no_scale)\n",
        "        overrides = {\n",
        "            'focal_gamma': gamma,\n",
        "            'loss_type': 'focal',\n",
        "        }\n",
        "        if base_overrides:\n",
        "            overrides.update(base_overrides)\n",
        "        cfg = prepare_config(f\"WEIGHT_SEARCH_{trial_idx:03d}\", overrides)\n",
        "\n",
        "        log_path = None\n",
        "        if save_path is not None:\n",
        "            save_path.parent.mkdir(parents=True, exist_ok=True)\n",
        "            log_path = save_path.with_suffix('.log')\n",
        "\n",
        "        cv_results = run_cross_validation(\n",
        "            cfg,\n",
        "            n_splits=folds,\n",
        "            shuffle=True,\n",
        "            random_state=random_seed,\n",
        "            verbose=verbose,\n",
        "            log_path=log_path,\n",
        "        )\n",
        "        fold_scores = [res['best_f1'] for res in cv_results]\n",
        "        fold_acc = [res['metrics']['accuracy'] for res in cv_results]\n",
        "        mean_f1 = float(np.mean(fold_scores))\n",
        "        std_f1 = float(np.std(fold_scores))\n",
        "        mean_acc = float(np.mean(fold_acc))\n",
        "        std_acc = float(np.std(fold_acc))\n",
        "\n",
        "        record = {\n",
        "            'trial': trial_idx,\n",
        "            'high_scale': high_scale,\n",
        "            'low_scale': low_scale,\n",
        "            'no_scale': no_scale,\n",
        "            'focal_gamma': gamma,\n",
        "            'mean_f1': mean_f1,\n",
        "            'std_f1': std_f1,\n",
        "            'mean_accuracy': mean_acc,\n",
        "            'std_accuracy': std_acc,\n",
        "            'class_weights': updated_weights.tolist(),\n",
        "        }\n",
        "        search_records.append(record)\n",
        "\n",
        "        if save_path is not None and trial_idx % 10 == 0:\n",
        "            pd.DataFrame(search_records).to_csv(save_path, index=False)\n",
        "\n",
        "    results_df = pd.DataFrame(search_records).sort_values('mean_f1', ascending=False).reset_index(drop=True)\n",
        "    if save_path is not None:\n",
        "        results_df.to_csv(save_path, index=False)\n",
        "    return results_df\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 310,
      "metadata": {
        "id": "ufAfwBqxTBGV"
      },
      "outputs": [],
      "source": [
        "def compute_normalization_stats(data: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:\n",
        "    # data shape: (N, T, F)\n",
        "    num_features = data.shape[-1]\n",
        "    mean = data.reshape(-1, num_features).mean(axis=0)\n",
        "    std = data.reshape(-1, num_features).std(axis=0) + 1e-6\n",
        "    return mean, std\n",
        "\n",
        "\n",
        "def normalize(data: np.ndarray, mean: np.ndarray, std: np.ndarray) -> np.ndarray:\n",
        "    return (data - mean) / std\n",
        "\n",
        "\n",
        "feat_mean, feat_std = compute_normalization_stats(X_train_numeric)\n",
        "X_train_numeric = normalize(X_train_numeric, feat_mean, feat_std)\n",
        "X_test_numeric = normalize(X_test_numeric, feat_mean, feat_std)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 311,
      "metadata": {
        "id": "cPxW7DukTBGV"
      },
      "outputs": [],
      "source": [
        "def make_dataloader_from_arrays(\n",
        "    X_numeric: np.ndarray,\n",
        "    X_categorical: Optional[np.ndarray],\n",
        "    y: Optional[np.ndarray],\n",
        "    batch_size: int,\n",
        "    shuffle: bool,\n",
        "    mode: str,\n",
        "    oversample_factor: int = 1,\n",
        "    augmentation_params: Optional[Dict[str, float]] = None,\n",
        "    window_size: Optional[int] = None,\n",
        "    window_stride: Optional[int] = None,\n",
        ") -> DataLoader:\n",
        "    dataset = TimeSeriesDataset(\n",
        "        numeric_data=X_numeric,\n",
        "        categorical_data=X_categorical,\n",
        "        labels=y,\n",
        "        window_size=window_size,\n",
        "        window_stride=window_stride,\n",
        "        mode=mode,\n",
        "        high_pain_targets=HIGH_PAIN_IDX,\n",
        "        oversample_factor=oversample_factor,\n",
        "        augmentation_params=augmentation_params,\n",
        "    )\n",
        "    return DataLoader(dataset, batch_size=batch_size, shuffle=shuffle, drop_last=False)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 312,
      "metadata": {
        "id": "ruF9R0TzTBGV"
      },
      "outputs": [],
      "source": [
        "class TimeSeriesDataset(Dataset):\n",
        "    def __init__(\n",
        "        self,\n",
        "        numeric_data: np.ndarray,\n",
        "        categorical_data: Optional[np.ndarray] = None,\n",
        "        labels: Optional[np.ndarray] = None,\n",
        "        *,\n",
        "        window_size: Optional[int] = None,\n",
        "        window_stride: Optional[int] = None,\n",
        "        mode: str = 'train',\n",
        "        high_pain_targets: Optional[set] = None,\n",
        "        oversample_factor: int = 1,\n",
        "        augmentation_params: Optional[Dict[str, float]] = None,\n",
        "    ):\n",
        "        self.numeric_data = torch.tensor(numeric_data, dtype=torch.float32)\n",
        "        self.categorical_data = None if categorical_data is None else torch.tensor(categorical_data, dtype=torch.long)\n",
        "        self.labels = None if labels is None else torch.tensor(labels, dtype=torch.long)\n",
        "        self.window_size = window_size\n",
        "        self.window_stride = max(1, window_stride or 1)\n",
        "        self.mode = mode\n",
        "        self.high_pain_targets = set(high_pain_targets or [])\n",
        "        self.augmentation_params = augmentation_params or {}\n",
        "        self.time_steps = self.numeric_data.shape[1]\n",
        "\n",
        "        if mode not in {'train', 'valid', 'test'}:\n",
        "            raise ValueError(f\"Unsupported mode '{mode}'. Use 'train', 'valid', or 'test'.\")\n",
        "        if self.window_size is not None and self.window_size > self.time_steps:\n",
        "            raise ValueError(\n",
        "                f'Window size {self.window_size} exceeds series length {self.time_steps}.'\n",
        "            )\n",
        "\n",
        "        self.indices = self._build_indices(max(1, oversample_factor))\n",
        "\n",
        "    def _build_indices(self, oversample_factor: int) -> List[int]:\n",
        "        base_indices = list(range(len(self.numeric_data)))\n",
        "        if (\n",
        "            self.labels is None\n",
        "            or not self.high_pain_targets\n",
        "            or oversample_factor <= 1\n",
        "        ):\n",
        "            return base_indices\n",
        "\n",
        "        high_indices = [idx for idx in base_indices if int(self.labels[idx].item()) in self.high_pain_targets]\n",
        "        if not high_indices:\n",
        "            return base_indices\n",
        "\n",
        "        extra_indices: List[int] = []\n",
        "        for _ in range(oversample_factor - 1):\n",
        "            extra_indices.extend(high_indices)\n",
        "        return base_indices + extra_indices\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        return len(self.indices)\n",
        "\n",
        "    def __getitem__(self, idx: int):\n",
        "        real_idx = self.indices[idx]\n",
        "        series_numeric = self.numeric_data[real_idx]\n",
        "        series_categorical = None if self.categorical_data is None else self.categorical_data[real_idx]\n",
        "        label = None if self.labels is None else self.labels[real_idx]\n",
        "\n",
        "        if self.window_size is not None and self.mode == 'train':\n",
        "            series_numeric = self._select_window(series_numeric, random_selection=True)\n",
        "            if series_categorical is not None:\n",
        "                series_categorical = self._select_window(series_categorical, random_selection=True)\n",
        "        elif self.window_size is not None:\n",
        "            series_numeric = self._select_window(series_numeric, random_selection=False)\n",
        "            if series_categorical is not None:\n",
        "                series_categorical = self._select_window(series_categorical, random_selection=False)\n",
        "        else:\n",
        "            series_numeric = series_numeric.clone()\n",
        "            if series_categorical is not None:\n",
        "                series_categorical = series_categorical.clone()\n",
        "\n",
        "        if (\n",
        "            self.mode == 'train'\n",
        "            and label is not None\n",
        "            and int(label.item()) in self.high_pain_targets\n",
        "            and self.augmentation_params\n",
        "        ):\n",
        "            series_numeric = self._augment(series_numeric)\n",
        "\n",
        "        if label is None:\n",
        "            return series_numeric, series_categorical\n",
        "        return series_numeric, series_categorical, label\n",
        "\n",
        "    def _select_window(self, series: torch.Tensor, *, random_selection: bool) -> torch.Tensor:\n",
        "        window = self.window_size or series.shape[0]\n",
        "        if window >= series.shape[0]:\n",
        "            return series.clone()\n",
        "\n",
        "        max_start = series.shape[0] - window\n",
        "        if max_start <= 0:\n",
        "            start = 0\n",
        "        else:\n",
        "            stride = max(1, self.window_stride)\n",
        "            positions = list(range(0, max_start + 1, stride))\n",
        "            if positions[-1] != max_start:\n",
        "                positions.append(max_start)\n",
        "            start = random.choice(positions) if random_selection else positions[len(positions) // 2]\n",
        "        end = start + window\n",
        "        return series[start:end].clone()\n",
        "\n",
        "    def _augment(self, series: torch.Tensor) -> torch.Tensor:\n",
        "        augmented = series.clone()\n",
        "        jitter_std = float(self.augmentation_params.get('jitter_std', 0.0))\n",
        "        if jitter_std > 0:\n",
        "            augmented = augmented + torch.randn_like(augmented) * jitter_std\n",
        "\n",
        "        scale_range = self.augmentation_params.get('scale_range')\n",
        "        if scale_range:\n",
        "            low, high = scale_range\n",
        "            scale = random.uniform(low, high)\n",
        "            augmented = augmented * scale\n",
        "\n",
        "        time_shift_range = int(self.augmentation_params.get('time_shift_range', 0))\n",
        "        if time_shift_range > 0:\n",
        "            shift = random.randint(-time_shift_range, time_shift_range)\n",
        "            if shift != 0:\n",
        "                augmented = torch.roll(augmented, shifts=shift, dims=0)\n",
        "\n",
        "        if random.random() < float(self.augmentation_params.get('time_flip_prob', 0.0)):\n",
        "            augmented = torch.flip(augmented, dims=[0])\n",
        "\n",
        "        time_mask_prob = float(self.augmentation_params.get('time_mask_prob', 0.0))\n",
        "        time_mask_ratio = float(self.augmentation_params.get('time_mask_ratio', 0.0))\n",
        "        if time_mask_prob > 0 and time_mask_ratio > 0 and random.random() < time_mask_prob:\n",
        "            mask_len = max(1, int(augmented.shape[0] * time_mask_ratio))\n",
        "            if mask_len < augmented.shape[0]:\n",
        "                start = random.randint(0, augmented.shape[0] - mask_len)\n",
        "                augmented[start:start + mask_len] = 0.0\n",
        "\n",
        "        return augmented\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 313,
      "metadata": {
        "id": "_85J2_tdoTB9"
      },
      "outputs": [],
      "source": [
        "def extract_sliding_windows(series: torch.Tensor, window_size: int, stride: Optional[int]) -> torch.Tensor:\n",
        "    \"\"\"Return a stack of sliding windows for a single time-series tensor.\"\"\"\n",
        "    if window_size <= 0:\n",
        "        raise ValueError('window_size must be a positive integer.')\n",
        "\n",
        "    series_length = series.shape[0]\n",
        "    if window_size >= series_length:\n",
        "        return series.unsqueeze(0)\n",
        "\n",
        "    step = max(1, int(stride or window_size))\n",
        "    max_start = series_length - window_size\n",
        "    starts = list(range(0, max_start + 1, step))\n",
        "    if starts[-1] != max_start:\n",
        "        starts.append(max_start)\n",
        "\n",
        "    windows = [series[start:start + window_size] for start in starts]\n",
        "    return torch.stack(windows, dim=0)\n",
        "\n",
        "\n",
        "class FocalLoss(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        gamma: float = 2.0,\n",
        "        weight: Optional[torch.Tensor] = None,\n",
        "        reduction: str = 'mean',\n",
        "        eps: float = 1e-6,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.gamma = float(gamma)\n",
        "        self.reduction = reduction\n",
        "        self.eps = eps\n",
        "        if weight is not None:\n",
        "            self.register_buffer('weight', weight.clone().detach())\n",
        "        else:\n",
        "            self.register_buffer('weight', None)\n",
        "\n",
        "    def forward(self, inputs: torch.Tensor, targets: torch.Tensor) -> torch.Tensor:\n",
        "        if inputs.ndim < 2:\n",
        "            raise ValueError('FocalLoss expects inputs of shape (N, C, ...)')\n",
        "\n",
        "        log_probs = F.log_softmax(inputs, dim=-1)\n",
        "        probs = log_probs.exp()\n",
        "        one_hot = F.one_hot(targets, num_classes=log_probs.size(-1)).type_as(log_probs)\n",
        "        pt = (probs * one_hot).sum(dim=-1).clamp(min=self.eps)\n",
        "        focal_factor = (1.0 - pt) ** self.gamma\n",
        "        log_pt = (log_probs * one_hot).sum(dim=-1)\n",
        "        loss = -focal_factor * log_pt\n",
        "\n",
        "        if self.weight is not None:\n",
        "            class_weights = self.weight.to(inputs.device)\n",
        "            loss = loss * class_weights[targets]\n",
        "\n",
        "        if self.reduction == 'mean':\n",
        "            return loss.mean()\n",
        "        if self.reduction == 'sum':\n",
        "            return loss.sum()\n",
        "        if self.reduction == 'none':\n",
        "            return loss\n",
        "        raise ValueError(f\"Unsupported reduction '{self.reduction}' for FocalLoss\")\n",
        "\n",
        "\n",
        "def build_loss_fn(config: Dict[str, Any]) -> nn.Module:\n",
        "    \"\"\"Factory per la funzione di loss in base alla configurazione.\"\"\"\n",
        "    loss_type = (config.get('loss_type') or 'ce').lower()\n",
        "    reduction = config.get('loss_reduction', 'mean')\n",
        "    label_smoothing = float(config.get('label_smoothing', 0.0))\n",
        "    weight = CLASS_WEIGHTS.clone().detach() if config.get('use_class_weights', False) else None\n",
        "\n",
        "    if loss_type in {'focal', 'focal_loss'}:\n",
        "        gamma = float(config.get('focal_gamma', 2.0))\n",
        "        return FocalLoss(gamma=gamma, weight=weight, reduction=reduction)\n",
        "    if loss_type in {'ce', 'cross_entropy'}:\n",
        "        return nn.CrossEntropyLoss(weight=weight, label_smoothing=label_smoothing, reduction=reduction)\n",
        "\n",
        "    raise ValueError(f\"Unsupported loss_type '{config.get('loss_type')}'\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 314,
      "metadata": {
        "id": "OSO6lpF1oTB9"
      },
      "outputs": [],
      "source": [
        "class TemporalAttentionPooling(nn.Module):\n",
        "    def __init__(self, input_dim: int, hidden_dim: int = 128, dropout: float = 0.0):\n",
        "        super().__init__()\n",
        "        self.proj = nn.Sequential(\n",
        "            nn.Linear(input_dim, hidden_dim),\n",
        "            nn.Tanh(),\n",
        "            nn.Dropout(dropout),\n",
        "        )\n",
        "        self.score = nn.Linear(hidden_dim, 1)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        # x: (batch, time, features)\n",
        "        attn_logits = self.score(self.proj(x))  # (batch, time, 1)\n",
        "        attn_weights = torch.softmax(attn_logits, dim=1)\n",
        "        context = (attn_weights * x).sum(dim=1)\n",
        "        return context\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 315,
      "metadata": {
        "id": "Sez_5vaUTBGW"
      },
      "outputs": [],
      "source": [
        "def forward_with_sliding_windows(\n",
        "    model: nn.Module,\n",
        "    inputs_numeric: torch.Tensor,\n",
        "    inputs_categorical: Optional[torch.Tensor],\n",
        "    window_size: Optional[int],\n",
        "    stride: Optional[int],\n",
        "    aggregation: str = 'max',\n",
        ") -> torch.Tensor:\n",
        "    if window_size is None or window_size >= inputs_numeric.shape[1]:\n",
        "        return model(inputs_numeric, inputs_categorical)\n",
        "\n",
        "    all_windows_num: List[torch.Tensor] = []\n",
        "    all_windows_cat: List[torch.Tensor] = [] if inputs_categorical is not None else []\n",
        "    window_counts: List[int] = []\n",
        "    for idx in range(inputs_numeric.shape[0]):\n",
        "        sample_num = inputs_numeric[idx]\n",
        "        num_windows = extract_sliding_windows(sample_num, window_size, stride)\n",
        "        all_windows_num.append(num_windows)\n",
        "        if inputs_categorical is not None:\n",
        "            sample_cat = inputs_categorical[idx]\n",
        "            cat_windows = extract_sliding_windows(sample_cat, window_size, stride)\n",
        "            all_windows_cat.append(cat_windows)\n",
        "        window_counts.append(num_windows.shape[0])\n",
        "\n",
        "    stacked_num = torch.cat(all_windows_num, dim=0)\n",
        "    stacked_cat = torch.cat(all_windows_cat, dim=0) if inputs_categorical is not None else None\n",
        "    logits = model(stacked_num, stacked_cat)\n",
        "    chunks = logits.split(window_counts, dim=0)\n",
        "\n",
        "    aggregated_logits: List[torch.Tensor] = []\n",
        "    aggregation = aggregation.lower()\n",
        "    for chunk in chunks:\n",
        "        if aggregation == 'max':\n",
        "            aggregated_logits.append(chunk.max(dim=0).values)\n",
        "        elif aggregation == 'mean':\n",
        "            aggregated_logits.append(chunk.mean(dim=0))\n",
        "        elif aggregation == 'logsumexp':\n",
        "            aggregated_logits.append(torch.logsumexp(chunk, dim=0))\n",
        "        else:\n",
        "            raise ValueError(f\"Unsupported aggregation '{aggregation}'.\")\n",
        "\n",
        "    return torch.stack(aggregated_logits, dim=0)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 316,
      "metadata": {
        "id": "hHmie09aTBGW"
      },
      "outputs": [],
      "source": [
        "def create_dataloaders(\n",
        "    X_numeric: np.ndarray,\n",
        "    X_categorical: Optional[np.ndarray],\n",
        "    y: np.ndarray,\n",
        "    valid_size: float = 0.2,\n",
        "    batch_size: int = 128,\n",
        "    oversample_factor: int = HIGH_PAIN_OVERSAMPLE,\n",
        "    window_size: Optional[int] = WINDOW_SIZE,\n",
        "    window_stride: Optional[int] = WINDOW_STRIDE,\n",
        "    augmentation_params: Optional[Dict[str, float]] = AUGMENTATION_PARAMS,\n",
        "):\n",
        "    if X_categorical is not None:\n",
        "        X_train_num, X_valid_num, X_train_cat, X_valid_cat, y_train, y_valid = train_test_split(\n",
        "            X_numeric,\n",
        "            X_categorical,\n",
        "            y,\n",
        "            test_size=valid_size,\n",
        "            random_state=SEED,\n",
        "            stratify=y,\n",
        "        )\n",
        "    else:\n",
        "        X_train_num, X_valid_num, y_train, y_valid = train_test_split(\n",
        "            X_numeric,\n",
        "            y,\n",
        "            test_size=valid_size,\n",
        "            random_state=SEED,\n",
        "            stratify=y,\n",
        "        )\n",
        "        X_train_cat = X_valid_cat = None\n",
        "\n",
        "    train_loader = make_dataloader_from_arrays(\n",
        "        X_train_num,\n",
        "        X_train_cat,\n",
        "        y_train,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=True,\n",
        "        mode='train',\n",
        "        oversample_factor=oversample_factor,\n",
        "        augmentation_params=augmentation_params,\n",
        "        window_size=window_size,\n",
        "        window_stride=window_stride,\n",
        "    )\n",
        "    valid_loader = make_dataloader_from_arrays(\n",
        "        X_valid_num,\n",
        "        X_valid_cat,\n",
        "        y_valid,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=False,\n",
        "        mode='valid',\n",
        "        window_size=None,\n",
        "        window_stride=None,\n",
        "    )\n",
        "    return train_loader, valid_loader, (X_train_num, X_train_cat, y_train, X_valid_num, X_valid_cat, y_valid)\n",
        "\n",
        "\n",
        "BATCH_SIZE = 128\n",
        "train_loader, valid_loader, (X_train_split, X_train_cat_split, y_train_split, X_valid_split, X_valid_cat_split, y_valid_split) = create_dataloaders(\n",
        "    X_train_numeric,\n",
        "    X_train_categorical,\n",
        "    y_train_idx,\n",
        "    batch_size=BATCH_SIZE,\n",
        ")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 317,
      "metadata": {
        "id": "EVRDa4oMTBGW"
      },
      "outputs": [],
      "source": [
        "class RecurrentBackbone(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        input_size: int,\n",
        "        hidden_size: int = 128,\n",
        "        num_layers: int = 2,\n",
        "        dropout: float = 0.2,\n",
        "        rnn_type: str = 'lstm',\n",
        "        bidirectional: bool = True,\n",
        "        conv_layers: int = 0,\n",
        "        conv_channels: Optional[int] = None,\n",
        "        conv_kernel_size: int = 3,\n",
        "        conv_dropout: float = 0.1,\n",
        "        categorical_cardinalities: Optional[List[int]] = None,\n",
        "        categorical_embed_dims: Optional[List[int]] = None,\n",
        "        attention_type: str = 'additive',\n",
        "        attention_hidden_dim: int = 128,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        if conv_kernel_size % 2 == 0:\n",
        "            raise ValueError('conv_kernel_size should be odd to preserve temporal dimension.')\n",
        "        rnn_cls = {\n",
        "            'rnn': nn.RNN,\n",
        "            'gru': nn.GRU,\n",
        "            'lstm': nn.LSTM,\n",
        "        }[rnn_type.lower()]\n",
        "        self.rnn_type = rnn_type.lower()\n",
        "        self.bidirectional = bidirectional\n",
        "\n",
        "        self.categorical_cardinalities = categorical_cardinalities or []\n",
        "        self.categorical_embed_dims = categorical_embed_dims or []\n",
        "        if self.categorical_cardinalities:\n",
        "            assert len(self.categorical_cardinalities) == len(self.categorical_embed_dims), 'Mismatch tra cardinalit√† ed embed dims.'\n",
        "            self.cat_embeddings = nn.ModuleList([\n",
        "                nn.Embedding(cardinality, embed_dim)\n",
        "                for cardinality, embed_dim in zip(self.categorical_cardinalities, self.categorical_embed_dims)\n",
        "            ])\n",
        "        else:\n",
        "            self.cat_embeddings = nn.ModuleList()\n",
        "\n",
        "        self.attention_type = (attention_type or 'none').lower()\n",
        "        self.attention_pool = None\n",
        "\n",
        "        self.conv_layers = conv_layers\n",
        "        self.conv = None\n",
        "        rnn_input_size = input_size\n",
        "        if conv_layers > 0:\n",
        "            conv_blocks: List[nn.Module] = []\n",
        "            in_channels = input_size\n",
        "            out_channels = conv_channels or input_size\n",
        "            for layer_idx in range(conv_layers):\n",
        "                conv_blocks.append(\n",
        "                    nn.Conv1d(\n",
        "                        in_channels,\n",
        "                        out_channels,\n",
        "                        kernel_size=conv_kernel_size,\n",
        "                        padding=conv_kernel_size // 2,\n",
        "                        bias=False,\n",
        "                    )\n",
        "                )\n",
        "                conv_blocks.append(nn.BatchNorm1d(out_channels))\n",
        "                conv_blocks.append(nn.ReLU())\n",
        "                if conv_dropout > 0:\n",
        "                    conv_blocks.append(nn.Dropout(conv_dropout))\n",
        "                in_channels = out_channels\n",
        "            self.conv = nn.Sequential(*conv_blocks)\n",
        "            rnn_input_size = out_channels\n",
        "\n",
        "        self.rnn = rnn_cls(\n",
        "            input_size=rnn_input_size,\n",
        "            hidden_size=hidden_size,\n",
        "            num_layers=num_layers,\n",
        "            dropout=dropout if num_layers > 1 else 0.0,\n",
        "            batch_first=True,\n",
        "            bidirectional=bidirectional,\n",
        "        )\n",
        "        proj_input = hidden_size * (2 if bidirectional else 1)\n",
        "        if self.attention_type == 'additive':\n",
        "            self.attention_pool = TemporalAttentionPooling(\n",
        "                proj_input,\n",
        "                hidden_dim=attention_hidden_dim,\n",
        "                dropout=dropout,\n",
        "            )\n",
        "        self.head = nn.Sequential(\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(proj_input, hidden_size),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(hidden_size, NUM_CLASSES),\n",
        "        )\n",
        "\n",
        "    def forward(self, x_numeric: torch.Tensor, x_categorical: Optional[torch.Tensor] = None):\n",
        "        x = x_numeric\n",
        "        if self.cat_embeddings:\n",
        "            if x_categorical is None:\n",
        "                raise ValueError('Input categoriali mancanti per il modello con embedding.')\n",
        "            embedded_features = []\n",
        "            for idx, embedding in enumerate(self.cat_embeddings):\n",
        "                embedded_features.append(embedding(x_categorical[..., idx]))\n",
        "            cat_tensor = torch.cat(embedded_features, dim=-1)\n",
        "            x = torch.cat([x, cat_tensor], dim=-1)\n",
        "\n",
        "        if self.conv is not None:\n",
        "            x = x.transpose(1, 2)\n",
        "            x = self.conv(x)\n",
        "            x = x.transpose(1, 2)\n",
        "        out, _ = self.rnn(x)\n",
        "        if self.attention_pool is not None:\n",
        "            pooled = self.attention_pool(out)\n",
        "        elif self.attention_type == 'mean':\n",
        "            pooled = out.mean(dim=1)\n",
        "        else:\n",
        "            pooled = out[:, -1, :]\n",
        "        return self.head(pooled)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 318,
      "metadata": {
        "id": "uFlPWsSPTBGX"
      },
      "outputs": [],
      "source": [
        "def compute_classification_metrics(preds: np.ndarray, targets: np.ndarray) -> Dict[str, float]:\n",
        "    accuracy = float((preds == targets).mean())\n",
        "    precision, recall, f1, _ = precision_recall_fscore_support(\n",
        "        targets,\n",
        "        preds,\n",
        "        average='macro',\n",
        "        zero_division=0,\n",
        "    )\n",
        "    return {\n",
        "        'accuracy': accuracy,\n",
        "        'precision': float(precision),\n",
        "        'recall': float(recall),\n",
        "        'f1': float(f1),\n",
        "    }\n",
        "\n",
        "\n",
        "def train_one_epoch(\n",
        "    model: nn.Module,\n",
        "    loader: DataLoader,\n",
        "    criterion: nn.Module,\n",
        "    optimizer: torch.optim.Optimizer,\n",
        "    scaler: GradScaler,\n",
        "    max_grad_norm: float = 5.0,\n",
        ") -> Tuple[float, Dict[str, float]]:\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    preds_all, targets_all = [], []\n",
        "    grad_norm_values: List[float] = []\n",
        "\n",
        "    for inputs_numeric, inputs_categorical, targets in loader:\n",
        "        inputs_numeric = inputs_numeric.to(DEVICE)\n",
        "        inputs_categorical = inputs_categorical.to(DEVICE) if inputs_categorical is not None else None\n",
        "        targets = targets.to(DEVICE)\n",
        "        optimizer.zero_grad(set_to_none=True)\n",
        "\n",
        "        with autocast_context():\n",
        "            logits = model(inputs_numeric, inputs_categorical)\n",
        "            loss = criterion(logits, targets)\n",
        "\n",
        "        scaler.scale(loss).backward()\n",
        "        if max_grad_norm is not None:\n",
        "            scaler.unscale_(optimizer)\n",
        "            grad_norm = nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
        "            if torch.isfinite(grad_norm):\n",
        "                grad_norm_values.append(float(grad_norm.detach().cpu()))\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "\n",
        "        running_loss += loss.item() * inputs_numeric.size(0)\n",
        "        preds_all.append(torch.argmax(logits.detach(), dim=1).cpu())\n",
        "        targets_all.append(targets.cpu())\n",
        "\n",
        "    preds_np = torch.cat(preds_all).numpy()\n",
        "    targets_np = torch.cat(targets_all).numpy()\n",
        "    metrics = compute_classification_metrics(preds_np, targets_np)\n",
        "    if grad_norm_values:\n",
        "        metrics['grad_norm'] = float(np.mean(grad_norm_values))\n",
        "    avg_loss = running_loss / len(loader.dataset)\n",
        "    return avg_loss, metrics\n",
        "\n",
        "\n",
        "def evaluate_epoch(\n",
        "    model: nn.Module,\n",
        "    loader: DataLoader,\n",
        "    criterion: nn.Module,\n",
        "    eval_window_size: Optional[int],\n",
        "    eval_window_stride: Optional[int],\n",
        "    aggregation: str,\n",
        ") -> Tuple[float, Dict[str, float], np.ndarray, np.ndarray]:\n",
        "    model.eval()\n",
        "    running_loss = 0.0\n",
        "    preds_all, targets_all = [], []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs_numeric, inputs_categorical, targets in loader:\n",
        "            inputs_numeric = inputs_numeric.to(DEVICE)\n",
        "            inputs_categorical = inputs_categorical.to(DEVICE) if inputs_categorical is not None else None\n",
        "            targets = targets.to(DEVICE)\n",
        "            with autocast_context():\n",
        "                logits = forward_with_sliding_windows(\n",
        "                    model,\n",
        "                    inputs_numeric,\n",
        "                    inputs_categorical,\n",
        "                    eval_window_size,\n",
        "                    eval_window_stride,\n",
        "                    aggregation,\n",
        "                )\n",
        "                loss = criterion(logits, targets)\n",
        "\n",
        "            running_loss += loss.item() * targets.size(0)\n",
        "            preds_all.append(torch.argmax(logits, dim=1).cpu())\n",
        "            targets_all.append(targets.cpu())\n",
        "\n",
        "    preds_np = torch.cat(preds_all).numpy()\n",
        "    targets_np = torch.cat(targets_all).numpy()\n",
        "    metrics = compute_classification_metrics(preds_np, targets_np)\n",
        "    avg_loss = running_loss / len(loader.dataset)\n",
        "    return avg_loss, metrics, preds_np, targets_np\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 319,
      "metadata": {
        "id": "x43v2OC9oTB-"
      },
      "outputs": [],
      "source": [
        "def create_dataloaders_3way(\n",
        "    X_numeric: np.ndarray,\n",
        "    X_categorical: Optional[np.ndarray],\n",
        "    y: np.ndarray,\n",
        "    batch_size: int = 128,\n",
        "    oversample_factor: int = HIGH_PAIN_OVERSAMPLE,\n",
        "    window_size: Optional[int] = WINDOW_SIZE,\n",
        "    window_stride: Optional[int] = WINDOW_STRIDE,\n",
        "    augmentation_params: Optional[Dict[str, float]] = AUGMENTATION_PARAMS,\n",
        "    random_state: int = SEED,\n",
        ") -> Tuple[DataLoader, DataLoader, DataLoader, Tuple]:\n",
        "    \"\"\"Crea DataLoader per train/valid/test con split 80/10/10 stratificato.\"\"\"\n",
        "    # 80% train, 20% temp\n",
        "    if X_categorical is not None:\n",
        "        X_train_num, X_temp_num, X_train_cat, X_temp_cat, y_train, y_temp = train_test_split(\n",
        "            X_numeric,\n",
        "            X_categorical,\n",
        "            y,\n",
        "            test_size=0.20,\n",
        "            random_state=random_state,\n",
        "            stratify=y,\n",
        "        )\n",
        "        # split temp 50/50 in valid/test\n",
        "        X_valid_num, X_test_num, X_valid_cat, X_test_cat, y_valid, y_test = train_test_split(\n",
        "            X_temp_num,\n",
        "            X_temp_cat,\n",
        "            y_temp,\n",
        "            test_size=0.50,\n",
        "            random_state=random_state,\n",
        "            stratify=y_temp,\n",
        "        )\n",
        "    else:\n",
        "        X_train_num, X_temp_num, y_train, y_temp = train_test_split(\n",
        "            X_numeric,\n",
        "            y,\n",
        "            test_size=0.20,\n",
        "            random_state=random_state,\n",
        "            stratify=y,\n",
        "        )\n",
        "        X_valid_num, X_test_num, y_valid, y_test = train_test_split(\n",
        "            X_temp_num,\n",
        "            y_temp,\n",
        "            test_size=0.50,\n",
        "            random_state=random_state,\n",
        "            stratify=y_temp,\n",
        "        )\n",
        "        X_train_cat = X_valid_cat = X_test_cat = None\n",
        "\n",
        "    train_loader = make_dataloader_from_arrays(\n",
        "        X_train_num,\n",
        "        X_train_cat,\n",
        "        y_train,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=True,\n",
        "        mode='train',\n",
        "        oversample_factor=oversample_factor,\n",
        "        augmentation_params=augmentation_params,\n",
        "        window_size=window_size,\n",
        "        window_stride=window_stride,\n",
        "    )\n",
        "    valid_loader = make_dataloader_from_arrays(\n",
        "        X_valid_num,\n",
        "        X_valid_cat,\n",
        "        y_valid,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=False,\n",
        "        mode='valid',\n",
        "        window_size=None,\n",
        "        window_stride=None,\n",
        "    )\n",
        "    test_loader = make_dataloader_from_arrays(\n",
        "        X_test_num,\n",
        "        X_test_cat,\n",
        "        y_test,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=False,\n",
        "        mode='valid',\n",
        "        window_size=None,\n",
        "        window_stride=None,\n",
        "    )\n",
        "\n",
        "    return (\n",
        "        train_loader,\n",
        "        valid_loader,\n",
        "        test_loader,\n",
        "        (X_train_num, X_train_cat, y_train, X_valid_num, X_valid_cat, y_valid, X_test_num, X_test_cat, y_test),\n",
        "    )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 320,
      "metadata": {
        "id": "DtrV_06VoTB-"
      },
      "outputs": [],
      "source": [
        "# Override di run_experiment per usare split 80/10/10 e valutazione test\n",
        "\n",
        "def run_experiment(config: Dict) -> Dict:\n",
        "    run_name = config.get('run_name') or f\"{config['rnn_type'].upper()}_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
        "    config = copy.deepcopy(config)\n",
        "    config['run_name'] = run_name\n",
        "\n",
        "    train_loader, valid_loader, test_loader, split = create_dataloaders_3way(\n",
        "        X_train_numeric,\n",
        "        X_train_categorical,\n",
        "        y_train_idx,\n",
        "        batch_size=config['batch_size'],\n",
        "        oversample_factor=config.get('oversample_factor', HIGH_PAIN_OVERSAMPLE),\n",
        "        window_size=config.get('window_size', WINDOW_SIZE),\n",
        "        window_stride=config.get('window_stride', WINDOW_STRIDE),\n",
        "        augmentation_params=config.get('augmentation_params', AUGMENTATION_PARAMS),\n",
        "    )\n",
        "    (X_tr_num, X_tr_cat, y_tr, X_val_num, X_val_cat, y_val, X_te_num, X_te_cat, y_te) = split\n",
        "\n",
        "    model, history, best_state = fit_model(\n",
        "        config,\n",
        "        train_loader,\n",
        "        valid_loader,\n",
        "        run_name=run_name,\n",
        "        tensorboard=config.get('tensorboard', True),\n",
        "    )\n",
        "\n",
        "    best_state['data_split'] = {\n",
        "        'X_train_num': X_tr_num,\n",
        "        'X_train_cat': X_tr_cat,\n",
        "        'y_train': y_tr,\n",
        "        'X_valid_num': X_val_num,\n",
        "        'X_valid_cat': X_val_cat,\n",
        "        'y_valid': y_val,\n",
        "        'X_test_num': X_te_num,\n",
        "        'X_test_cat': X_te_cat,\n",
        "        'y_test': y_te,\n",
        "    }\n",
        "    best_state['model'] = model\n",
        "    best_state['history'] = history\n",
        "\n",
        "    # Test evaluation\n",
        "    model.eval()\n",
        "    all_logits = []\n",
        "    all_targets = []\n",
        "    eval_window_size = config.get('eval_window_size', EVAL_WINDOW_SIZE)\n",
        "    eval_window_stride = config.get('eval_window_stride', EVAL_WINDOW_STRIDE)\n",
        "    eval_aggregation = config.get('eval_aggregation', EVAL_AGGREGATION)\n",
        "    with torch.no_grad():\n",
        "        for inputs_numeric, inputs_categorical, targets in test_loader:\n",
        "            inputs_numeric = inputs_numeric.to(DEVICE)\n",
        "            inputs_categorical = inputs_categorical.to(DEVICE) if inputs_categorical is not None else None\n",
        "            targets = targets.to(DEVICE)\n",
        "            with autocast_context():\n",
        "                logits = forward_with_sliding_windows(\n",
        "                    model,\n",
        "                    inputs_numeric,\n",
        "                    inputs_categorical,\n",
        "                    eval_window_size,\n",
        "                    eval_window_stride,\n",
        "                    eval_aggregation,\n",
        "                )\n",
        "            all_logits.append(logits.cpu())\n",
        "            all_targets.append(targets.cpu())\n",
        "    logits_cat = torch.cat(all_logits, dim=0)\n",
        "    targets_np = torch.cat(all_targets, dim=0).numpy()\n",
        "    preds_np = torch.argmax(logits_cat, dim=1).numpy()\n",
        "    test_metrics = compute_classification_metrics(preds_np, targets_np)\n",
        "    best_state['test_preds'] = preds_np\n",
        "    best_state['test_targets'] = targets_np\n",
        "    best_state['test_metrics'] = test_metrics\n",
        "\n",
        "    return best_state\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 321,
      "metadata": {
        "id": "i8CvXdcDTBGX"
      },
      "outputs": [],
      "source": [
        "def fit_model(\n",
        "    config: Dict,\n",
        "    train_loader: DataLoader,\n",
        "    valid_loader: DataLoader,\n",
        "    run_name: str,\n",
        "    tensorboard: bool = True,\n",
        ") -> Tuple[nn.Module, Dict[str, List[float]], Dict]:\n",
        "    model = RecurrentBackbone(\n",
        "        input_size=MODEL_INPUT_FEATURES,\n",
        "        hidden_size=config['hidden_size'],\n",
        "        num_layers=config['num_layers'],\n",
        "        dropout=config['dropout'],\n",
        "        rnn_type=config['rnn_type'],\n",
        "        bidirectional=config.get('bidirectional', False),\n",
        "        conv_layers=config.get('conv_layers', 0),\n",
        "        conv_channels=config.get('conv_channels'),\n",
        "        conv_kernel_size=config.get('conv_kernel_size', 3),\n",
        "        conv_dropout=config.get('conv_dropout', 0.1),\n",
        "        categorical_cardinalities=CATEGORICAL_CARDINALITY_LIST,\n",
        "        categorical_embed_dims=CATEGORICAL_EMBED_DIM_LIST,\n",
        "    ).to(DEVICE)\n",
        "\n",
        "    criterion = build_loss_fn(config).to(DEVICE)\n",
        "    optimizer = torch.optim.AdamW(\n",
        "        model.parameters(),\n",
        "        lr=config['lr'],\n",
        "        weight_decay=config.get('weight_decay', 0.0),\n",
        "    )\n",
        "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
        "        optimizer,\n",
        "        mode='max',\n",
        "        factor=config.get('scheduler_factor', 0.5),\n",
        "        patience=config.get('scheduler_patience', 3),\n",
        "    )\n",
        "    scaler = create_grad_scaler()\n",
        "\n",
        "    history: Dict[str, List[float]] = {\n",
        "        'train_loss': [],\n",
        "        'train_accuracy': [],\n",
        "        'train_f1': [],\n",
        "        'train_precision': [],\n",
        "        'train_recall': [],\n",
        "        'train_grad_norm': [],\n",
        "        'valid_loss': [],\n",
        "        'valid_accuracy': [],\n",
        "        'valid_f1': [],\n",
        "        'valid_precision': [],\n",
        "        'valid_recall': [],\n",
        "        'lr': [],\n",
        "    }\n",
        "\n",
        "    run_log_dir = (LOG_DIR / run_name).resolve()\n",
        "    writer = SummaryWriter(run_log_dir.as_posix()) if tensorboard else None\n",
        "\n",
        "    best_metric = -np.inf\n",
        "    best_state: Optional[Dict] = None\n",
        "    patience = config.get('patience', 10)\n",
        "    patience_counter = 0\n",
        "    checkpoint_path = (CHECKPOINT_DIR / f'{run_name}.pt').resolve()\n",
        "\n",
        "    eval_window_size = config.get('eval_window_size', EVAL_WINDOW_SIZE)\n",
        "    eval_window_stride = config.get('eval_window_stride', EVAL_WINDOW_STRIDE)\n",
        "    eval_aggregation = config.get('eval_aggregation', EVAL_AGGREGATION)\n",
        "\n",
        "    for epoch in range(1, config['epochs'] + 1):\n",
        "        train_loss, train_metrics = train_one_epoch(\n",
        "            model,\n",
        "            train_loader,\n",
        "            criterion,\n",
        "            optimizer,\n",
        "            scaler,\n",
        "            max_grad_norm=config.get('max_grad_norm', 5.0),\n",
        "        )\n",
        "        valid_loss, valid_metrics, preds, targets = evaluate_epoch(\n",
        "            model,\n",
        "            valid_loader,\n",
        "            criterion,\n",
        "            eval_window_size,\n",
        "            eval_window_stride,\n",
        "            eval_aggregation,\n",
        "        )\n",
        "        scheduler.step(valid_metrics['f1'])\n",
        "\n",
        "        current_lr = optimizer.param_groups[0]['lr']\n",
        "\n",
        "        history['train_loss'].append(train_loss)\n",
        "        history['train_accuracy'].append(train_metrics['accuracy'])\n",
        "        history['train_f1'].append(train_metrics['f1'])\n",
        "        history['train_precision'].append(train_metrics['precision'])\n",
        "        history['train_recall'].append(train_metrics['recall'])\n",
        "        history['train_grad_norm'].append(train_metrics.get('grad_norm', 0.0))\n",
        "        history['valid_loss'].append(valid_loss)\n",
        "        history['valid_accuracy'].append(valid_metrics['accuracy'])\n",
        "        history['valid_f1'].append(valid_metrics['f1'])\n",
        "        history['valid_precision'].append(valid_metrics['precision'])\n",
        "        history['valid_recall'].append(valid_metrics['recall'])\n",
        "        history['lr'].append(current_lr)\n",
        "\n",
        "        if writer is not None:\n",
        "            writer.add_scalar('Loss/train', train_loss, epoch)\n",
        "            writer.add_scalar('Loss/valid', valid_loss, epoch)\n",
        "            writer.add_scalar('F1/train', train_metrics['f1'], epoch)\n",
        "            writer.add_scalar('F1/valid', valid_metrics['f1'], epoch)\n",
        "            writer.add_scalar('Accuracy/train', train_metrics['accuracy'], epoch)\n",
        "            writer.add_scalar('Accuracy/valid', valid_metrics['accuracy'], epoch)\n",
        "            writer.add_scalar('GradNorm/train', train_metrics.get('grad_norm', 0.0), epoch)\n",
        "            writer.add_scalar('LearningRate', current_lr, epoch)\n",
        "\n",
        "        msg = (\n",
        "            f\"Epoch {epoch:03d} | \"\n",
        "            f\"train_loss={train_loss:.4f} acc={train_metrics['accuracy']:.3f} f1={train_metrics['f1']:.3f} | \"\n",
        "            f\"valid_loss={valid_loss:.4f} acc={valid_metrics['accuracy']:.3f} f1={valid_metrics['f1']:.3f} | \"\n",
        "            f\"lr={current_lr:.2e}\"\n",
        "        )\n",
        "        print(msg)\n",
        "\n",
        "        if valid_metrics['f1'] > best_metric + config.get('min_improvement', 0.0):\n",
        "            best_metric = valid_metrics['f1']\n",
        "            patience_counter = 0\n",
        "            best_state = {\n",
        "                'epoch': epoch,\n",
        "                'model_state': copy.deepcopy(model.state_dict()),\n",
        "                'optimizer_state': copy.deepcopy(optimizer.state_dict()),\n",
        "                'metrics': valid_metrics,\n",
        "                'train_metrics': train_metrics,\n",
        "                'preds': preds,\n",
        "                'targets': targets,\n",
        "            }\n",
        "            torch.save(best_state['model_state'], checkpoint_path)\n",
        "        else:\n",
        "            patience_counter += 1\n",
        "            if patience_counter >= patience:\n",
        "                print(f\"Early stopping triggered at epoch {epoch}. Best f1={best_metric:.4f}.\")\n",
        "                break\n",
        "\n",
        "    if writer is not None:\n",
        "        writer.close()\n",
        "\n",
        "    if best_state is None:\n",
        "        best_state = {\n",
        "            'epoch': config['epochs'],\n",
        "            'model_state': copy.deepcopy(model.state_dict()),\n",
        "            'optimizer_state': copy.deepcopy(optimizer.state_dict()),\n",
        "            'metrics': valid_metrics,\n",
        "            'train_metrics': train_metrics,\n",
        "            'preds': preds,\n",
        "            'targets': targets,\n",
        "        }\n",
        "        torch.save(best_state['model_state'], checkpoint_path)\n",
        "\n",
        "    model.load_state_dict(best_state['model_state'])\n",
        "    best_state.update(\n",
        "        {\n",
        "            'run_name': run_name,\n",
        "            'config': copy.deepcopy(config),\n",
        "            'history': history,\n",
        "            'checkpoint_path': checkpoint_path,\n",
        "            'best_f1': best_metric,\n",
        "        }\n",
        "    )\n",
        "    return model, history, best_state\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 322,
      "metadata": {
        "id": "ltbq2G3RTBGX"
      },
      "outputs": [],
      "source": [
        "def run_cross_validation(\n",
        "    config: Dict,\n",
        "    n_splits: int = 5,\n",
        "    shuffle: bool = True,\n",
        "    random_state: int = SEED,\n",
        "    verbose: bool = True,\n",
        "    log_path: Optional[Path] = None,\n",
        ") -> List[Dict]:\n",
        "    skf = StratifiedKFold(n_splits=n_splits, shuffle=shuffle, random_state=random_state)\n",
        "    fold_results: List[Dict] = []\n",
        "\n",
        "    def _log(message: str) -> None:\n",
        "        if verbose:\n",
        "            print(message)\n",
        "        if log_path is not None:\n",
        "            log_path.parent.mkdir(parents=True, exist_ok=True)\n",
        "            with log_path.open('a', encoding='utf-8') as fp:\n",
        "                fp.write(message + '\\n')\n",
        "\n",
        "    _log(\n",
        "        f\"\\n>>> Starting {n_splits}-fold CV for {config['run_name']} ({config['rnn_type'].upper()})\"\n",
        "    )\n",
        "    for fold_idx, (train_idx, valid_idx) in enumerate(skf.split(X_train_numeric, y_train_idx), start=1):\n",
        "        fold_config = copy.deepcopy(config)\n",
        "        fold_config['run_name'] = f\"{config['run_name']}_fold{fold_idx}\"\n",
        "\n",
        "        X_tr_num, X_val_num = X_train_numeric[train_idx], X_train_numeric[valid_idx]\n",
        "        if X_train_categorical is not None:\n",
        "            X_tr_cat, X_val_cat = X_train_categorical[train_idx], X_train_categorical[valid_idx]\n",
        "        else:\n",
        "            X_tr_cat = X_val_cat = None\n",
        "        y_tr, y_val = y_train_idx[train_idx], y_train_idx[valid_idx]\n",
        "\n",
        "        train_loader = make_dataloader_from_arrays(\n",
        "            X_tr_num,\n",
        "            X_tr_cat,\n",
        "            y_tr,\n",
        "            batch_size=fold_config['batch_size'],\n",
        "            shuffle=True,\n",
        "            mode='train',\n",
        "            oversample_factor=fold_config.get('oversample_factor', HIGH_PAIN_OVERSAMPLE),\n",
        "            augmentation_params=fold_config.get('augmentation_params', AUGMENTATION_PARAMS),\n",
        "            window_size=fold_config.get('window_size', WINDOW_SIZE),\n",
        "            window_stride=fold_config.get('window_stride', WINDOW_STRIDE),\n",
        "        )\n",
        "        valid_loader = make_dataloader_from_arrays(\n",
        "            X_val_num,\n",
        "            X_val_cat,\n",
        "            y_val,\n",
        "            batch_size=fold_config['batch_size'],\n",
        "            shuffle=False,\n",
        "            mode='valid',\n",
        "            window_size=None,\n",
        "            window_stride=None,\n",
        "        )\n",
        "\n",
        "        model, history, best_state = fit_model(\n",
        "            fold_config,\n",
        "            train_loader,\n",
        "            valid_loader,\n",
        "            run_name=fold_config['run_name'],\n",
        "            tensorboard=fold_config.get('tensorboard', True),\n",
        "        )\n",
        "        best_state['fold'] = fold_idx\n",
        "        best_state['model'] = model\n",
        "        best_state['history'] = history\n",
        "        fold_results.append(best_state)\n",
        "\n",
        "        metrics = best_state['metrics']\n",
        "        _log(\n",
        "            f\"Fold {fold_idx}/{n_splits} | best F1={metrics['f1']:.4f} | \"\n",
        "            f\"Accuracy={metrics['accuracy']:.4f}\"\n",
        "        )\n",
        "\n",
        "    return fold_results\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 323,
      "metadata": {
        "id": "DVYP7IPEoTB_"
      },
      "outputs": [],
      "source": [
        "def run_smoke_test(\n",
        "    *,\n",
        "    epochs: int = 3,\n",
        "    batch_size: int = 64,\n",
        "    window_size: int = WINDOW_SIZE,\n",
        "    attention: str = 'additive',\n",
        ") -> Dict:\n",
        "    \"\"\"Esegue un training rapidissimo per verificare che la pipeline funzioni.\"\"\"\n",
        "    overrides = {\n",
        "        'run_name': f'SMOKE_TEST_{datetime.now().strftime(\"%H%M%S\")}',\n",
        "        'epochs': epochs,\n",
        "        'batch_size': batch_size,\n",
        "        'patience': max(2, epochs // 2),\n",
        "        'tensorboard': False,\n",
        "        'window_size': window_size,\n",
        "        'eval_window_size': window_size,\n",
        "        'attention_type': attention,\n",
        "        'attention_hidden_dim': 64,\n",
        "    }\n",
        "    cfg = prepare_config('SMOKE_TEST', overrides)\n",
        "    result = run_experiment(cfg)\n",
        "    print(\n",
        "        f\"Smoke test F1={result['best_f1']:.4f} | Acc={result['metrics']['accuracy']:.4f} | \"\n",
        "        f\"Epoch={result['epoch']}\"\n",
        "    )\n",
        "    return result\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 324,
      "metadata": {
        "id": "4MCuWUYeTBGX"
      },
      "outputs": [],
      "source": [
        "def summarize_cv_results(cv_results: List[Dict]) -> pd.DataFrame:\n",
        "    rows = []\n",
        "    for res in cv_results:\n",
        "        cfg = res['config']\n",
        "        metrics = res['metrics']\n",
        "        rows.append({\n",
        "            'run_name': res['run_name'],\n",
        "            'fold': res['fold'],\n",
        "            'rnn_type': cfg['rnn_type'],\n",
        "            'bidirectional': cfg.get('bidirectional', False),\n",
        "            'hidden_size': cfg['hidden_size'],\n",
        "            'num_layers': cfg['num_layers'],\n",
        "            'dropout': cfg['dropout'],\n",
        "            'f1': metrics['f1'],\n",
        "            'accuracy': metrics['accuracy'],\n",
        "            'precision': metrics['precision'],\n",
        "            'recall': metrics['recall'],\n",
        "            'checkpoint': str(res['checkpoint_path']),\n",
        "            'log_dir': str(LOG_DIR / res['run_name']),\n",
        "        })\n",
        "\n",
        "    df = pd.DataFrame(rows)\n",
        "    agg = df.groupby(['rnn_type', 'bidirectional'])[['f1', 'accuracy', 'precision', 'recall']].agg(['mean', 'std'])\n",
        "    agg.columns = ['_'.join(col) for col in agg.columns]\n",
        "    agg = agg.reset_index()\n",
        "    return df, agg\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 325,
      "metadata": {
        "id": "n-MPhw5QoTB_"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import StratifiedShuffleSplit\n",
        "\n",
        "\n",
        "def stratified_train_test_split_arrays(\n",
        "    X_num: np.ndarray,\n",
        "    X_cat: Optional[np.ndarray],\n",
        "    y: np.ndarray,\n",
        "    test_size: float = 0.1,\n",
        "    random_state: int = SEED,\n",
        "):\n",
        "    splitter = StratifiedShuffleSplit(n_splits=1, test_size=test_size, random_state=random_state)\n",
        "    (train_idx, test_idx), = splitter.split(X_num, y)\n",
        "    X_tr_num, X_te_num = X_num[train_idx], X_num[test_idx]\n",
        "    y_tr, y_te = y[train_idx], y[test_idx]\n",
        "    if X_cat is not None:\n",
        "        X_tr_cat, X_te_cat = X_cat[train_idx], X_cat[test_idx]\n",
        "    else:\n",
        "        X_tr_cat = X_te_cat = None\n",
        "    return X_tr_num, X_tr_cat, y_tr, X_te_num, X_te_cat, y_te\n",
        "\n",
        "\n",
        "def run_cross_validation_on_arrays(\n",
        "    config: Dict,\n",
        "    X_num: np.ndarray,\n",
        "    X_cat: Optional[np.ndarray],\n",
        "    y: np.ndarray,\n",
        "    n_splits: int = 5,\n",
        "    shuffle: bool = True,\n",
        "    random_state: int = SEED,\n",
        "    verbose: bool = True,\n",
        "    log_path: Optional[Path] = None,\n",
        "    X_te_num: Optional[np.ndarray] = None,\n",
        "    X_te_cat: Optional[np.ndarray] = None,\n",
        "    y_te: Optional[np.ndarray] = None,\n",
        ") -> List[Dict]:\n",
        "    skf = StratifiedKFold(n_splits=n_splits, shuffle=shuffle, random_state=random_state)\n",
        "    fold_results: List[Dict] = []\n",
        "\n",
        "    def _log(message: str) -> None:\n",
        "        if verbose:\n",
        "            print(message)\n",
        "        if log_path is not None:\n",
        "            log_path.parent.mkdir(parents=True, exist_ok=True)\n",
        "            with log_path.open('a', encoding='utf-8') as fp:\n",
        "                fp.write(message + '\\n')\n",
        "\n",
        "    _log(f\"\\n>>> Starting {n_splits}-fold CV for {config['run_name']} ({config['rnn_type'].upper()}) on TRAIN portion\")\n",
        "    for fold_idx, (train_idx, valid_idx) in enumerate(skf.split(X_num, y), start=1):\n",
        "        fold_config = copy.deepcopy(config)\n",
        "        fold_config['run_name'] = f\"{config['run_name']}_fold{fold_idx}\"\n",
        "\n",
        "        X_tr_num, X_val_num = X_num[train_idx], X_num[valid_idx]\n",
        "        if X_cat is not None:\n",
        "            X_tr_cat, X_val_cat = X_cat[train_idx], X_cat[valid_idx]\n",
        "        else:\n",
        "            X_tr_cat = X_val_cat = None\n",
        "        y_tr, y_val = y[train_idx], y[valid_idx]\n",
        "\n",
        "        train_loader = make_dataloader_from_arrays(\n",
        "            X_tr_num,\n",
        "            X_tr_cat,\n",
        "            y_tr,\n",
        "            batch_size=fold_config['batch_size'],\n",
        "            shuffle=True,\n",
        "            mode='train',\n",
        "            oversample_factor=fold_config.get('oversample_factor', HIGH_PAIN_OVERSAMPLE),\n",
        "            augmentation_params=fold_config.get('augmentation_params', AUGMENTATION_PARAMS),\n",
        "            window_size=fold_config.get('window_size', WINDOW_SIZE),\n",
        "            window_stride=fold_config.get('window_stride', WINDOW_STRIDE),\n",
        "        )\n",
        "        valid_loader = make_dataloader_from_arrays(\n",
        "            X_val_num,\n",
        "            X_val_cat,\n",
        "            y_val,\n",
        "            batch_size=fold_config['batch_size'],\n",
        "            shuffle=False,\n",
        "            mode='valid',\n",
        "            window_size=None,\n",
        "            window_stride=None,\n",
        "        )\n",
        "\n",
        "        model, history, best_state = fit_model(\n",
        "            fold_config,\n",
        "            train_loader,\n",
        "            valid_loader,\n",
        "            run_name=fold_config['run_name'],\n",
        "            tensorboard=fold_config.get('tensorboard', True),\n",
        "        )\n",
        "        best_state['fold'] = fold_idx\n",
        "        best_state['model'] = model\n",
        "        best_state['history'] = history\n",
        "\n",
        "        # If an external TEST set is provided, evaluate this fold's model on it\n",
        "        if X_te_num is not None and y_te is not None:\n",
        "            ext = evaluate_on_external_test(\n",
        "                model,\n",
        "                X_te_num,\n",
        "                X_te_cat,\n",
        "                y_te,\n",
        "                config=fold_config,\n",
        "            )\n",
        "            best_state.update(ext)\n",
        "\n",
        "        fold_results.append(best_state)\n",
        "\n",
        "        metrics = best_state['metrics']\n",
        "        msg = (\n",
        "            f\"Fold {fold_idx}/{n_splits} | best F1(valid)={metrics['f1']:.4f} | \"\n",
        "            f\"Accuracy(valid)={metrics['accuracy']:.4f}\"\n",
        "        )\n",
        "        if 'test_metrics' in best_state:\n",
        "            msg += f\" | Test F1={best_state['test_metrics']['f1']:.4f}\"\n",
        "        _log(msg)\n",
        "\n",
        "    return fold_results\n",
        "\n",
        "\n",
        "def evaluate_on_external_test(\n",
        "    model: nn.Module,\n",
        "    X_te_num: np.ndarray,\n",
        "    X_te_cat: Optional[np.ndarray],\n",
        "    y_te: np.ndarray,\n",
        "    *,\n",
        "    config: Dict,\n",
        ") -> Dict[str, Any]:\n",
        "    test_loader = make_dataloader_from_arrays(\n",
        "        X_te_num,\n",
        "        X_te_cat,\n",
        "        y_te,\n",
        "        batch_size=256,\n",
        "        shuffle=False,\n",
        "        mode='valid',\n",
        "        window_size=None,\n",
        "        window_stride=None,\n",
        "    )\n",
        "\n",
        "    model.eval()\n",
        "    all_logits = []\n",
        "    all_targets = []\n",
        "    eval_window_size = config.get('eval_window_size', EVAL_WINDOW_SIZE)\n",
        "    eval_window_stride = config.get('eval_window_stride', EVAL_WINDOW_STRIDE)\n",
        "    eval_aggregation = config.get('eval_aggregation', EVAL_AGGREGATION)\n",
        "    with torch.no_grad():\n",
        "        for inputs_numeric, inputs_categorical, targets in test_loader:\n",
        "            inputs_numeric = inputs_numeric.to(DEVICE)\n",
        "            inputs_categorical = inputs_categorical.to(DEVICE) if inputs_categorical is not None else None\n",
        "            targets = targets.to(DEVICE)\n",
        "            with autocast_context():\n",
        "                logits = forward_with_sliding_windows(\n",
        "                    model,\n",
        "                    inputs_numeric,\n",
        "                    inputs_categorical,\n",
        "                    eval_window_size,\n",
        "                    eval_window_stride,\n",
        "                    eval_aggregation,\n",
        "                )\n",
        "            all_logits.append(logits.cpu())\n",
        "            all_targets.append(targets.cpu())\n",
        "\n",
        "    logits_cat = torch.cat(all_logits, dim=0)\n",
        "    targets_np = torch.cat(all_targets, dim=0).numpy()\n",
        "    preds_np = torch.argmax(logits_cat, dim=1).numpy()\n",
        "    test_metrics = compute_classification_metrics(preds_np, targets_np)\n",
        "    return {\n",
        "        'test_preds': preds_np,\n",
        "        'test_targets': targets_np,\n",
        "        'test_metrics': test_metrics,\n",
        "    }\n",
        "\n",
        "\n",
        "def train_config_on_train_and_eval_test(\n",
        "    config: Dict,\n",
        "    X_tr_num: np.ndarray,\n",
        "    X_tr_cat: Optional[np.ndarray],\n",
        "    y_tr: np.ndarray,\n",
        "    X_te_num: np.ndarray,\n",
        "    X_te_cat: Optional[np.ndarray],\n",
        "    y_te: np.ndarray,\n",
        ") -> Dict:\n",
        "    run_name = config.get('run_name') or f\"{config['rnn_type'].upper()}_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
        "    config = copy.deepcopy(config)\n",
        "    config['run_name'] = run_name\n",
        "\n",
        "    train_loader, valid_loader, _ = create_dataloaders(\n",
        "        X_tr_num,\n",
        "        X_tr_cat,\n",
        "        y_tr,\n",
        "        valid_size=config.get('valid_size', 0.1),\n",
        "        batch_size=config['batch_size'],\n",
        "        oversample_factor=config.get('oversample_factor', HIGH_PAIN_OVERSAMPLE),\n",
        "        window_size=config.get('window_size', WINDOW_SIZE),\n",
        "        window_stride=config.get('window_stride', WINDOW_STRIDE),\n",
        "        augmentation_params=config.get('augmentation_params', AUGMENTATION_PARAMS),\n",
        "    )\n",
        "\n",
        "    model, history, best_state = fit_model(\n",
        "        config,\n",
        "        train_loader,\n",
        "        valid_loader,\n",
        "        run_name=run_name,\n",
        "        tensorboard=config.get('tensorboard', True),\n",
        "    )\n",
        "\n",
        "    best_state['model'] = model\n",
        "    best_state['history'] = history\n",
        "\n",
        "    # External test evaluation\n",
        "    ext = evaluate_on_external_test(\n",
        "        model,\n",
        "        X_te_num,\n",
        "        X_te_cat,\n",
        "        y_te,\n",
        "        config=config,\n",
        "    )\n",
        "    best_state.update(ext)\n",
        "    best_state['external_test'] = True\n",
        "    return best_state\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 326,
      "metadata": {
        "id": "Muu55EYyoTB_"
      },
      "outputs": [],
      "source": [
        "def prepare_config(name: str, overrides: Dict) -> Dict:\n",
        "    base_config = {\n",
        "        'run_name': name,\n",
        "        'rnn_type': 'gru',\n",
        "        'hidden_size': 128,\n",
        "        'num_layers': 2,\n",
        "        'dropout': 0.4,\n",
        "        'bidirectional': True,\n",
        "        'lr': 2e-3,\n",
        "        'weight_decay': 1e-4,\n",
        "        'epochs': 200,\n",
        "        'batch_size': 64,\n",
        "        'valid_size': 0.1,\n",
        "        'patience': 20,\n",
        "        'max_grad_norm': 5.0,\n",
        "        'scheduler_factor': 0.5,\n",
        "        'scheduler_patience': 5,\n",
        "        'min_improvement': 5e-4,\n",
        "        'tensorboard': True,\n",
        "        'oversample_factor': HIGH_PAIN_OVERSAMPLE,\n",
        "        'window_size': WINDOW_SIZE,\n",
        "        'window_stride': WINDOW_STRIDE,\n",
        "        'eval_window_size': EVAL_WINDOW_SIZE,\n",
        "        'eval_window_stride': EVAL_WINDOW_STRIDE,\n",
        "        'eval_aggregation': EVAL_AGGREGATION,\n",
        "        'loss_type': 'focal',\n",
        "        'use_class_weights': True,\n",
        "        'focal_gamma': 0.75,\n",
        "        'label_smoothing': 0.0,\n",
        "        'augmentation_params': AUGMENTATION_PARAMS,\n",
        "        'conv_layers': 2,\n",
        "        'conv_channels': min(MODEL_INPUT_FEATURES, 128),\n",
        "        'conv_kernel_size': 5,\n",
        "        'conv_dropout': 0.1,\n",
        "        'attention_type': 'additive',\n",
        "        'attention_hidden_dim': 128,\n",
        "    }\n",
        "    config = copy.deepcopy(base_config)\n",
        "    config.update(overrides)\n",
        "    return config\n",
        "\n",
        "\n",
        "def run_experiment(config: Dict) -> Dict:\n",
        "    run_name = config.get('run_name') or f\"{config['rnn_type'].upper()}_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
        "    config = copy.deepcopy(config)\n",
        "    config['run_name'] = run_name\n",
        "\n",
        "    # 80/10/10 held-out split for ALL runs\n",
        "    train_loader, valid_loader, test_loader, split = create_dataloaders_3way(\n",
        "        X_train_numeric,\n",
        "        X_train_categorical,\n",
        "        y_train_idx,\n",
        "        batch_size=config['batch_size'],\n",
        "        oversample_factor=config.get('oversample_factor', HIGH_PAIN_OVERSAMPLE),\n",
        "        window_size=config.get('window_size', WINDOW_SIZE),\n",
        "        window_stride=config.get('window_stride', WINDOW_STRIDE),\n",
        "        augmentation_params=config.get('augmentation_params', AUGMENTATION_PARAMS),\n",
        "    )\n",
        "    (X_tr_num, X_tr_cat, y_tr, X_val_num, X_val_cat, y_val, X_te_num, X_te_cat, y_te) = split\n",
        "\n",
        "    model, history, best_state = fit_model(\n",
        "        config,\n",
        "        train_loader,\n",
        "        valid_loader,\n",
        "        run_name=run_name,\n",
        "        tensorboard=config.get('tensorboard', True),\n",
        "    )\n",
        "\n",
        "    best_state['data_split'] = {\n",
        "        'X_train_num': X_tr_num,\n",
        "        'X_train_cat': X_tr_cat,\n",
        "        'y_train': y_tr,\n",
        "        'X_valid_num': X_val_num,\n",
        "        'X_valid_cat': X_val_cat,\n",
        "        'y_valid': y_val,\n",
        "        'X_test_num': X_te_num,\n",
        "        'X_test_cat': X_te_cat,\n",
        "        'y_test': y_te,\n",
        "    }\n",
        "    best_state['model'] = model\n",
        "    best_state['history'] = history\n",
        "\n",
        "    # Evaluate on held-out test\n",
        "    model.eval()\n",
        "    all_logits = []\n",
        "    all_targets = []\n",
        "    eval_window_size = config.get('eval_window_size', EVAL_WINDOW_SIZE)\n",
        "    eval_window_stride = config.get('eval_window_stride', EVAL_WINDOW_STRIDE)\n",
        "    eval_aggregation = config.get('eval_aggregation', EVAL_AGGREGATION)\n",
        "    with torch.no_grad():\n",
        "        for inputs_numeric, inputs_categorical, targets in test_loader:\n",
        "            inputs_numeric = inputs_numeric.to(DEVICE)\n",
        "            inputs_categorical = inputs_categorical.to(DEVICE) if inputs_categorical is not None else None\n",
        "            targets = targets.to(DEVICE)\n",
        "            with autocast_context():\n",
        "                logits = forward_with_sliding_windows(\n",
        "                    model,\n",
        "                    inputs_numeric,\n",
        "                    inputs_categorical,\n",
        "                    eval_window_size,\n",
        "                    eval_window_stride,\n",
        "                    eval_aggregation,\n",
        "                )\n",
        "            all_logits.append(logits.cpu())\n",
        "            all_targets.append(targets.cpu())\n",
        "    logits_cat = torch.cat(all_logits, dim=0)\n",
        "    targets_np = torch.cat(all_targets, dim=0).numpy()\n",
        "    preds_np = torch.argmax(logits_cat, dim=1).numpy()\n",
        "    test_metrics = compute_classification_metrics(preds_np, targets_np)\n",
        "    best_state['test_preds'] = preds_np\n",
        "    best_state['test_targets'] = targets_np\n",
        "    best_state['test_metrics'] = test_metrics\n",
        "\n",
        "    return best_state\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rCrkiQqCTBGY",
        "outputId": "3f657f9e-6e62-48de-dd7c-5ceaaba9536c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Training (fixed TEST) ‚Äî GRU_BI (GRU - BI) ===\n",
            "Epoch 001 | train_loss=0.2283 acc=0.713 f1=0.278 | valid_loss=0.1811 acc=0.767 f1=0.289 | lr=2.00e-03\n",
            "Epoch 002 | train_loss=0.1724 acc=0.748 f1=0.442 | valid_loss=0.1747 acc=0.767 f1=0.349 | lr=2.00e-03\n",
            "Epoch 003 | train_loss=0.1411 acc=0.782 f1=0.528 | valid_loss=0.1431 acc=0.783 f1=0.456 | lr=2.00e-03\n",
            "Epoch 004 | train_loss=0.1295 acc=0.820 f1=0.659 | valid_loss=0.1601 acc=0.750 f1=0.345 | lr=2.00e-03\n",
            "Epoch 005 | train_loss=0.1340 acc=0.822 f1=0.663 | valid_loss=0.1523 acc=0.800 f1=0.542 | lr=2.00e-03\n",
            "Epoch 006 | train_loss=0.1152 acc=0.832 f1=0.678 | valid_loss=0.1665 acc=0.800 f1=0.495 | lr=2.00e-03\n",
            "Epoch 007 | train_loss=0.1119 acc=0.865 f1=0.753 | valid_loss=0.1387 acc=0.800 f1=0.542 | lr=2.00e-03\n",
            "Epoch 008 | train_loss=0.1035 acc=0.841 f1=0.706 | valid_loss=0.1617 acc=0.833 f1=0.632 | lr=2.00e-03\n",
            "Epoch 009 | train_loss=0.1141 acc=0.858 f1=0.742 | valid_loss=0.1425 acc=0.833 f1=0.644 | lr=2.00e-03\n",
            "Epoch 010 | train_loss=0.1181 acc=0.839 f1=0.721 | valid_loss=0.1136 acc=0.833 f1=0.616 | lr=2.00e-03\n",
            "Epoch 011 | train_loss=0.0965 acc=0.864 f1=0.769 | valid_loss=0.1138 acc=0.850 f1=0.553 | lr=2.00e-03\n",
            "Epoch 012 | train_loss=0.0843 acc=0.889 f1=0.822 | valid_loss=0.1370 acc=0.817 f1=0.559 | lr=2.00e-03\n",
            "Epoch 013 | train_loss=0.1117 acc=0.848 f1=0.728 | valid_loss=0.1507 acc=0.800 f1=0.597 | lr=2.00e-03\n",
            "Epoch 014 | train_loss=0.1010 acc=0.836 f1=0.706 | valid_loss=0.1445 acc=0.833 f1=0.629 | lr=2.00e-03\n",
            "Epoch 015 | train_loss=0.0896 acc=0.877 f1=0.800 | valid_loss=0.1391 acc=0.867 f1=0.668 | lr=2.00e-03\n",
            "Epoch 016 | train_loss=0.0791 acc=0.888 f1=0.816 | valid_loss=0.1360 acc=0.850 f1=0.691 | lr=2.00e-03\n",
            "Epoch 017 | train_loss=0.0908 acc=0.870 f1=0.793 | valid_loss=0.1360 acc=0.850 f1=0.691 | lr=2.00e-03\n",
            "Epoch 018 | train_loss=0.0842 acc=0.889 f1=0.822 | valid_loss=0.1069 acc=0.900 f1=0.801 | lr=2.00e-03\n",
            "Epoch 019 | train_loss=0.0811 acc=0.889 f1=0.811 | valid_loss=0.1098 acc=0.883 f1=0.743 | lr=2.00e-03\n",
            "Epoch 020 | train_loss=0.0761 acc=0.896 f1=0.838 | valid_loss=0.1242 acc=0.850 f1=0.636 | lr=2.00e-03\n",
            "Epoch 021 | train_loss=0.0657 acc=0.905 f1=0.850 | valid_loss=0.1208 acc=0.850 f1=0.648 | lr=2.00e-03\n",
            "Epoch 022 | train_loss=0.0796 acc=0.869 f1=0.801 | valid_loss=0.1145 acc=0.833 f1=0.633 | lr=2.00e-03\n",
            "Epoch 023 | train_loss=0.0789 acc=0.886 f1=0.809 | valid_loss=0.1099 acc=0.867 f1=0.733 | lr=2.00e-03\n",
            "Epoch 024 | train_loss=0.0710 acc=0.898 f1=0.833 | valid_loss=0.0982 acc=0.867 f1=0.668 | lr=1.00e-03\n",
            "Epoch 025 | train_loss=0.0809 acc=0.898 f1=0.830 | valid_loss=0.1069 acc=0.850 f1=0.691 | lr=1.00e-03\n",
            "Epoch 026 | train_loss=0.0696 acc=0.912 f1=0.862 | valid_loss=0.1118 acc=0.867 f1=0.738 | lr=1.00e-03\n",
            "Epoch 027 | train_loss=0.0713 acc=0.903 f1=0.852 | valid_loss=0.1189 acc=0.867 f1=0.711 | lr=1.00e-03\n",
            "Epoch 028 | train_loss=0.0561 acc=0.924 f1=0.884 | valid_loss=0.1032 acc=0.900 f1=0.817 | lr=1.00e-03\n",
            "Epoch 029 | train_loss=0.0508 acc=0.924 f1=0.886 | valid_loss=0.1062 acc=0.867 f1=0.711 | lr=1.00e-03\n",
            "Epoch 030 | train_loss=0.0593 acc=0.903 f1=0.843 | valid_loss=0.1041 acc=0.867 f1=0.667 | lr=1.00e-03\n",
            "Epoch 031 | train_loss=0.0546 acc=0.919 f1=0.873 | valid_loss=0.1045 acc=0.883 f1=0.743 | lr=1.00e-03\n",
            "Epoch 032 | train_loss=0.0408 acc=0.938 f1=0.906 | valid_loss=0.1078 acc=0.883 f1=0.743 | lr=1.00e-03\n",
            "Epoch 033 | train_loss=0.0467 acc=0.941 f1=0.909 | valid_loss=0.1132 acc=0.900 f1=0.801 | lr=1.00e-03\n",
            "Epoch 034 | train_loss=0.0540 acc=0.924 f1=0.878 | valid_loss=0.1185 acc=0.850 f1=0.714 | lr=5.00e-04\n",
            "Epoch 035 | train_loss=0.0516 acc=0.931 f1=0.895 | valid_loss=0.1187 acc=0.867 f1=0.731 | lr=5.00e-04\n",
            "Epoch 036 | train_loss=0.0483 acc=0.938 f1=0.904 | valid_loss=0.1119 acc=0.867 f1=0.675 | lr=5.00e-04\n",
            "Epoch 037 | train_loss=0.0358 acc=0.948 f1=0.920 | valid_loss=0.1061 acc=0.867 f1=0.668 | lr=5.00e-04\n",
            "Epoch 038 | train_loss=0.0594 acc=0.931 f1=0.892 | valid_loss=0.0965 acc=0.883 f1=0.743 | lr=5.00e-04\n",
            "Epoch 039 | train_loss=0.0469 acc=0.929 f1=0.890 | valid_loss=0.1167 acc=0.867 f1=0.668 | lr=5.00e-04\n",
            "Epoch 040 | train_loss=0.0475 acc=0.941 f1=0.908 | valid_loss=0.1164 acc=0.867 f1=0.668 | lr=2.50e-04\n",
            "Epoch 041 | train_loss=0.0448 acc=0.933 f1=0.900 | valid_loss=0.1072 acc=0.883 f1=0.743 | lr=2.50e-04\n",
            "Epoch 042 | train_loss=0.0434 acc=0.945 f1=0.914 | valid_loss=0.1013 acc=0.883 f1=0.750 | lr=2.50e-04\n",
            "Epoch 043 | train_loss=0.0381 acc=0.950 f1=0.926 | valid_loss=0.1003 acc=0.900 f1=0.771 | lr=2.50e-04\n",
            "Epoch 044 | train_loss=0.0378 acc=0.952 f1=0.923 | valid_loss=0.1066 acc=0.867 f1=0.675 | lr=2.50e-04\n",
            "Epoch 045 | train_loss=0.0393 acc=0.953 f1=0.926 | valid_loss=0.1057 acc=0.883 f1=0.696 | lr=2.50e-04\n",
            "Epoch 046 | train_loss=0.0353 acc=0.948 f1=0.918 | valid_loss=0.1046 acc=0.867 f1=0.675 | lr=1.25e-04\n",
            "Epoch 047 | train_loss=0.0349 acc=0.945 f1=0.912 | valid_loss=0.1075 acc=0.883 f1=0.696 | lr=1.25e-04\n",
            "Epoch 048 | train_loss=0.0343 acc=0.945 f1=0.917 | valid_loss=0.1116 acc=0.867 f1=0.668 | lr=1.25e-04\n",
            "Early stopping triggered at epoch 48. Best f1=0.8173.\n",
            "Valid best F1: 0.8173 at epoch 28 | Test F1: 0.8346 | Acc: 0.9000\n",
            "\n",
            ">>> Starting 5-fold CV for GRU_BI (GRU) on TRAIN portion\n",
            "Epoch 001 | train_loss=0.2582 acc=0.643 f1=0.327 | valid_loss=0.2093 acc=0.782 f1=0.329 | lr=2.00e-03\n",
            "Epoch 002 | train_loss=0.1759 acc=0.742 f1=0.412 | valid_loss=0.1489 acc=0.790 f1=0.379 | lr=2.00e-03\n",
            "Epoch 003 | train_loss=0.1410 acc=0.786 f1=0.551 | valid_loss=0.1592 acc=0.773 f1=0.337 | lr=2.00e-03\n",
            "Epoch 004 | train_loss=0.1350 acc=0.804 f1=0.610 | valid_loss=0.1226 acc=0.832 f1=0.639 | lr=2.00e-03\n",
            "Epoch 005 | train_loss=0.1248 acc=0.816 f1=0.622 | valid_loss=0.1138 acc=0.874 f1=0.693 | lr=2.00e-03\n",
            "Epoch 006 | train_loss=0.1240 acc=0.823 f1=0.689 | valid_loss=0.1482 acc=0.807 f1=0.489 | lr=2.00e-03\n",
            "Epoch 007 | train_loss=0.1245 acc=0.841 f1=0.713 | valid_loss=0.1263 acc=0.832 f1=0.629 | lr=2.00e-03\n",
            "Epoch 008 | train_loss=0.1090 acc=0.854 f1=0.735 | valid_loss=0.1126 acc=0.840 f1=0.571 | lr=2.00e-03\n",
            "Epoch 009 | train_loss=0.1156 acc=0.827 f1=0.691 | valid_loss=0.1180 acc=0.857 f1=0.676 | lr=2.00e-03\n",
            "Epoch 010 | train_loss=0.1037 acc=0.868 f1=0.761 | valid_loss=0.1059 acc=0.866 f1=0.729 | lr=2.00e-03\n",
            "Epoch 011 | train_loss=0.0893 acc=0.880 f1=0.785 | valid_loss=0.1165 acc=0.866 f1=0.649 | lr=2.00e-03\n",
            "Epoch 012 | train_loss=0.1062 acc=0.849 f1=0.698 | valid_loss=0.1034 acc=0.874 f1=0.690 | lr=2.00e-03\n",
            "Epoch 013 | train_loss=0.1116 acc=0.854 f1=0.751 | valid_loss=0.0863 acc=0.874 f1=0.703 | lr=2.00e-03\n",
            "Epoch 014 | train_loss=0.1162 acc=0.850 f1=0.747 | valid_loss=0.1073 acc=0.866 f1=0.708 | lr=2.00e-03\n",
            "Epoch 015 | train_loss=0.0955 acc=0.883 f1=0.787 | valid_loss=0.0993 acc=0.866 f1=0.692 | lr=2.00e-03\n",
            "Epoch 016 | train_loss=0.0925 acc=0.897 f1=0.823 | valid_loss=0.1094 acc=0.849 f1=0.737 | lr=2.00e-03\n",
            "Epoch 017 | train_loss=0.0791 acc=0.895 f1=0.827 | valid_loss=0.0770 acc=0.874 f1=0.658 | lr=2.00e-03\n",
            "Epoch 018 | train_loss=0.0769 acc=0.880 f1=0.790 | valid_loss=0.0791 acc=0.891 f1=0.760 | lr=2.00e-03\n",
            "Epoch 019 | train_loss=0.0829 acc=0.891 f1=0.838 | valid_loss=0.0767 acc=0.899 f1=0.752 | lr=2.00e-03\n",
            "Epoch 020 | train_loss=0.0913 acc=0.883 f1=0.798 | valid_loss=0.0709 acc=0.899 f1=0.695 | lr=2.00e-03\n",
            "Epoch 021 | train_loss=0.1066 acc=0.837 f1=0.655 | valid_loss=0.0820 acc=0.899 f1=0.742 | lr=2.00e-03\n",
            "Epoch 022 | train_loss=0.0903 acc=0.870 f1=0.771 | valid_loss=0.0700 acc=0.899 f1=0.759 | lr=2.00e-03\n",
            "Epoch 023 | train_loss=0.0965 acc=0.852 f1=0.736 | valid_loss=0.0777 acc=0.874 f1=0.665 | lr=2.00e-03\n",
            "Epoch 024 | train_loss=0.0823 acc=0.889 f1=0.813 | valid_loss=0.0782 acc=0.916 f1=0.805 | lr=2.00e-03\n",
            "Epoch 025 | train_loss=0.0829 acc=0.887 f1=0.828 | valid_loss=0.0850 acc=0.908 f1=0.795 | lr=2.00e-03\n",
            "Epoch 026 | train_loss=0.0761 acc=0.903 f1=0.846 | valid_loss=0.0730 acc=0.924 f1=0.815 | lr=2.00e-03\n",
            "Epoch 027 | train_loss=0.0878 acc=0.854 f1=0.699 | valid_loss=0.0849 acc=0.891 f1=0.691 | lr=2.00e-03\n",
            "Epoch 028 | train_loss=0.0770 acc=0.883 f1=0.789 | valid_loss=0.0739 acc=0.899 f1=0.760 | lr=2.00e-03\n",
            "Epoch 029 | train_loss=0.0700 acc=0.917 f1=0.852 | valid_loss=0.0666 acc=0.899 f1=0.701 | lr=2.00e-03\n",
            "Epoch 030 | train_loss=0.0754 acc=0.913 f1=0.846 | valid_loss=0.0855 acc=0.899 f1=0.758 | lr=2.00e-03\n",
            "Epoch 031 | train_loss=0.0624 acc=0.936 f1=0.901 | valid_loss=0.0853 acc=0.899 f1=0.758 | lr=2.00e-03\n",
            "Epoch 032 | train_loss=0.0701 acc=0.903 f1=0.858 | valid_loss=0.0687 acc=0.899 f1=0.760 | lr=1.00e-03\n",
            "Epoch 033 | train_loss=0.0647 acc=0.920 f1=0.867 | valid_loss=0.0837 acc=0.874 f1=0.658 | lr=1.00e-03\n",
            "Epoch 034 | train_loss=0.0546 acc=0.907 f1=0.850 | valid_loss=0.0774 acc=0.908 f1=0.770 | lr=1.00e-03\n",
            "Epoch 035 | train_loss=0.0531 acc=0.932 f1=0.897 | valid_loss=0.0806 acc=0.899 f1=0.751 | lr=1.00e-03\n",
            "Epoch 036 | train_loss=0.0484 acc=0.930 f1=0.899 | valid_loss=0.0735 acc=0.899 f1=0.704 | lr=1.00e-03\n",
            "Epoch 037 | train_loss=0.0640 acc=0.913 f1=0.876 | valid_loss=0.0786 acc=0.899 f1=0.706 | lr=1.00e-03\n",
            "Epoch 038 | train_loss=0.0430 acc=0.938 f1=0.908 | valid_loss=0.0652 acc=0.899 f1=0.704 | lr=5.00e-04\n",
            "Epoch 039 | train_loss=0.0439 acc=0.938 f1=0.905 | valid_loss=0.0666 acc=0.899 f1=0.704 | lr=5.00e-04\n",
            "Epoch 040 | train_loss=0.0480 acc=0.930 f1=0.892 | valid_loss=0.0684 acc=0.899 f1=0.704 | lr=5.00e-04\n",
            "Epoch 041 | train_loss=0.0495 acc=0.934 f1=0.902 | valid_loss=0.0808 acc=0.882 f1=0.676 | lr=5.00e-04\n",
            "Epoch 042 | train_loss=0.0560 acc=0.924 f1=0.882 | valid_loss=0.0717 acc=0.891 f1=0.686 | lr=5.00e-04\n",
            "Epoch 043 | train_loss=0.0454 acc=0.938 f1=0.901 | valid_loss=0.0676 acc=0.899 f1=0.704 | lr=5.00e-04\n",
            "Epoch 044 | train_loss=0.0413 acc=0.948 f1=0.923 | valid_loss=0.0667 acc=0.899 f1=0.701 | lr=2.50e-04\n",
            "Epoch 045 | train_loss=0.0314 acc=0.951 f1=0.926 | valid_loss=0.0657 acc=0.899 f1=0.701 | lr=2.50e-04\n",
            "Epoch 046 | train_loss=0.0456 acc=0.940 f1=0.907 | valid_loss=0.0672 acc=0.899 f1=0.733 | lr=2.50e-04\n",
            "Early stopping triggered at epoch 46. Best f1=0.8149.\n",
            "Fold 1/5 | best F1(valid)=0.8149 | Accuracy(valid)=0.9244 | Test F1=0.7800\n",
            "Epoch 001 | train_loss=0.2568 acc=0.643 f1=0.309 | valid_loss=0.2164 acc=0.790 f1=0.379 | lr=2.00e-03\n",
            "Epoch 002 | train_loss=0.1869 acc=0.744 f1=0.446 | valid_loss=0.1526 acc=0.815 f1=0.443 | lr=2.00e-03\n",
            "Epoch 003 | train_loss=0.1656 acc=0.748 f1=0.479 | valid_loss=0.1259 acc=0.798 f1=0.412 | lr=2.00e-03\n",
            "Epoch 004 | train_loss=0.1501 acc=0.746 f1=0.421 | valid_loss=0.1358 acc=0.807 f1=0.495 | lr=2.00e-03\n",
            "Epoch 005 | train_loss=0.1398 acc=0.777 f1=0.532 | valid_loss=0.1301 acc=0.824 f1=0.522 | lr=2.00e-03\n",
            "Epoch 006 | train_loss=0.1246 acc=0.802 f1=0.604 | valid_loss=0.1090 acc=0.857 f1=0.639 | lr=2.00e-03\n",
            "Epoch 007 | train_loss=0.1290 acc=0.808 f1=0.637 | valid_loss=0.1166 acc=0.857 f1=0.696 | lr=2.00e-03\n",
            "Epoch 008 | train_loss=0.1295 acc=0.816 f1=0.644 | valid_loss=0.1160 acc=0.849 f1=0.686 | lr=2.00e-03\n",
            "Epoch 009 | train_loss=0.1083 acc=0.845 f1=0.710 | valid_loss=0.0993 acc=0.866 f1=0.721 | lr=2.00e-03\n",
            "Epoch 010 | train_loss=0.1000 acc=0.852 f1=0.741 | valid_loss=0.1108 acc=0.840 f1=0.699 | lr=2.00e-03\n",
            "Epoch 011 | train_loss=0.0864 acc=0.866 f1=0.765 | valid_loss=0.0952 acc=0.849 f1=0.682 | lr=2.00e-03\n",
            "Epoch 012 | train_loss=0.1404 acc=0.798 f1=0.628 | valid_loss=0.0906 acc=0.899 f1=0.769 | lr=2.00e-03\n",
            "Epoch 013 | train_loss=0.1033 acc=0.825 f1=0.654 | valid_loss=0.0923 acc=0.866 f1=0.746 | lr=2.00e-03\n",
            "Epoch 014 | train_loss=0.1045 acc=0.845 f1=0.730 | valid_loss=0.0969 acc=0.882 f1=0.750 | lr=2.00e-03\n",
            "Epoch 015 | train_loss=0.0791 acc=0.891 f1=0.802 | valid_loss=0.0704 acc=0.882 f1=0.757 | lr=2.00e-03\n",
            "Epoch 016 | train_loss=0.0832 acc=0.893 f1=0.819 | valid_loss=0.0849 acc=0.874 f1=0.739 | lr=2.00e-03\n",
            "Epoch 017 | train_loss=0.0871 acc=0.889 f1=0.816 | valid_loss=0.0886 acc=0.874 f1=0.690 | lr=2.00e-03\n",
            "Epoch 018 | train_loss=0.0737 acc=0.887 f1=0.808 | valid_loss=0.0879 acc=0.857 f1=0.715 | lr=1.00e-03\n",
            "Epoch 019 | train_loss=0.0893 acc=0.917 f1=0.860 | valid_loss=0.0617 acc=0.899 f1=0.788 | lr=1.00e-03\n",
            "Epoch 020 | train_loss=0.0716 acc=0.876 f1=0.782 | valid_loss=0.0622 acc=0.908 f1=0.790 | lr=1.00e-03\n",
            "Epoch 021 | train_loss=0.0645 acc=0.917 f1=0.862 | valid_loss=0.0569 acc=0.916 f1=0.806 | lr=1.00e-03\n",
            "Epoch 022 | train_loss=0.0698 acc=0.909 f1=0.860 | valid_loss=0.0646 acc=0.924 f1=0.812 | lr=1.00e-03\n",
            "Epoch 023 | train_loss=0.0679 acc=0.907 f1=0.849 | valid_loss=0.0534 acc=0.908 f1=0.806 | lr=1.00e-03\n",
            "Epoch 024 | train_loss=0.0571 acc=0.918 f1=0.874 | valid_loss=0.0557 acc=0.891 f1=0.783 | lr=1.00e-03\n",
            "Epoch 025 | train_loss=0.0649 acc=0.907 f1=0.850 | valid_loss=0.0525 acc=0.924 f1=0.818 | lr=1.00e-03\n",
            "Epoch 026 | train_loss=0.0588 acc=0.887 f1=0.795 | valid_loss=0.0587 acc=0.924 f1=0.834 | lr=1.00e-03\n",
            "Epoch 027 | train_loss=0.0661 acc=0.905 f1=0.843 | valid_loss=0.0561 acc=0.916 f1=0.834 | lr=1.00e-03\n",
            "Epoch 028 | train_loss=0.0554 acc=0.907 f1=0.852 | valid_loss=0.0612 acc=0.916 f1=0.834 | lr=1.00e-03\n",
            "Epoch 029 | train_loss=0.0544 acc=0.917 f1=0.872 | valid_loss=0.0490 acc=0.933 f1=0.860 | lr=1.00e-03\n",
            "Epoch 030 | train_loss=0.0507 acc=0.915 f1=0.866 | valid_loss=0.0516 acc=0.933 f1=0.828 | lr=1.00e-03\n",
            "Epoch 031 | train_loss=0.0632 acc=0.901 f1=0.839 | valid_loss=0.0594 acc=0.916 f1=0.819 | lr=1.00e-03\n",
            "Epoch 032 | train_loss=0.0633 acc=0.889 f1=0.793 | valid_loss=0.0567 acc=0.933 f1=0.853 | lr=1.00e-03\n",
            "Epoch 033 | train_loss=0.0759 acc=0.874 f1=0.781 | valid_loss=0.0796 acc=0.882 f1=0.751 | lr=1.00e-03\n",
            "Epoch 034 | train_loss=0.0566 acc=0.924 f1=0.871 | valid_loss=0.0812 acc=0.882 f1=0.763 | lr=1.00e-03\n",
            "Epoch 035 | train_loss=0.0676 acc=0.895 f1=0.820 | valid_loss=0.0706 acc=0.916 f1=0.819 | lr=5.00e-04\n",
            "Epoch 036 | train_loss=0.0518 acc=0.932 f1=0.886 | valid_loss=0.0699 acc=0.899 f1=0.775 | lr=5.00e-04\n",
            "Epoch 037 | train_loss=0.0559 acc=0.936 f1=0.904 | valid_loss=0.0746 acc=0.874 f1=0.757 | lr=5.00e-04\n",
            "Epoch 038 | train_loss=0.0600 acc=0.932 f1=0.896 | valid_loss=0.0625 acc=0.899 f1=0.784 | lr=5.00e-04\n",
            "Epoch 039 | train_loss=0.0545 acc=0.932 f1=0.890 | valid_loss=0.0584 acc=0.899 f1=0.798 | lr=5.00e-04\n",
            "Epoch 040 | train_loss=0.0509 acc=0.940 f1=0.906 | valid_loss=0.0620 acc=0.924 f1=0.834 | lr=5.00e-04\n",
            "Epoch 041 | train_loss=0.0542 acc=0.928 f1=0.883 | valid_loss=0.0644 acc=0.908 f1=0.819 | lr=2.50e-04\n",
            "Epoch 042 | train_loss=0.0554 acc=0.928 f1=0.889 | valid_loss=0.0610 acc=0.891 f1=0.792 | lr=2.50e-04\n",
            "Epoch 043 | train_loss=0.0439 acc=0.936 f1=0.897 | valid_loss=0.0658 acc=0.933 f1=0.862 | lr=2.50e-04\n",
            "Epoch 044 | train_loss=0.0413 acc=0.953 f1=0.928 | valid_loss=0.0644 acc=0.891 f1=0.768 | lr=2.50e-04\n",
            "Epoch 045 | train_loss=0.0512 acc=0.944 f1=0.912 | valid_loss=0.0665 acc=0.916 f1=0.819 | lr=2.50e-04\n",
            "Epoch 046 | train_loss=0.0423 acc=0.948 f1=0.912 | valid_loss=0.0595 acc=0.933 f1=0.866 | lr=2.50e-04\n",
            "Epoch 047 | train_loss=0.0438 acc=0.946 f1=0.920 | valid_loss=0.0586 acc=0.924 f1=0.852 | lr=2.50e-04\n",
            "Epoch 048 | train_loss=0.0420 acc=0.948 f1=0.926 | valid_loss=0.0606 acc=0.924 f1=0.852 | lr=2.50e-04\n",
            "Epoch 049 | train_loss=0.0425 acc=0.946 f1=0.917 | valid_loss=0.0593 acc=0.916 f1=0.835 | lr=2.50e-04\n",
            "Epoch 050 | train_loss=0.0383 acc=0.942 f1=0.908 | valid_loss=0.0598 acc=0.933 f1=0.866 | lr=2.50e-04\n",
            "Epoch 051 | train_loss=0.0386 acc=0.942 f1=0.910 | valid_loss=0.0599 acc=0.924 f1=0.840 | lr=2.50e-04\n",
            "Epoch 052 | train_loss=0.0477 acc=0.944 f1=0.911 | valid_loss=0.0528 acc=0.924 f1=0.856 | lr=1.25e-04\n",
            "Epoch 053 | train_loss=0.0389 acc=0.944 f1=0.910 | valid_loss=0.0535 acc=0.916 f1=0.839 | lr=1.25e-04\n",
            "Epoch 054 | train_loss=0.0402 acc=0.959 f1=0.941 | valid_loss=0.0528 acc=0.916 f1=0.813 | lr=1.25e-04\n",
            "Epoch 055 | train_loss=0.0353 acc=0.961 f1=0.943 | valid_loss=0.0579 acc=0.924 f1=0.849 | lr=1.25e-04\n",
            "Epoch 056 | train_loss=0.0416 acc=0.948 f1=0.924 | valid_loss=0.0655 acc=0.924 f1=0.838 | lr=1.25e-04\n",
            "Epoch 057 | train_loss=0.0395 acc=0.942 f1=0.913 | valid_loss=0.0608 acc=0.924 f1=0.849 | lr=1.25e-04\n",
            "Epoch 058 | train_loss=0.0436 acc=0.948 f1=0.922 | valid_loss=0.0574 acc=0.908 f1=0.807 | lr=6.25e-05\n",
            "Epoch 059 | train_loss=0.0373 acc=0.940 f1=0.912 | valid_loss=0.0604 acc=0.916 f1=0.821 | lr=6.25e-05\n",
            "Epoch 060 | train_loss=0.0305 acc=0.961 f1=0.944 | valid_loss=0.0567 acc=0.924 f1=0.856 | lr=6.25e-05\n",
            "Epoch 061 | train_loss=0.0373 acc=0.951 f1=0.931 | valid_loss=0.0619 acc=0.924 f1=0.849 | lr=6.25e-05\n",
            "Epoch 062 | train_loss=0.0374 acc=0.951 f1=0.922 | valid_loss=0.0585 acc=0.916 f1=0.823 | lr=6.25e-05\n",
            "Epoch 063 | train_loss=0.0439 acc=0.942 f1=0.913 | valid_loss=0.0544 acc=0.908 f1=0.802 | lr=6.25e-05\n",
            "Epoch 064 | train_loss=0.0449 acc=0.946 f1=0.918 | valid_loss=0.0503 acc=0.916 f1=0.817 | lr=3.13e-05\n",
            "Epoch 065 | train_loss=0.0378 acc=0.957 f1=0.938 | valid_loss=0.0617 acc=0.924 f1=0.827 | lr=3.13e-05\n",
            "Epoch 066 | train_loss=0.0507 acc=0.950 f1=0.924 | valid_loss=0.0550 acc=0.941 f1=0.880 | lr=3.13e-05\n",
            "Epoch 067 | train_loss=0.0338 acc=0.951 f1=0.925 | valid_loss=0.0673 acc=0.933 f1=0.853 | lr=3.13e-05\n",
            "Epoch 068 | train_loss=0.0379 acc=0.951 f1=0.926 | valid_loss=0.0605 acc=0.924 f1=0.838 | lr=3.13e-05\n",
            "Epoch 069 | train_loss=0.0327 acc=0.963 f1=0.947 | valid_loss=0.0580 acc=0.916 f1=0.813 | lr=3.13e-05\n",
            "Epoch 070 | train_loss=0.0356 acc=0.961 f1=0.938 | valid_loss=0.0645 acc=0.933 f1=0.853 | lr=3.13e-05\n",
            "Epoch 071 | train_loss=0.0371 acc=0.953 f1=0.930 | valid_loss=0.0626 acc=0.933 f1=0.853 | lr=3.13e-05\n",
            "Epoch 072 | train_loss=0.0388 acc=0.953 f1=0.932 | valid_loss=0.0630 acc=0.924 f1=0.837 | lr=1.56e-05\n",
            "Epoch 073 | train_loss=0.0364 acc=0.951 f1=0.929 | valid_loss=0.0581 acc=0.916 f1=0.827 | lr=1.56e-05\n",
            "Epoch 074 | train_loss=0.0371 acc=0.948 f1=0.917 | valid_loss=0.0576 acc=0.933 f1=0.866 | lr=1.56e-05\n",
            "Epoch 075 | train_loss=0.0309 acc=0.957 f1=0.939 | valid_loss=0.0675 acc=0.941 f1=0.880 | lr=1.56e-05\n",
            "Epoch 076 | train_loss=0.0361 acc=0.957 f1=0.934 | valid_loss=0.0619 acc=0.933 f1=0.853 | lr=1.56e-05\n",
            "Epoch 077 | train_loss=0.0353 acc=0.948 f1=0.923 | valid_loss=0.0585 acc=0.924 f1=0.838 | lr=1.56e-05\n",
            "Epoch 078 | train_loss=0.0357 acc=0.950 f1=0.923 | valid_loss=0.0631 acc=0.924 f1=0.827 | lr=7.81e-06\n",
            "Epoch 079 | train_loss=0.0366 acc=0.951 f1=0.928 | valid_loss=0.0610 acc=0.924 f1=0.827 | lr=7.81e-06\n",
            "Epoch 080 | train_loss=0.0358 acc=0.942 f1=0.915 | valid_loss=0.0570 acc=0.924 f1=0.837 | lr=7.81e-06\n",
            "Epoch 081 | train_loss=0.0292 acc=0.961 f1=0.944 | valid_loss=0.0569 acc=0.933 f1=0.853 | lr=7.81e-06\n",
            "Epoch 082 | train_loss=0.0339 acc=0.957 f1=0.935 | valid_loss=0.0570 acc=0.933 f1=0.847 | lr=7.81e-06\n",
            "Epoch 083 | train_loss=0.0374 acc=0.953 f1=0.920 | valid_loss=0.0598 acc=0.933 f1=0.847 | lr=7.81e-06\n",
            "Epoch 084 | train_loss=0.0411 acc=0.951 f1=0.928 | valid_loss=0.0604 acc=0.941 f1=0.880 | lr=3.91e-06\n",
            "Epoch 085 | train_loss=0.0263 acc=0.963 f1=0.941 | valid_loss=0.0644 acc=0.941 f1=0.880 | lr=3.91e-06\n",
            "Epoch 086 | train_loss=0.0376 acc=0.946 f1=0.922 | valid_loss=0.0600 acc=0.924 f1=0.838 | lr=3.91e-06\n",
            "Early stopping triggered at epoch 86. Best f1=0.8800.\n",
            "Fold 2/5 | best F1(valid)=0.8800 | Accuracy(valid)=0.9412 | Test F1=0.7936\n",
            "Epoch 001 | train_loss=0.2354 acc=0.697 f1=0.299 | valid_loss=0.1600 acc=0.773 f1=0.291 | lr=2.00e-03\n",
            "Epoch 002 | train_loss=0.1775 acc=0.738 f1=0.386 | valid_loss=0.1380 acc=0.807 f1=0.412 | lr=2.00e-03\n",
            "Epoch 003 | train_loss=0.1546 acc=0.759 f1=0.471 | valid_loss=0.1332 acc=0.824 f1=0.457 | lr=2.00e-03\n",
            "Epoch 004 | train_loss=0.1175 acc=0.825 f1=0.624 | valid_loss=0.1350 acc=0.849 f1=0.626 | lr=2.00e-03\n",
            "Epoch 005 | train_loss=0.1061 acc=0.843 f1=0.711 | valid_loss=0.1292 acc=0.857 f1=0.621 | lr=2.00e-03\n",
            "Epoch 006 | train_loss=0.1143 acc=0.827 f1=0.695 | valid_loss=0.1301 acc=0.866 f1=0.664 | lr=2.00e-03\n",
            "Epoch 007 | train_loss=0.1078 acc=0.847 f1=0.751 | valid_loss=0.1190 acc=0.840 f1=0.564 | lr=2.00e-03\n",
            "Epoch 008 | train_loss=0.1290 acc=0.821 f1=0.665 | valid_loss=0.1343 acc=0.849 f1=0.691 | lr=2.00e-03\n",
            "Epoch 009 | train_loss=0.0975 acc=0.872 f1=0.793 | valid_loss=0.1114 acc=0.849 f1=0.608 | lr=2.00e-03\n",
            "Epoch 010 | train_loss=0.0945 acc=0.866 f1=0.756 | valid_loss=0.1014 acc=0.908 f1=0.770 | lr=2.00e-03\n",
            "Epoch 011 | train_loss=0.1134 acc=0.854 f1=0.781 | valid_loss=0.1041 acc=0.882 f1=0.732 | lr=2.00e-03\n",
            "Epoch 012 | train_loss=0.1010 acc=0.858 f1=0.749 | valid_loss=0.1243 acc=0.882 f1=0.731 | lr=2.00e-03\n",
            "Epoch 013 | train_loss=0.0836 acc=0.883 f1=0.811 | valid_loss=0.1262 acc=0.849 f1=0.657 | lr=2.00e-03\n",
            "Epoch 014 | train_loss=0.0799 acc=0.893 f1=0.826 | valid_loss=0.1129 acc=0.866 f1=0.674 | lr=2.00e-03\n",
            "Epoch 015 | train_loss=0.0862 acc=0.883 f1=0.803 | valid_loss=0.1187 acc=0.866 f1=0.706 | lr=2.00e-03\n",
            "Epoch 016 | train_loss=0.1142 acc=0.860 f1=0.764 | valid_loss=0.1143 acc=0.891 f1=0.757 | lr=1.00e-03\n",
            "Epoch 017 | train_loss=0.0793 acc=0.878 f1=0.784 | valid_loss=0.1135 acc=0.882 f1=0.718 | lr=1.00e-03\n",
            "Epoch 018 | train_loss=0.0688 acc=0.909 f1=0.849 | valid_loss=0.1150 acc=0.866 f1=0.751 | lr=1.00e-03\n",
            "Epoch 019 | train_loss=0.0767 acc=0.899 f1=0.835 | valid_loss=0.1130 acc=0.891 f1=0.757 | lr=1.00e-03\n",
            "Epoch 020 | train_loss=0.0543 acc=0.917 f1=0.857 | valid_loss=0.1181 acc=0.891 f1=0.757 | lr=1.00e-03\n",
            "Epoch 021 | train_loss=0.0699 acc=0.895 f1=0.833 | valid_loss=0.1089 acc=0.891 f1=0.757 | lr=1.00e-03\n",
            "Epoch 022 | train_loss=0.0568 acc=0.913 f1=0.860 | valid_loss=0.1175 acc=0.874 f1=0.705 | lr=5.00e-04\n",
            "Epoch 023 | train_loss=0.0524 acc=0.924 f1=0.883 | valid_loss=0.0954 acc=0.882 f1=0.724 | lr=5.00e-04\n",
            "Epoch 024 | train_loss=0.0513 acc=0.928 f1=0.888 | valid_loss=0.0936 acc=0.866 f1=0.705 | lr=5.00e-04\n",
            "Epoch 025 | train_loss=0.0418 acc=0.918 f1=0.875 | valid_loss=0.0972 acc=0.882 f1=0.724 | lr=5.00e-04\n",
            "Epoch 026 | train_loss=0.0564 acc=0.918 f1=0.871 | valid_loss=0.0939 acc=0.882 f1=0.724 | lr=5.00e-04\n",
            "Epoch 027 | train_loss=0.0592 acc=0.917 f1=0.874 | valid_loss=0.0855 acc=0.857 f1=0.700 | lr=5.00e-04\n",
            "Epoch 028 | train_loss=0.0713 acc=0.911 f1=0.865 | valid_loss=0.0843 acc=0.899 f1=0.787 | lr=5.00e-04\n",
            "Epoch 029 | train_loss=0.0543 acc=0.932 f1=0.903 | valid_loss=0.0834 acc=0.899 f1=0.787 | lr=5.00e-04\n",
            "Epoch 030 | train_loss=0.0649 acc=0.909 f1=0.865 | valid_loss=0.0948 acc=0.899 f1=0.787 | lr=5.00e-04\n",
            "Epoch 031 | train_loss=0.0516 acc=0.922 f1=0.881 | valid_loss=0.1028 acc=0.891 f1=0.757 | lr=5.00e-04\n",
            "Epoch 032 | train_loss=0.0561 acc=0.913 f1=0.866 | valid_loss=0.0966 acc=0.891 f1=0.757 | lr=5.00e-04\n",
            "Epoch 033 | train_loss=0.0434 acc=0.936 f1=0.904 | valid_loss=0.0941 acc=0.891 f1=0.757 | lr=5.00e-04\n",
            "Epoch 034 | train_loss=0.0444 acc=0.936 f1=0.901 | valid_loss=0.1004 acc=0.891 f1=0.757 | lr=2.50e-04\n",
            "Epoch 035 | train_loss=0.0486 acc=0.924 f1=0.889 | valid_loss=0.1047 acc=0.891 f1=0.757 | lr=2.50e-04\n",
            "Epoch 036 | train_loss=0.0458 acc=0.930 f1=0.894 | valid_loss=0.0997 acc=0.891 f1=0.757 | lr=2.50e-04\n",
            "Epoch 037 | train_loss=0.0521 acc=0.920 f1=0.881 | valid_loss=0.0915 acc=0.891 f1=0.757 | lr=2.50e-04\n",
            "Epoch 038 | train_loss=0.0444 acc=0.932 f1=0.896 | valid_loss=0.0939 acc=0.891 f1=0.757 | lr=2.50e-04\n",
            "Epoch 039 | train_loss=0.0400 acc=0.946 f1=0.918 | valid_loss=0.0827 acc=0.891 f1=0.757 | lr=2.50e-04\n",
            "Epoch 040 | train_loss=0.0393 acc=0.936 f1=0.908 | valid_loss=0.0832 acc=0.891 f1=0.757 | lr=1.25e-04\n",
            "Epoch 041 | train_loss=0.0471 acc=0.922 f1=0.881 | valid_loss=0.0823 acc=0.891 f1=0.757 | lr=1.25e-04\n",
            "Epoch 042 | train_loss=0.0477 acc=0.932 f1=0.898 | valid_loss=0.0866 acc=0.891 f1=0.757 | lr=1.25e-04\n",
            "Epoch 043 | train_loss=0.0466 acc=0.934 f1=0.898 | valid_loss=0.0779 acc=0.908 f1=0.791 | lr=1.25e-04\n",
            "Epoch 044 | train_loss=0.0406 acc=0.938 f1=0.910 | valid_loss=0.0820 acc=0.899 f1=0.775 | lr=1.25e-04\n",
            "Epoch 045 | train_loss=0.0442 acc=0.938 f1=0.905 | valid_loss=0.0842 acc=0.891 f1=0.757 | lr=1.25e-04\n",
            "Epoch 046 | train_loss=0.0377 acc=0.944 f1=0.918 | valid_loss=0.0841 acc=0.891 f1=0.757 | lr=1.25e-04\n",
            "Epoch 047 | train_loss=0.0515 acc=0.930 f1=0.890 | valid_loss=0.0879 acc=0.891 f1=0.757 | lr=1.25e-04\n",
            "Epoch 048 | train_loss=0.0436 acc=0.934 f1=0.900 | valid_loss=0.0828 acc=0.899 f1=0.775 | lr=1.25e-04\n",
            "Epoch 049 | train_loss=0.0446 acc=0.942 f1=0.912 | valid_loss=0.0915 acc=0.882 f1=0.718 | lr=6.25e-05\n",
            "Epoch 050 | train_loss=0.0476 acc=0.926 f1=0.887 | valid_loss=0.0905 acc=0.891 f1=0.757 | lr=6.25e-05\n",
            "Epoch 051 | train_loss=0.0401 acc=0.936 f1=0.907 | valid_loss=0.0850 acc=0.891 f1=0.757 | lr=6.25e-05\n",
            "Epoch 052 | train_loss=0.0358 acc=0.957 f1=0.937 | valid_loss=0.0856 acc=0.891 f1=0.757 | lr=6.25e-05\n",
            "Epoch 053 | train_loss=0.0503 acc=0.928 f1=0.896 | valid_loss=0.0841 acc=0.891 f1=0.757 | lr=6.25e-05\n",
            "Epoch 054 | train_loss=0.0432 acc=0.950 f1=0.923 | valid_loss=0.0801 acc=0.899 f1=0.787 | lr=6.25e-05\n",
            "Epoch 055 | train_loss=0.0442 acc=0.932 f1=0.901 | valid_loss=0.0817 acc=0.891 f1=0.757 | lr=3.13e-05\n",
            "Epoch 056 | train_loss=0.0505 acc=0.938 f1=0.911 | valid_loss=0.0813 acc=0.899 f1=0.787 | lr=3.13e-05\n",
            "Epoch 057 | train_loss=0.0416 acc=0.936 f1=0.908 | valid_loss=0.0819 acc=0.899 f1=0.787 | lr=3.13e-05\n",
            "Epoch 058 | train_loss=0.0470 acc=0.942 f1=0.915 | valid_loss=0.0845 acc=0.899 f1=0.787 | lr=3.13e-05\n",
            "Epoch 059 | train_loss=0.0414 acc=0.940 f1=0.908 | valid_loss=0.0829 acc=0.899 f1=0.787 | lr=3.13e-05\n",
            "Epoch 060 | train_loss=0.0351 acc=0.951 f1=0.931 | valid_loss=0.0833 acc=0.899 f1=0.787 | lr=3.13e-05\n",
            "Epoch 061 | train_loss=0.0401 acc=0.938 f1=0.905 | valid_loss=0.0854 acc=0.899 f1=0.787 | lr=1.56e-05\n",
            "Epoch 062 | train_loss=0.0403 acc=0.950 f1=0.929 | valid_loss=0.0842 acc=0.899 f1=0.787 | lr=1.56e-05\n",
            "Epoch 063 | train_loss=0.0357 acc=0.948 f1=0.925 | valid_loss=0.0857 acc=0.899 f1=0.787 | lr=1.56e-05\n",
            "Early stopping triggered at epoch 63. Best f1=0.7912.\n",
            "Fold 3/5 | best F1(valid)=0.7912 | Accuracy(valid)=0.9076 | Test F1=0.7800\n",
            "Epoch 001 | train_loss=0.2359 acc=0.703 f1=0.316 | valid_loss=0.1773 acc=0.824 f1=0.467 | lr=2.00e-03\n",
            "Epoch 002 | train_loss=0.1661 acc=0.740 f1=0.426 | valid_loss=0.1112 acc=0.840 f1=0.577 | lr=2.00e-03\n",
            "Epoch 003 | train_loss=0.1660 acc=0.784 f1=0.560 | valid_loss=0.1175 acc=0.840 f1=0.551 | lr=2.00e-03\n",
            "Epoch 004 | train_loss=0.1464 acc=0.761 f1=0.495 | valid_loss=0.0993 acc=0.832 f1=0.590 | lr=2.00e-03\n",
            "Epoch 005 | train_loss=0.1262 acc=0.802 f1=0.662 | valid_loss=0.0680 acc=0.899 f1=0.717 | lr=2.00e-03\n",
            "Epoch 006 | train_loss=0.1212 acc=0.821 f1=0.661 | valid_loss=0.0636 acc=0.916 f1=0.804 | lr=2.00e-03\n",
            "Epoch 007 | train_loss=0.1101 acc=0.845 f1=0.709 | valid_loss=0.0743 acc=0.882 f1=0.709 | lr=2.00e-03\n",
            "Epoch 008 | train_loss=0.1157 acc=0.810 f1=0.685 | valid_loss=0.0871 acc=0.882 f1=0.742 | lr=2.00e-03\n",
            "Epoch 009 | train_loss=0.1010 acc=0.862 f1=0.753 | valid_loss=0.0620 acc=0.899 f1=0.719 | lr=2.00e-03\n",
            "Epoch 010 | train_loss=0.0907 acc=0.862 f1=0.751 | valid_loss=0.0557 acc=0.908 f1=0.740 | lr=2.00e-03\n",
            "Epoch 011 | train_loss=0.1020 acc=0.864 f1=0.751 | valid_loss=0.0575 acc=0.916 f1=0.796 | lr=2.00e-03\n",
            "Epoch 012 | train_loss=0.0860 acc=0.872 f1=0.780 | valid_loss=0.0606 acc=0.916 f1=0.798 | lr=1.00e-03\n",
            "Epoch 013 | train_loss=0.1113 acc=0.849 f1=0.766 | valid_loss=0.0654 acc=0.916 f1=0.781 | lr=1.00e-03\n",
            "Epoch 014 | train_loss=0.0853 acc=0.883 f1=0.803 | valid_loss=0.0472 acc=0.941 f1=0.869 | lr=1.00e-03\n",
            "Epoch 015 | train_loss=0.0848 acc=0.885 f1=0.809 | valid_loss=0.0506 acc=0.950 f1=0.881 | lr=1.00e-03\n",
            "Epoch 016 | train_loss=0.0916 acc=0.862 f1=0.778 | valid_loss=0.0572 acc=0.924 f1=0.820 | lr=1.00e-03\n",
            "Epoch 017 | train_loss=0.0823 acc=0.874 f1=0.782 | valid_loss=0.0600 acc=0.908 f1=0.771 | lr=1.00e-03\n",
            "Epoch 018 | train_loss=0.0902 acc=0.876 f1=0.787 | valid_loss=0.0689 acc=0.908 f1=0.790 | lr=1.00e-03\n",
            "Epoch 019 | train_loss=0.0727 acc=0.889 f1=0.801 | valid_loss=0.0441 acc=0.933 f1=0.854 | lr=1.00e-03\n",
            "Epoch 020 | train_loss=0.0653 acc=0.901 f1=0.834 | valid_loss=0.0511 acc=0.924 f1=0.831 | lr=1.00e-03\n",
            "Epoch 021 | train_loss=0.0611 acc=0.911 f1=0.856 | valid_loss=0.0511 acc=0.924 f1=0.841 | lr=5.00e-04\n",
            "Epoch 022 | train_loss=0.0714 acc=0.905 f1=0.838 | valid_loss=0.0415 acc=0.933 f1=0.846 | lr=5.00e-04\n",
            "Epoch 023 | train_loss=0.0538 acc=0.922 f1=0.875 | valid_loss=0.0562 acc=0.916 f1=0.830 | lr=5.00e-04\n",
            "Epoch 024 | train_loss=0.0583 acc=0.926 f1=0.891 | valid_loss=0.0515 acc=0.933 f1=0.866 | lr=5.00e-04\n",
            "Epoch 025 | train_loss=0.0562 acc=0.918 f1=0.868 | valid_loss=0.0439 acc=0.924 f1=0.856 | lr=5.00e-04\n",
            "Epoch 026 | train_loss=0.0562 acc=0.920 f1=0.874 | valid_loss=0.0484 acc=0.924 f1=0.856 | lr=5.00e-04\n",
            "Epoch 027 | train_loss=0.0601 acc=0.920 f1=0.880 | valid_loss=0.0514 acc=0.924 f1=0.853 | lr=2.50e-04\n",
            "Epoch 028 | train_loss=0.0640 acc=0.901 f1=0.860 | valid_loss=0.0563 acc=0.924 f1=0.853 | lr=2.50e-04\n",
            "Epoch 029 | train_loss=0.0598 acc=0.909 f1=0.860 | valid_loss=0.0452 acc=0.941 f1=0.883 | lr=2.50e-04\n",
            "Epoch 030 | train_loss=0.0579 acc=0.915 f1=0.858 | valid_loss=0.0440 acc=0.941 f1=0.872 | lr=2.50e-04\n",
            "Epoch 031 | train_loss=0.0498 acc=0.934 f1=0.896 | valid_loss=0.0426 acc=0.933 f1=0.856 | lr=2.50e-04\n",
            "Epoch 032 | train_loss=0.0547 acc=0.928 f1=0.891 | valid_loss=0.0499 acc=0.950 f1=0.898 | lr=2.50e-04\n",
            "Epoch 033 | train_loss=0.0589 acc=0.934 f1=0.895 | valid_loss=0.0427 acc=0.950 f1=0.894 | lr=2.50e-04\n",
            "Epoch 034 | train_loss=0.0523 acc=0.924 f1=0.877 | valid_loss=0.0351 acc=0.941 f1=0.870 | lr=2.50e-04\n",
            "Epoch 035 | train_loss=0.0531 acc=0.932 f1=0.887 | valid_loss=0.0363 acc=0.941 f1=0.870 | lr=2.50e-04\n",
            "Epoch 036 | train_loss=0.0566 acc=0.928 f1=0.890 | valid_loss=0.0392 acc=0.933 f1=0.868 | lr=2.50e-04\n",
            "Epoch 037 | train_loss=0.0556 acc=0.926 f1=0.886 | valid_loss=0.0498 acc=0.933 f1=0.881 | lr=2.50e-04\n",
            "Epoch 038 | train_loss=0.0536 acc=0.913 f1=0.864 | valid_loss=0.0486 acc=0.941 f1=0.881 | lr=1.25e-04\n",
            "Epoch 039 | train_loss=0.0526 acc=0.924 f1=0.883 | valid_loss=0.0436 acc=0.933 f1=0.866 | lr=1.25e-04\n",
            "Epoch 040 | train_loss=0.0483 acc=0.932 f1=0.892 | valid_loss=0.0407 acc=0.924 f1=0.819 | lr=1.25e-04\n",
            "Epoch 041 | train_loss=0.0502 acc=0.930 f1=0.889 | valid_loss=0.0444 acc=0.941 f1=0.881 | lr=1.25e-04\n",
            "Epoch 042 | train_loss=0.0488 acc=0.926 f1=0.885 | valid_loss=0.0420 acc=0.941 f1=0.879 | lr=1.25e-04\n",
            "Epoch 043 | train_loss=0.0501 acc=0.915 f1=0.854 | valid_loss=0.0382 acc=0.933 f1=0.866 | lr=1.25e-04\n",
            "Epoch 044 | train_loss=0.0434 acc=0.936 f1=0.897 | valid_loss=0.0377 acc=0.941 f1=0.890 | lr=6.25e-05\n",
            "Epoch 045 | train_loss=0.0545 acc=0.934 f1=0.897 | valid_loss=0.0414 acc=0.933 f1=0.876 | lr=6.25e-05\n",
            "Epoch 046 | train_loss=0.0360 acc=0.965 f1=0.951 | valid_loss=0.0381 acc=0.933 f1=0.868 | lr=6.25e-05\n",
            "Epoch 047 | train_loss=0.0433 acc=0.946 f1=0.918 | valid_loss=0.0398 acc=0.933 f1=0.868 | lr=6.25e-05\n",
            "Epoch 048 | train_loss=0.0461 acc=0.936 f1=0.901 | valid_loss=0.0382 acc=0.933 f1=0.868 | lr=6.25e-05\n",
            "Epoch 049 | train_loss=0.0470 acc=0.936 f1=0.901 | valid_loss=0.0365 acc=0.933 f1=0.868 | lr=6.25e-05\n",
            "Epoch 050 | train_loss=0.0474 acc=0.940 f1=0.912 | valid_loss=0.0392 acc=0.933 f1=0.868 | lr=3.13e-05\n",
            "Epoch 051 | train_loss=0.0426 acc=0.940 f1=0.905 | valid_loss=0.0422 acc=0.941 f1=0.887 | lr=3.13e-05\n",
            "Epoch 052 | train_loss=0.0477 acc=0.932 f1=0.899 | valid_loss=0.0412 acc=0.941 f1=0.887 | lr=3.13e-05\n",
            "Early stopping triggered at epoch 52. Best f1=0.8981.\n",
            "Fold 4/5 | best F1(valid)=0.8981 | Accuracy(valid)=0.9496 | Test F1=0.8377\n",
            "Epoch 001 | train_loss=0.2347 acc=0.655 f1=0.345 | valid_loss=0.1896 acc=0.805 f1=0.436 | lr=2.00e-03\n",
            "Epoch 002 | train_loss=0.1819 acc=0.727 f1=0.424 | valid_loss=0.1492 acc=0.780 f1=0.362 | lr=2.00e-03\n",
            "Epoch 003 | train_loss=0.1533 acc=0.775 f1=0.534 | valid_loss=0.1110 acc=0.847 f1=0.528 | lr=2.00e-03\n",
            "Epoch 004 | train_loss=0.1388 acc=0.775 f1=0.538 | valid_loss=0.1261 acc=0.822 f1=0.463 | lr=2.00e-03\n",
            "Epoch 005 | train_loss=0.1346 acc=0.818 f1=0.658 | valid_loss=0.1234 acc=0.814 f1=0.493 | lr=2.00e-03\n",
            "Epoch 006 | train_loss=0.1157 acc=0.820 f1=0.649 | valid_loss=0.1029 acc=0.847 f1=0.606 | lr=2.00e-03\n",
            "Epoch 007 | train_loss=0.1048 acc=0.853 f1=0.752 | valid_loss=0.1289 acc=0.864 f1=0.690 | lr=2.00e-03\n",
            "Epoch 008 | train_loss=0.1332 acc=0.833 f1=0.727 | valid_loss=0.1364 acc=0.797 f1=0.496 | lr=2.00e-03\n",
            "Epoch 009 | train_loss=0.1139 acc=0.828 f1=0.670 | valid_loss=0.0834 acc=0.890 f1=0.599 | lr=2.00e-03\n",
            "Epoch 010 | train_loss=0.0949 acc=0.857 f1=0.719 | valid_loss=0.1064 acc=0.839 f1=0.586 | lr=2.00e-03\n",
            "Epoch 011 | train_loss=0.1114 acc=0.845 f1=0.733 | valid_loss=0.1030 acc=0.890 f1=0.739 | lr=2.00e-03\n",
            "Epoch 012 | train_loss=0.1133 acc=0.835 f1=0.692 | valid_loss=0.1165 acc=0.856 f1=0.634 | lr=2.00e-03\n",
            "Epoch 013 | train_loss=0.1151 acc=0.853 f1=0.744 | valid_loss=0.1209 acc=0.847 f1=0.592 | lr=2.00e-03\n",
            "Epoch 014 | train_loss=0.1023 acc=0.866 f1=0.762 | valid_loss=0.0978 acc=0.873 f1=0.680 | lr=2.00e-03\n",
            "Epoch 015 | train_loss=0.0845 acc=0.891 f1=0.817 | valid_loss=0.1017 acc=0.873 f1=0.699 | lr=2.00e-03\n",
            "Epoch 016 | train_loss=0.0771 acc=0.897 f1=0.827 | valid_loss=0.0952 acc=0.898 f1=0.737 | lr=2.00e-03\n",
            "Epoch 017 | train_loss=0.0922 acc=0.874 f1=0.773 | valid_loss=0.1043 acc=0.856 f1=0.600 | lr=1.00e-03\n",
            "Epoch 018 | train_loss=0.0999 acc=0.859 f1=0.770 | valid_loss=0.0855 acc=0.890 f1=0.736 | lr=1.00e-03\n",
            "Epoch 019 | train_loss=0.0823 acc=0.876 f1=0.797 | valid_loss=0.0904 acc=0.873 f1=0.667 | lr=1.00e-03\n",
            "Epoch 020 | train_loss=0.0663 acc=0.905 f1=0.850 | valid_loss=0.0845 acc=0.881 f1=0.688 | lr=1.00e-03\n",
            "Epoch 021 | train_loss=0.0550 acc=0.917 f1=0.867 | valid_loss=0.0938 acc=0.890 f1=0.711 | lr=1.00e-03\n",
            "Epoch 022 | train_loss=0.0526 acc=0.938 f1=0.900 | valid_loss=0.0913 acc=0.898 f1=0.742 | lr=1.00e-03\n"
          ]
        }
      ],
      "source": [
        "# Fixed external Test split (10%), CV and training on the remaining 90%\n",
        "\n",
        "# 1) Create a single stratified Train/Test split once\n",
        "X_tr_num, X_tr_cat, y_tr, X_te_num, X_te_cat, y_te = stratified_train_test_split_arrays(\n",
        "    X_train_numeric,\n",
        "    X_train_categorical,\n",
        "    y_train_idx,\n",
        "    test_size=0.1,\n",
        "    random_state=SEED,\n",
        ")\n",
        "\n",
        "# 2) Train baseline configs on TRAIN (with internal VAL) and evaluate on TEST\n",
        "EXPERIMENT_CONFIGS = [\n",
        "\n",
        "    prepare_config('GRU_BI', {'rnn_type': 'gru', 'bidirectional': True}),\n",
        "\n",
        "]\n",
        "\n",
        "experiment_results: List[Dict] = []\n",
        "for cfg in EXPERIMENT_CONFIGS:\n",
        "    print(f\"\\n=== Training (fixed TEST) ‚Äî {cfg['run_name']} ({cfg['rnn_type'].upper()} - {'BI' if cfg['bidirectional'] else 'UNI'}) ===\")\n",
        "    result = train_config_on_train_and_eval_test(\n",
        "        cfg,\n",
        "        X_tr_num,\n",
        "        X_tr_cat,\n",
        "        y_tr,\n",
        "        X_te_num,\n",
        "        X_te_cat,\n",
        "        y_te,\n",
        "    )\n",
        "    experiment_results.append(result)\n",
        "    print(\n",
        "        f\"Valid best F1: {result['best_f1']:.4f} at epoch {result['epoch']} | \"\n",
        "        f\"Test F1: {result['test_metrics']['f1']:.4f} | Acc: {result['metrics']['accuracy']:.4f}\"\n",
        "    )\n",
        "\n",
        "# 3) Cross-validate on TRAIN only; each fold model is also evaluated on fixed TEST\n",
        "BEST_FOR_CV = prepare_config('GRU_BI', {'bidirectional': True})\n",
        "cv_results = run_cross_validation_on_arrays(\n",
        "    BEST_FOR_CV,\n",
        "    X_tr_num,\n",
        "    X_tr_cat,\n",
        "    y_tr,\n",
        "    n_splits=5,\n",
        "    X_te_num=X_te_num,\n",
        "    X_te_cat=X_te_cat,\n",
        "    y_te=y_te,\n",
        ")\n",
        "\n",
        "# cv_results now contains, for each fold, both valid metrics and external test_metrics\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aE9xeS7dTBGY"
      },
      "outputs": [],
      "source": [
        "def gather_results(source_names: List[str]) -> List[Dict]:\n",
        "    collected: List[Dict] = []\n",
        "    for name in source_names:\n",
        "        if name in globals():\n",
        "            value = globals()[name]\n",
        "            if isinstance(value, list) and value:\n",
        "                collected.extend(value)\n",
        "    return collected\n",
        "\n",
        "\n",
        "def build_summary_table(result_sources: List[str]) -> Tuple[pd.DataFrame, List[Dict]]:\n",
        "    collected_results = gather_results(result_sources)\n",
        "    if not collected_results:\n",
        "        raise RuntimeError('No experiment results available. Run the training/sweep cells first.')\n",
        "\n",
        "    summary_rows = []\n",
        "    for res in collected_results:\n",
        "        cfg = res['config']\n",
        "        metrics = res['metrics']\n",
        "        summary_rows.append({\n",
        "            'run_name': res['run_name'],\n",
        "            'fold': res.get('fold'),\n",
        "            'rnn_type': cfg['rnn_type'],\n",
        "            'bidirectional': cfg.get('bidirectional', False),\n",
        "            'hidden_size': cfg['hidden_size'],\n",
        "            'num_layers': cfg['num_layers'],\n",
        "            'dropout': cfg['dropout'],\n",
        "            'lr': cfg['lr'],\n",
        "            'weight_decay': cfg.get('weight_decay', 0.0),\n",
        "            'best_epoch': res['epoch'],\n",
        "            'best_f1': res['best_f1'],\n",
        "            'accuracy': metrics['accuracy'],\n",
        "            'precision': metrics['precision'],\n",
        "            'recall': metrics['recall'],\n",
        "            'checkpoint': str(res['checkpoint_path']),\n",
        "            'log_dir': str(LOG_DIR / res['run_name']),\n",
        "            'is_cv': res.get('fold') is not None,\n",
        "        })\n",
        "\n",
        "    summary_df = pd.DataFrame(summary_rows)\n",
        "    summary_df = summary_df.sort_values(by='best_f1', ascending=False).reset_index(drop=True)\n",
        "    return summary_df, collected_results\n",
        "\n",
        "\n",
        "RESULT_SOURCES = ['experiment_results', 'sweep_results', 'cv_results', 'auto_cv_results']\n",
        "summary_table, all_results = build_summary_table(RESULT_SOURCES)\n",
        "summary_table\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yC993yBuTBGY"
      },
      "outputs": [],
      "source": [
        "for _, row in summary_table.iterrows():\n",
        "    print(\n",
        "        f\"Run: {row['run_name']} | Model: {row['rnn_type'].upper()} | \"\n",
        "        f\"Bidirectional: {row['bidirectional']} | F1: {row['best_f1']:.4f}\"\n",
        "    )\n",
        "    print(f\"  Logs: {row['log_dir']}\")\n",
        "\n",
        "# Per TensorBoard combinato (eseguire su Colab / locale):\n",
        "# %tensorboard --logdir \"{LOG_DIR.as_posix()}\"\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pO9BAXqcTBGY"
      },
      "outputs": [],
      "source": [
        "cv_folds_df, cv_summary = summarize_cv_results(cv_results)\n",
        "cv_summary\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ii4ogcA4oTCB"
      },
      "outputs": [],
      "source": [
        "# Robust summary + best_run selection with held-out test\n",
        "# This cell supersedes any previous summary/best_run selection cells\n",
        "\n",
        "# 1) Rebuild summary from experiment_results + cv_results (entrambi ora possono avere test)\n",
        "RESULT_SOURCES = ['experiment_results', 'cv_results']\n",
        "summary_table, all_results = build_summary_table(RESULT_SOURCES)\n",
        "\n",
        "# 2) Select best run ensuring test is present ‚Äì scegliamo in base al F1 sul TEST fisso\n",
        "runs_with_test = [r for r in all_results if 'test_metrics' in r]\n",
        "assert runs_with_test, \"Nessun run con test: riesegui la cella EXPERIMENT_CONFIGS.\"\n",
        "\n",
        "best_run = max(runs_with_test, key=lambda x: x['test_metrics']['f1'])\n",
        "best_model = best_run['model']\n",
        "best_history = best_run['history']\n",
        "\n",
        "print(\n",
        "    f\"Selected best run: {best_run['run_name']} | \"\n",
        "    f\"Valid F1={best_run['best_f1']:.4f} | Valid Acc={best_run['metrics']['accuracy']:.4f} | \"\n",
        "    f\"Test F1={best_run['test_metrics']['f1']:.4f}\"\n",
        ")\n",
        "\n",
        "summary_table.head(10)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OR95BKT0TBGZ"
      },
      "outputs": [],
      "source": [
        "# Safe best_run selection: prefer runs with held-out test\n",
        "if 'best_run' in globals() and isinstance(best_run, dict) and 'test_metrics' in best_run:\n",
        "    print(\n",
        "        f\"Selected best run: {best_run['run_name']} | \"\n",
        "        f\"Valid F1={best_run['best_f1']:.4f} | Valid Acc={best_run['metrics']['accuracy']:.4f} | \"\n",
        "        f\"Test F1={best_run['test_metrics']['f1']:.4f}\"\n",
        "    )\n",
        "else:\n",
        "    # Fallback to experiment_results-only, which are guaranteed to include 80/10/10 test\n",
        "    if 'experiment_results' not in globals() or not experiment_results:\n",
        "        raise RuntimeError('No experiment results with test available. Run the EXPERIMENT_CONFIGS cell first.')\n",
        "    runs_with_test = [r for r in experiment_results if 'test_metrics' in r]\n",
        "    if not runs_with_test:\n",
        "        raise RuntimeError('Nessun run con test: riesegui la cella EXPERIMENT_CONFIGS.')\n",
        "\n",
        "    best_run = max(runs_with_test, key=lambda x: x['best_f1'])\n",
        "    best_model = best_run['model']\n",
        "    best_history = best_run['history']\n",
        "    print(\n",
        "        f\"Selected best run: {best_run['run_name']} | \"\n",
        "        f\"Valid F1={best_run['best_f1']:.4f} | Valid Acc={best_run['metrics']['accuracy']:.4f} | \"\n",
        "        f\"Test F1={best_run['test_metrics']['f1']:.4f}\"\n",
        "    )\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cBAVbNotoTCB"
      },
      "outputs": [],
      "source": [
        "# Test set evaluation report & confusion matrix (if available)\n",
        "if 'test_preds' in best_run and 'test_targets' in best_run:\n",
        "    test_preds = best_run['test_preds']\n",
        "    test_targets = best_run['test_targets']\n",
        "    print(\"Test metrics:\", best_run.get('test_metrics', {}))\n",
        "    print(\n",
        "        classification_report(\n",
        "            test_targets,\n",
        "            test_preds,\n",
        "            target_names=[IDX2LABEL[i] for i in range(NUM_CLASSES)],\n",
        "        )\n",
        "    )\n",
        "\n",
        "    cf_test = confusion_matrix(test_targets, test_preds)\n",
        "    plt.figure(figsize=(6, 5))\n",
        "    sns.heatmap(\n",
        "        cf_test,\n",
        "        annot=True,\n",
        "        fmt='d',\n",
        "        cmap='Greens',\n",
        "        xticklabels=[IDX2LABEL[i] for i in range(NUM_CLASSES)],\n",
        "        yticklabels=[IDX2LABEL[i] for i in range(NUM_CLASSES)],\n",
        "    )\n",
        "    plt.xlabel('Predicted')\n",
        "    plt.ylabel('True')\n",
        "    plt.title(f\"Test Confusion Matrix ‚Äî {best_run['run_name']}\")\n",
        "    plt.show()\n",
        "else:\n",
        "    print(\"No held-out test results available in best_run.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Psxajjq9TBGZ"
      },
      "outputs": [],
      "source": [
        "def plot_history(history: Dict[str, List[float]], title: str = 'Learning Curves'):\n",
        "    epochs = range(1, len(history['train_loss']) + 1)\n",
        "    fig, axs = plt.subplots(1, 3, figsize=(18, 4))\n",
        "\n",
        "    axs[0].plot(epochs, history['train_loss'], label='Train')\n",
        "    axs[0].plot(epochs, history['valid_loss'], label='Valid')\n",
        "    axs[0].set_title('Loss')\n",
        "    axs[0].set_xlabel('Epoch')\n",
        "    axs[0].legend()\n",
        "\n",
        "    axs[1].plot(epochs, history['train_accuracy'], label='Train')\n",
        "    axs[1].plot(epochs, history['valid_accuracy'], label='Valid')\n",
        "    axs[1].set_title('Accuracy')\n",
        "    axs[1].set_xlabel('Epoch')\n",
        "    axs[1].legend()\n",
        "\n",
        "    axs[2].plot(epochs, history['train_f1'], label='Train F1')\n",
        "    axs[2].plot(epochs, history['valid_f1'], label='Valid F1')\n",
        "    axs[2].set_title('Macro F1')\n",
        "    axs[2].set_xlabel('Epoch')\n",
        "    axs[2].legend()\n",
        "\n",
        "    plt.suptitle(title)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "plot_history(best_history, title=f\"Learning Curves ‚Äî {best_run['run_name']}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3jnk70CJTBGZ"
      },
      "outputs": [],
      "source": [
        "best_preds = best_run['preds']\n",
        "best_targets = best_run['targets']\n",
        "print(f\"Best validation macro F1: {best_run['best_f1']:.3f}\")\n",
        "print(\n",
        "    classification_report(\n",
        "        best_targets,\n",
        "        best_preds,\n",
        "        target_names=[IDX2LABEL[i] for i in range(NUM_CLASSES)],\n",
        "    )\n",
        ")\n",
        "\n",
        "cf = confusion_matrix(best_targets, best_preds)\n",
        "plt.figure(figsize=(6, 5))\n",
        "sns.heatmap(\n",
        "    cf,\n",
        "    annot=True,\n",
        "    fmt='d',\n",
        "    cmap='Blues',\n",
        "    xticklabels=[IDX2LABEL[i] for i in range(NUM_CLASSES)],\n",
        "    yticklabels=[IDX2LABEL[i] for i in range(NUM_CLASSES)],\n",
        ")\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('True')\n",
        "plt.title(f\"Validation Confusion Matrix ‚Äî {best_run['run_name']}\")\n",
        "plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ALogtk9WTBGZ"
      },
      "outputs": [],
      "source": [
        "test_dataset = TimeSeriesDataset(\n",
        "    numeric_data=X_test_numeric,\n",
        "    categorical_data=X_test_categorical,\n",
        "    labels=None,\n",
        "    window_size=None,\n",
        "    mode='test',\n",
        "    high_pain_targets=HIGH_PAIN_IDX,\n",
        ")\n",
        "test_loader = DataLoader(test_dataset, batch_size=256, shuffle=False, drop_last=False)\n",
        "\n",
        "best_model.eval()\n",
        "test_preds = []\n",
        "best_config = best_run['config']\n",
        "eval_window_size = best_config.get('eval_window_size', EVAL_WINDOW_SIZE)\n",
        "eval_window_stride = best_config.get('eval_window_stride', EVAL_WINDOW_STRIDE)\n",
        "eval_aggregation = best_config.get('eval_aggregation', EVAL_AGGREGATION)\n",
        "with torch.no_grad():\n",
        "    for inputs_numeric, inputs_categorical in test_loader:\n",
        "        inputs_numeric = inputs_numeric.to(DEVICE)\n",
        "        inputs_categorical = inputs_categorical.to(DEVICE) if inputs_categorical is not None else None\n",
        "        with autocast_context():\n",
        "            logits = forward_with_sliding_windows(\n",
        "                best_model,\n",
        "                inputs_numeric,\n",
        "                inputs_categorical,\n",
        "                eval_window_size,\n",
        "                eval_window_stride,\n",
        "                eval_aggregation,\n",
        "            )\n",
        "        test_preds.append(torch.argmax(logits, dim=1).cpu().numpy())\n",
        "\n",
        "test_preds = np.concatenate(test_preds)\n",
        "test_labels = [IDX2LABEL[idx] for idx in test_preds]\n",
        "\n",
        "submission = pd.DataFrame({\n",
        "    'sample_index': pd.unique(X_test_raw['sample_index']),\n",
        "    'label': test_labels,\n",
        "})\n",
        "submission_filename = OUTPUT_DIR / f\"submission_{best_run['run_name'].lower()}.csv\"\n",
        "submission.to_csv(submission_filename, index=False)\n",
        "print(f\"Saved submission to {submission_filename}\")\n",
        "submission.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_Ip0Gs2ATBGZ"
      },
      "outputs": [],
      "source": [
        "logdir_path = LOG_DIR.as_posix()\n",
        "ipython_getter = globals().get('get_ipython')\n",
        "ipython = ipython_getter() if callable(ipython_getter) else None\n",
        "cli_hint = f\"tensorboard --logdir {logdir_path}\"\n",
        "\n",
        "if ipython is None:\n",
        "    print(\n",
        "        \"TensorBoard non pu√≤ essere avviato automaticamente fuori da IPython. \"\n",
        "        f\"Esegui manualmente: {cli_hint}\"\n",
        "    )\n",
        "else:\n",
        "    try:\n",
        "        ipython.run_line_magic('load_ext', 'tensorboard')\n",
        "    except Exception as exc:  # pragma: no cover\n",
        "        print(f\"Impossibile caricare l'estensione TensorBoard automaticamente: {exc}\")\n",
        "    try:\n",
        "        ipython.run_line_magic('tensorboard', f'--logdir \"{logdir_path}\"')\n",
        "    except Exception as exc:  # pragma: no cover\n",
        "        print(f\"Impossibile avviare TensorBoard: {exc}\")\n",
        "        print(f\"Avvia manualmente con: {cli_hint}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fnmB3tAcoTCH"
      },
      "outputs": [],
      "source": [
        "raise KeyboardInterrupt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "64lgieJwTBGZ"
      },
      "source": [
        "## Next Steps\n",
        "\n",
        "- Espandere `EXPERIMENT_CONFIGS` con ricerche random/grid su hidden size, depth, dropout, learning rate e scheduler per automatizzare l'hyperparameter tuning.\n",
        "- Utilizzare `run_cross_validation` su pi√π configurazioni e confrontare le metriche aggregate in `cv_summary`, esportando i risultati (CSV/LaTeX) per il report finale.\n",
        "- Monitorare tutti i run con `%tensorboard --logdir outputs/logs`, salvando screenshot delle curve principali e confrontando tempi/risorse.\n",
        "- Integrare tecniche di regularizzazione avanzate (label smoothing, mixup temporale, stochastic weight averaging) o layer di attention/pooling.\n",
        "- Costruire ensemble sui checkpoint migliori (media delle probabilit√† o voting) prima della submission Kaggle definitiva.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VDHn21-0oTCI"
      },
      "outputs": [],
      "source": [
        "#Dopo il grafico a colonne\n",
        "\n",
        "# -- Analisi autocorrelazione per suggerire window size ----------------------\n",
        "def compute_autocorr_for_feature(feature: str, max_lag: int = 60) -> Tuple[List[int], List[float]]:\n",
        "    pivot = (\n",
        "        X_train_raw\n",
        "        .pivot(index='sample_index', columns='time', values=feature)\n",
        "        .sort_index(axis=1)\n",
        "    )\n",
        "    lags = list(range(1, max_lag + 1))\n",
        "    autocorr_values: List[float] = []\n",
        "    for lag in lags:\n",
        "        ac_series = pivot.apply(lambda row: row.autocorr(lag=lag), axis=1).dropna()\n",
        "        autocorr_values.append(ac_series.mean() if not ac_series.empty else 0.0)\n",
        "    return lags, autocorr_values\n",
        "\n",
        "\n",
        "def find_significant_autocorr_peaks(\n",
        "    lags: List[int],\n",
        "    autocorr_vals: List[float],\n",
        "    min_autocorr: float = 0.1,\n",
        "    min_lag: int = 5,\n",
        "    max_lag: int = None,\n",
        "    peak_neighborhood: int = 3,\n",
        ") -> List[Tuple[int, float]]:\n",
        "    \"\"\"\n",
        "    Trova i picchi significativi di autocorrelazione.\n",
        "    Restituisce una lista di (lag, autocorr_value) per i picchi trovati.\n",
        "\n",
        "    Args:\n",
        "        min_autocorr: Soglia minima di autocorrelazione (default 0.1, pi√π permissivo)\n",
        "        min_lag: Lag minimo da considerare\n",
        "        max_lag: Lag massimo da considerare\n",
        "        peak_neighborhood: Raggio del vicinato per considerare un picco (default 3)\n",
        "    \"\"\"\n",
        "    if max_lag is None:\n",
        "        max_lag = len(lags)\n",
        "\n",
        "    # Filtra per lag validi e autocorrelazione minima\n",
        "    valid_indices = [\n",
        "        i for i in range(len(lags))\n",
        "        if min_lag <= lags[i] <= max_lag and autocorr_vals[i] >= min_autocorr\n",
        "    ]\n",
        "\n",
        "    if not valid_indices:\n",
        "        return []\n",
        "\n",
        "    # Trova i picchi locali (valori maggiori dei vicini nel neighborhood)\n",
        "    peaks: List[Tuple[int, float]] = []\n",
        "    for i in valid_indices:\n",
        "        is_peak = True\n",
        "        current_val = autocorr_vals[i]\n",
        "\n",
        "        # Controlla i vicini nel neighborhood\n",
        "        for offset in range(-peak_neighborhood, peak_neighborhood + 1):\n",
        "            if offset == 0:\n",
        "                continue\n",
        "            neighbor_idx = i + offset\n",
        "            if 0 <= neighbor_idx < len(autocorr_vals):\n",
        "                if autocorr_vals[neighbor_idx] >= current_val:\n",
        "                    is_peak = False\n",
        "                    break\n",
        "\n",
        "        if is_peak:\n",
        "            peaks.append((lags[i], autocorr_vals[i]))\n",
        "\n",
        "    # Ordina per autocorrelazione decrescente\n",
        "    peaks.sort(key=lambda x: x[1], reverse=True)\n",
        "    return peaks\n",
        "\n",
        "\n",
        "def find_autocorr_decay_point(\n",
        "    lags: List[int],\n",
        "    autocorr_vals: List[float],\n",
        "    threshold: float = 0.2,\n",
        "    min_lag: int = 5,\n",
        "    max_lag: int = None,\n",
        ") -> Optional[Tuple[int, float]]:\n",
        "    \"\"\"\n",
        "    Trova il punto dove l'autocorrelazione scende sotto una soglia.\n",
        "    Utile per determinare la window size quando non ci sono picchi evidenti.\n",
        "    \"\"\"\n",
        "    if max_lag is None:\n",
        "        max_lag = len(lags)\n",
        "\n",
        "    for i, (lag, val) in enumerate(zip(lags, autocorr_vals)):\n",
        "        if min_lag <= lag <= max_lag and val <= threshold:\n",
        "            return (lag, val)\n",
        "\n",
        "    return None\n",
        "\n",
        "\n",
        "AUTO_CORR_FEATURES = FEATURE_COLUMNS[:min(5, len(FEATURE_COLUMNS))]\n",
        "MAX_AUTOCORR_LAG = min(150, FULL_TIME_STEPS - 1)  # Aumentato per catturare picchi secondari\n",
        "MIN_AUTOCORR_THRESHOLD = 0.1  # Ridotto da 0.3 a 0.1 per essere meno restrittivo\n",
        "MIN_WINDOW_LAG = 5\n",
        "MAX_WINDOW_LAG = min(100, FULL_TIME_STEPS // 2)  # Aumentato da 50 a 100\n",
        "DECAY_THRESHOLD = 0.2  # Soglia per trovare il punto di decadimento\n",
        "\n",
        "all_peaks: List[Tuple[int, float]] = []\n",
        "all_decay_points: List[Tuple[int, float]] = []\n",
        "plt.figure(figsize=(14, 7))\n",
        "\n",
        "for feature in AUTO_CORR_FEATURES:\n",
        "    lags, autocorr_vals = compute_autocorr_for_feature(feature, MAX_AUTOCORR_LAG)\n",
        "    plt.plot(lags, autocorr_vals, label=feature, alpha=0.7, linewidth=1.5)\n",
        "\n",
        "    # Cerca picchi\n",
        "    peaks = find_significant_autocorr_peaks(\n",
        "        lags,\n",
        "        autocorr_vals,\n",
        "        min_autocorr=MIN_AUTOCORR_THRESHOLD,\n",
        "        min_lag=MIN_WINDOW_LAG,\n",
        "        max_lag=MAX_WINDOW_LAG,\n",
        "        peak_neighborhood=3,\n",
        "    )\n",
        "\n",
        "    # Cerca anche il punto di decadimento\n",
        "    decay_point = find_autocorr_decay_point(\n",
        "        lags,\n",
        "        autocorr_vals,\n",
        "        threshold=DECAY_THRESHOLD,\n",
        "        min_lag=MIN_WINDOW_LAG,\n",
        "        max_lag=MAX_WINDOW_LAG,\n",
        "    )\n",
        "\n",
        "    if peaks:\n",
        "        top_lag, top_autocorr = peaks[0]\n",
        "        print(f\"Feature {feature}: miglior lag ‚âà {top_lag} con autocorr {top_autocorr:.3f}\")\n",
        "        all_peaks.extend(peaks)\n",
        "        # Evidenzia il picco principale\n",
        "        plt.scatter([top_lag], [top_autocorr], s=100, marker='o', zorder=5,\n",
        "                   label=f'{feature} peak' if feature == AUTO_CORR_FEATURES[0] else '')\n",
        "\n",
        "    if decay_point:\n",
        "        decay_lag, decay_val = decay_point\n",
        "        all_decay_points.append(decay_point)\n",
        "        # Evidenzia il punto di decadimento\n",
        "        plt.scatter([decay_lag], [decay_val], s=80, marker='x', zorder=5,\n",
        "                   color='red', alpha=0.7)\n",
        "\n",
        "plt.axhline(y=MIN_AUTOCORR_THRESHOLD, color='r', linestyle='--', alpha=0.5,\n",
        "           label=f'Soglia picchi ({MIN_AUTOCORR_THRESHOLD})')\n",
        "plt.axhline(y=DECAY_THRESHOLD, color='orange', linestyle='--', alpha=0.5,\n",
        "           label=f'Soglia decadimento ({DECAY_THRESHOLD})')\n",
        "plt.title('Autocorrelazione media per feature selezionate')\n",
        "plt.xlabel('Lag')\n",
        "plt.ylabel('Autocorrelation')\n",
        "plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Determina WINDOW_SIZE suggerita basandosi sui picchi e/o decay points\n",
        "suggested_lags: List[int] = []\n",
        "\n",
        "if all_peaks:\n",
        "    # Prendi i top picchi (primi N per feature, pesati per autocorrelazione)\n",
        "    top_peaks = sorted(all_peaks, key=lambda x: x[1], reverse=True)[:len(AUTO_CORR_FEATURES) * 3]\n",
        "    peak_lags = [lag for lag, val in top_peaks if MIN_WINDOW_LAG <= lag <= MAX_WINDOW_LAG]\n",
        "    suggested_lags.extend(peak_lags)\n",
        "    print(f\"\\nüìä Picchi trovati: {len(all_peaks)} (top {len(peak_lags)} nel range utile)\")\n",
        "\n",
        "if all_decay_points:\n",
        "    # Usa anche i punti di decadimento come suggerimento\n",
        "    decay_lags = [lag for lag, val in all_decay_points if MIN_WINDOW_LAG <= lag <= MAX_WINDOW_LAG]\n",
        "    suggested_lags.extend(decay_lags)\n",
        "    print(f\"üìä Punti di decadimento trovati: {len(all_decay_points)} (range utile: {len(decay_lags)})\")\n",
        "\n",
        "# Strategia alternativa: trova il lag medio dove l'autocorrelazione √® ancora significativa\n",
        "if not suggested_lags:\n",
        "    # Calcola autocorrelazione media su tutte le feature\n",
        "    avg_autocorr = {}\n",
        "    for feature in AUTO_CORR_FEATURES:\n",
        "        lags, autocorr_vals = compute_autocorr_for_feature(feature, MAX_AUTOCORR_LAG)\n",
        "        for lag, val in zip(lags, autocorr_vals):\n",
        "            if MIN_WINDOW_LAG <= lag <= MAX_WINDOW_LAG:\n",
        "                if lag not in avg_autocorr:\n",
        "                    avg_autocorr[lag] = []\n",
        "                avg_autocorr[lag].append(val)\n",
        "\n",
        "    # Trova il lag dove l'autocorrelazione media scende sotto la soglia\n",
        "    for lag in sorted(avg_autocorr.keys()):\n",
        "        avg_val = np.mean(avg_autocorr[lag])\n",
        "        if avg_val >= MIN_AUTOCORR_THRESHOLD:\n",
        "            suggested_lags.append(lag)\n",
        "        elif len(suggested_lags) == 0:\n",
        "            # Se non abbiamo ancora nulla, prendi questo come fallback\n",
        "            suggested_lags.append(lag)\n",
        "            break\n",
        "\n",
        "if suggested_lags:\n",
        "    # Rimuovi duplicati e ordina\n",
        "    suggested_lags = sorted(set(suggested_lags))\n",
        "\n",
        "    # Usa diverse strategie per determinare la window size:\n",
        "    # 1. Mediana (robusta ai outliers)\n",
        "    median_window = int(np.median(suggested_lags))\n",
        "\n",
        "    # 2. Media (pi√π sensibile)\n",
        "    mean_window = int(np.mean(suggested_lags))\n",
        "\n",
        "    # 3. Primo quartile (conservativo, cattura il decadimento iniziale)\n",
        "    q25_window = int(np.percentile(suggested_lags, 25))\n",
        "\n",
        "    # 4. Preferisci valori nel range 20-40 se disponibili\n",
        "    preferred_range = [lag for lag in suggested_lags if 20 <= lag <= 40]\n",
        "    if preferred_range:\n",
        "        suggested_window = int(np.median(preferred_range))\n",
        "    else:\n",
        "        # Altrimenti usa la mediana\n",
        "        suggested_window = median_window\n",
        "\n",
        "    # Assicurati che sia ragionevole\n",
        "    suggested_window = max(MIN_WINDOW_LAG, min(suggested_window, MAX_WINDOW_LAG))\n",
        "\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"‚úÖ Analisi autocorrelazione completata\")\n",
        "    print(f\"{'='*60}\")\n",
        "    print(f\"Lags trovati nel range [{MIN_WINDOW_LAG}, {MAX_WINDOW_LAG}]: {len(suggested_lags)}\")\n",
        "    print(f\"Top 10 lags: {suggested_lags[:10]}\")\n",
        "    print(f\"\\nStatistiche:\")\n",
        "    print(f\"  Mediana: {median_window}\")\n",
        "    print(f\"  Media: {mean_window}\")\n",
        "    print(f\"  Q25: {q25_window}\")\n",
        "    print(f\"  Range preferito (20-40): {preferred_range if preferred_range else 'nessuno'}\")\n",
        "    print(f\"\\nüéØ WINDOW_SIZE suggerita: {suggested_window}\")\n",
        "    print(f\"   WINDOW_SIZE attuale: {WINDOW_SIZE}\")\n",
        "\n",
        "    # Aggiorna WINDOW_SIZE se il valore suggerito √® diverso\n",
        "    if abs(suggested_window - WINDOW_SIZE) >= 2:\n",
        "        print(f\"\\n‚ö†Ô∏è  Aggiornando WINDOW_SIZE da {WINDOW_SIZE} a {suggested_window}\")\n",
        "        WINDOW_SIZE = suggested_window\n",
        "        EVAL_WINDOW_SIZE = WINDOW_SIZE\n",
        "        print(f\"‚úÖ WINDOW_SIZE aggiornata a {WINDOW_SIZE}\")\n",
        "    else:\n",
        "        print(f\"\\n‚úì WINDOW_SIZE attuale ({WINDOW_SIZE}) √® coerente con l'autocorrelazione\")\n",
        "    print(f\"{'='*60}\\n\")\n",
        "else:\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"‚ö†Ô∏è  Nessun picco o punto di decadimento trovato nel range utile\")\n",
        "    print(f\"{'='*60}\")\n",
        "    print(f\"Mantenendo WINDOW_SIZE={WINDOW_SIZE} (default)\")\n",
        "    print(f\"\\nSuggerimento: verifica che:\")\n",
        "    print(f\"  - MIN_AUTOCORR_THRESHOLD ({MIN_AUTOCORR_THRESHOLD}) non sia troppo alto\")\n",
        "    print(f\"  - MAX_WINDOW_LAG ({MAX_WINDOW_LAG}) sia abbastanza grande\")\n",
        "    print(f\"  - Le feature selezionate ({AUTO_CORR_FEATURES}) abbiano autocorrelazione significativa\")\n",
        "    print(f\"{'='*60}\\n\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a5OnARfRoTCI"
      },
      "outputs": [],
      "source": [
        "#Dopo gather results\n",
        "\n",
        "# ============================================================================\n",
        "# HYPERPARAMETER TUNING CON CROSS-VALIDATION\n",
        "# ============================================================================\n",
        "# Questa cella esegue un tuning completo dei parametri pi√π importanti usando\n",
        "# Random Search con Cross-Validation. Pu√≤ richiedere molte ore.\n",
        "\n",
        "import json\n",
        "from random import choice, uniform, randint\n",
        "from time import time\n",
        "\n",
        "def update_class_weights(high_scale: float, low_scale: float, no_scale: float):\n",
        "    \"\"\"Aggiorna i class weights con i nuovi scale factors.\"\"\"\n",
        "    global HIGH_PAIN_WEIGHT_SCALE, LOW_PAIN_WEIGHT_SCALE, NO_PAIN_WEIGHT_SCALE, CLASS_WEIGHTS\n",
        "    HIGH_PAIN_WEIGHT_SCALE = float(high_scale)\n",
        "    LOW_PAIN_WEIGHT_SCALE = float(low_scale)\n",
        "    NO_PAIN_WEIGHT_SCALE = float(no_scale)\n",
        "\n",
        "    class_counts = np.bincount(y_train_idx, minlength=NUM_CLASSES)\n",
        "    class_weights = class_counts.sum() / (class_counts + 1e-6)\n",
        "    class_weights = class_weights / class_weights.mean()\n",
        "\n",
        "    if HIGH_PAIN_IDX:\n",
        "        for idx in HIGH_PAIN_IDX:\n",
        "            class_weights[idx] *= HIGH_PAIN_WEIGHT_SCALE\n",
        "    if LOW_PAIN_IDX:\n",
        "        for idx in LOW_PAIN_IDX:\n",
        "            class_weights[idx] *= LOW_PAIN_WEIGHT_SCALE\n",
        "    if NO_PAIN_IDX:\n",
        "        for idx in NO_PAIN_IDX:\n",
        "            class_weights[idx] *= NO_PAIN_WEIGHT_SCALE\n",
        "\n",
        "    CLASS_WEIGHTS = torch.tensor(class_weights, dtype=torch.float32)\n",
        "\n",
        "\n",
        "# Definizione degli spazi di ricerca per i parametri pi√π importanti\n",
        "PARAMETER_SEARCH_SPACES = {\n",
        "    # Class weight scales (MOLTO IMPORTANTI per class imbalance)\n",
        "    'high_pain_weight_scale': [0.1, 0.15, 0.2, 0.25, 0.3, 0.4, 0.5],\n",
        "    'low_pain_weight_scale': [0.3, 0.4, 0.465, 0.5, 0.6, 0.7],\n",
        "    'no_pain_weight_scale': [1.5, 2.0, 2.3, 2.5, 3.0],\n",
        "\n",
        "    # Loss function parameters (CRITICI per class imbalance)\n",
        "    'focal_gamma': [0.5, 0.75, 1.0, 1.25, 1.5, 2.0],\n",
        "    'label_smoothing': [0.0, 0.05, 0.1, 0.15],\n",
        "\n",
        "    # Model architecture (IMPORTANTI per capacit√†)\n",
        "    'hidden_size': [128, 192, 256, 320],\n",
        "    'num_layers': [2, 3, 4],\n",
        "    'dropout': [0.3, 0.35, 0.4, 0.45, 0.5],\n",
        "\n",
        "    # Training parameters\n",
        "    'lr': [1e-3, 1.5e-3, 2e-3, 2.5e-3, 3e-3],\n",
        "    'batch_size': [32, 64, 128],\n",
        "    'weight_decay': [1e-5, 5e-5, 1e-4, 2e-4],\n",
        "\n",
        "    # Scheduler\n",
        "    'scheduler_patience': [3, 5, 7],\n",
        "    'scheduler_factor': [0.3, 0.5, 0.7],\n",
        "\n",
        "    # Convolutional layers\n",
        "    'conv_layers': [1, 2, 3],\n",
        "    'conv_channels': [64, 96, 128, 160],\n",
        "    'conv_kernel_size': [3, 5, 7],\n",
        "\n",
        "    # Attention\n",
        "    'attention_hidden_dim': [64, 96, 128, 192],\n",
        "}\n",
        "\n",
        "\n",
        "def sample_random_config() -> Dict[str, Any]:\n",
        "    \"\"\"Campiona una configurazione casuale dallo spazio di ricerca.\"\"\"\n",
        "    config = {}\n",
        "    for param, values in PARAMETER_SEARCH_SPACES.items():\n",
        "        config[param] = choice(values)\n",
        "    return config\n",
        "\n",
        "\n",
        "def config_to_overrides(config: Dict[str, Any]) -> Dict[str, Any]:\n",
        "    \"\"\"Converte la configurazione campionata in overrides per prepare_config.\"\"\"\n",
        "    overrides = {\n",
        "        'hidden_size': config['hidden_size'],\n",
        "        'num_layers': config['num_layers'],\n",
        "        'dropout': config['dropout'],\n",
        "        'lr': config['lr'],\n",
        "        'batch_size': config['batch_size'],\n",
        "        'weight_decay': config['weight_decay'],\n",
        "        'scheduler_patience': config['scheduler_patience'],\n",
        "        'scheduler_factor': config['scheduler_factor'],\n",
        "        'focal_gamma': config['focal_gamma'],\n",
        "        'label_smoothing': config['label_smoothing'],\n",
        "        'conv_layers': config['conv_layers'],\n",
        "        'conv_channels': config['conv_channels'],\n",
        "        'conv_kernel_size': config['conv_kernel_size'],\n",
        "        'attention_hidden_dim': config['attention_hidden_dim'],\n",
        "    }\n",
        "    return overrides\n",
        "\n",
        "\n",
        "def run_hyperparameter_tuning(\n",
        "    n_trials: int = 50,\n",
        "    n_splits: int = 5,\n",
        "    verbose: bool = True,\n",
        "    save_results: bool = True,\n",
        ") -> Tuple[List[Dict], pd.DataFrame]:\n",
        "    \"\"\"\n",
        "    Esegue hyperparameter tuning usando Random Search con Cross-Validation.\n",
        "\n",
        "    Args:\n",
        "        n_trials: Numero di configurazioni da testare\n",
        "        n_splits: Numero di fold per CV\n",
        "        verbose: Stampa progressi\n",
        "        save_results: Salva i risultati su file\n",
        "\n",
        "    Returns:\n",
        "        Lista di risultati e DataFrame riassuntivo\n",
        "    \"\"\"\n",
        "    tuning_results: List[Dict] = []\n",
        "    tuning_start = time()\n",
        "\n",
        "    print(f\"{'='*80}\")\n",
        "    print(f\"üöÄ INIZIO HYPERPARAMETER TUNING\")\n",
        "    print(f\"{'='*80}\")\n",
        "    print(f\"Trials: {n_trials} | CV Folds: {n_splits} | Parametri: {len(PARAMETER_SEARCH_SPACES)}\")\n",
        "    print(f\"{'='*80}\\n\")\n",
        "\n",
        "    best_f1 = -np.inf\n",
        "    best_config = None\n",
        "\n",
        "    for trial_idx in range(1, n_trials + 1):\n",
        "        trial_start = time()\n",
        "\n",
        "        # Campiona configurazione casuale\n",
        "        sampled_config = sample_random_config()\n",
        "\n",
        "        # Aggiorna class weights\n",
        "        update_class_weights(\n",
        "            sampled_config['high_pain_weight_scale'],\n",
        "            sampled_config['low_pain_weight_scale'],\n",
        "            sampled_config['no_pain_weight_scale'],\n",
        "        )\n",
        "\n",
        "        # Prepara configurazione per il modello\n",
        "        overrides = config_to_overrides(sampled_config)\n",
        "        run_name = f\"TUNE_T{trial_idx:03d}_{datetime.now().strftime('%H%M%S')}\"\n",
        "        overrides['run_name'] = run_name\n",
        "        overrides['tensorboard'] = False  # Disabilita TensorBoard per velocit√†\n",
        "\n",
        "        config = prepare_config(run_name, overrides)\n",
        "\n",
        "        if verbose:\n",
        "            print(f\"\\n{'‚îÄ'*80}\")\n",
        "            print(f\"Trial {trial_idx}/{n_trials}: {run_name}\")\n",
        "            print(f\"{'‚îÄ'*80}\")\n",
        "            print(f\"  Weight scales: H={sampled_config['high_pain_weight_scale']:.3f} | \"\n",
        "                  f\"L={sampled_config['low_pain_weight_scale']:.3f} | \"\n",
        "                  f\"N={sampled_config['no_pain_weight_scale']:.3f}\")\n",
        "            print(f\"  Model: hidden={config['hidden_size']}, layers={config['num_layers']}, \"\n",
        "                  f\"dropout={config['dropout']:.2f}\")\n",
        "            print(f\"  Training: lr={config['lr']:.4f}, batch={config['batch_size']}, \"\n",
        "                  f\"wd={config['weight_decay']:.5f}\")\n",
        "            print(f\"  Loss: focal_gamma={config['focal_gamma']:.2f}, \"\n",
        "                  f\"label_smoothing={config['label_smoothing']:.2f}\")\n",
        "            print(f\"  Conv: layers={config['conv_layers']}, channels={config['conv_channels']}, \"\n",
        "                  f\"kernel={config['conv_kernel_size']}\")\n",
        "\n",
        "        try:\n",
        "            # Esegui cross-validation\n",
        "            cv_results = run_cross_validation(\n",
        "                config,\n",
        "                n_splits=n_splits,\n",
        "                shuffle=True,\n",
        "                random_state=SEED,\n",
        "                verbose=False,  # Riduce output durante tuning\n",
        "            )\n",
        "\n",
        "            # Calcola metriche aggregate\n",
        "            f1_scores = [r['metrics']['f1'] for r in cv_results]\n",
        "            acc_scores = [r['metrics']['accuracy'] for r in cv_results]\n",
        "\n",
        "            mean_f1 = np.mean(f1_scores)\n",
        "            std_f1 = np.std(f1_scores)\n",
        "            mean_acc = np.mean(acc_scores)\n",
        "            std_acc = np.std(acc_scores)\n",
        "\n",
        "            trial_time = time() - trial_start\n",
        "\n",
        "            result = {\n",
        "                'trial': trial_idx,\n",
        "                'run_name': run_name,\n",
        "                'config': config,\n",
        "                'sampled_config': sampled_config,\n",
        "                'cv_results': cv_results,\n",
        "                'mean_f1': mean_f1,\n",
        "                'std_f1': std_f1,\n",
        "                'mean_acc': mean_acc,\n",
        "                'std_acc': std_acc,\n",
        "                'trial_time': trial_time,\n",
        "            }\n",
        "\n",
        "            tuning_results.append(result)\n",
        "\n",
        "            # Aggiorna best\n",
        "            if mean_f1 > best_f1:\n",
        "                best_f1 = mean_f1\n",
        "                best_config = sampled_config.copy()\n",
        "                best_result = result\n",
        "\n",
        "            if verbose:\n",
        "                print(f\"  ‚úÖ CV F1: {mean_f1:.4f} ¬± {std_f1:.4f} | \"\n",
        "                      f\"Acc: {mean_acc:.4f} ¬± {std_acc:.4f} | \"\n",
        "                      f\"Time: {trial_time:.1f}s\")\n",
        "                if mean_f1 > best_f1 - 1e-6:\n",
        "                    print(f\"  üèÜ NUOVO BEST! F1={mean_f1:.4f}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            trial_time = time() - trial_start\n",
        "            if verbose:\n",
        "                print(f\"  ‚ùå ERRORE: {str(e)} | Time: {trial_time:.1f}s\")\n",
        "            result = {\n",
        "                'trial': trial_idx,\n",
        "                'run_name': run_name,\n",
        "                'config': config,\n",
        "                'sampled_config': sampled_config,\n",
        "                'error': str(e),\n",
        "                'trial_time': trial_time,\n",
        "            }\n",
        "            tuning_results.append(result)\n",
        "\n",
        "        # Progress update ogni 10 trials\n",
        "        if trial_idx % 10 == 0:\n",
        "            elapsed = time() - tuning_start\n",
        "            avg_time = elapsed / trial_idx\n",
        "            remaining = avg_time * (n_trials - trial_idx)\n",
        "            print(f\"\\nüìä Progress: {trial_idx}/{n_trials} | \"\n",
        "                  f\"Elapsed: {elapsed/60:.1f}m | \"\n",
        "                  f\"Remaining: ~{remaining/60:.1f}m | \"\n",
        "                  f\"Best F1: {best_f1:.4f}\")\n",
        "\n",
        "    total_time = time() - tuning_start\n",
        "\n",
        "    # Crea DataFrame riassuntivo\n",
        "    summary_rows = []\n",
        "    for res in tuning_results:\n",
        "        if 'error' not in res:\n",
        "            row = {\n",
        "                'trial': res['trial'],\n",
        "                'run_name': res['run_name'],\n",
        "                'mean_f1': res['mean_f1'],\n",
        "                'std_f1': res['std_f1'],\n",
        "                'mean_acc': res['mean_acc'],\n",
        "                'std_acc': res['std_acc'],\n",
        "                'trial_time': res['trial_time'],\n",
        "            }\n",
        "            # Aggiungi parametri chiave\n",
        "            sc = res['sampled_config']\n",
        "            row.update({\n",
        "                'high_scale': sc['high_pain_weight_scale'],\n",
        "                'low_scale': sc['low_pain_weight_scale'],\n",
        "                'no_scale': sc['no_pain_weight_scale'],\n",
        "                'focal_gamma': sc['focal_gamma'],\n",
        "                'label_smoothing': sc['label_smoothing'],\n",
        "                'hidden_size': sc['hidden_size'],\n",
        "                'num_layers': sc['num_layers'],\n",
        "                'dropout': sc['dropout'],\n",
        "                'lr': sc['lr'],\n",
        "                'batch_size': sc['batch_size'],\n",
        "            })\n",
        "            summary_rows.append(row)\n",
        "\n",
        "    summary_df = pd.DataFrame(summary_rows)\n",
        "\n",
        "    # Stampa risultati finali\n",
        "    print(f\"\\n{'='*80}\")\n",
        "    print(f\"‚úÖ TUNING COMPLETATO\")\n",
        "    print(f\"{'='*80}\")\n",
        "    print(f\"Total trials: {n_trials}\")\n",
        "    print(f\"Successful: {len(summary_df)}\")\n",
        "    print(f\"Total time: {total_time/3600:.2f} hours ({total_time/60:.1f} minutes)\")\n",
        "    print(f\"\\nüèÜ MIGLIOR CONFIGURAZIONE (F1={best_f1:.4f}):\")\n",
        "    print(f\"{'‚îÄ'*80}\")\n",
        "    for key, value in best_config.items():\n",
        "        print(f\"  {key}: {value}\")\n",
        "    print(f\"{'='*80}\\n\")\n",
        "\n",
        "    # Salva risultati\n",
        "    if save_results:\n",
        "        results_dir = OUTPUT_DIR / 'tuning_results'\n",
        "        results_dir.mkdir(exist_ok=True, parents=True)\n",
        "\n",
        "        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
        "\n",
        "        # Salva DataFrame\n",
        "        summary_path = results_dir / f'tuning_summary_{timestamp}.csv'\n",
        "        summary_df.to_csv(summary_path, index=False)\n",
        "        print(f\"üíæ Summary salvato: {summary_path}\")\n",
        "\n",
        "        # Salva best config\n",
        "        best_config_path = results_dir / f'best_config_{timestamp}.json'\n",
        "        with open(best_config_path, 'w') as f:\n",
        "            json.dump(best_config, f, indent=2)\n",
        "        print(f\"üíæ Best config salvato: {best_config_path}\")\n",
        "\n",
        "        # Salva tutti i risultati (solo metadata, non i modelli)\n",
        "        all_results_meta = []\n",
        "        for res in tuning_results:\n",
        "            meta = {\n",
        "                'trial': res['trial'],\n",
        "                'run_name': res['run_name'],\n",
        "                'sampled_config': res['sampled_config'],\n",
        "            }\n",
        "            if 'error' not in res:\n",
        "                meta.update({\n",
        "                    'mean_f1': res['mean_f1'],\n",
        "                    'std_f1': res['std_f1'],\n",
        "                    'mean_acc': res['mean_acc'],\n",
        "                })\n",
        "            else:\n",
        "                meta['error'] = res['error']\n",
        "            all_results_meta.append(meta)\n",
        "\n",
        "        all_results_path = results_dir / f'all_results_{timestamp}.json'\n",
        "        with open(all_results_path, 'w') as f:\n",
        "            json.dump(all_results_meta, f, indent=2)\n",
        "        print(f\"üíæ All results salvato: {all_results_path}\\n\")\n",
        "\n",
        "    return tuning_results, summary_df\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# ESECUZIONE TUNING\n",
        "# ============================================================================\n",
        "# Modifica questi parametri per controllare il tuning:\n",
        "# - n_trials: numero di configurazioni da testare (pi√π = meglio ma pi√π lento)\n",
        "# - n_splits: numero di fold per CV (5 √® un buon compromesso)\n",
        "\n",
        "TUNING_N_TRIALS = 50  # Aumenta per pi√π esplorazione (50-100+ per risultati migliori)\n",
        "TUNING_N_SPLITS = 5   # 5-fold CV √® un buon compromesso\n",
        "\n",
        "print(\"‚ö†Ô∏è  ATTENZIONE: Il tuning pu√≤ richiedere molte ore!\")\n",
        "print(f\"   Configurato per {TUNING_N_TRIALS} trials con {TUNING_N_SPLITS}-fold CV\")\n",
        "print(\"   Per iniziare, decommenta la riga seguente:\\n\")\n",
        "print(\"# tuning_results, tuning_summary = run_hyperparameter_tuning(\")\n",
        "print(f\"#     n_trials={TUNING_N_TRIALS},\")\n",
        "print(f\"#     n_splits={TUNING_N_SPLITS},\")\n",
        "print(\"#     verbose=True,\")\n",
        "print(\"#     save_results=True,\")\n",
        "print(\"# )\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZG_XwiNSoTCJ"
      },
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# ANALISI RISULTATI TUNING E APPLICAZIONE MIGLIORE CONFIGURAZIONE\n",
        "# ============================================================================\n",
        "\n",
        "def analyze_tuning_results(tuning_summary: pd.DataFrame, top_n: int = 10):\n",
        "    \"\"\"\n",
        "    Analizza i risultati del tuning e mostra i migliori parametri.\n",
        "    \"\"\"\n",
        "    if tuning_summary.empty:\n",
        "        print(\"‚ö†Ô∏è  Nessun risultato disponibile per l'analisi.\")\n",
        "        return None\n",
        "\n",
        "    print(f\"{'='*80}\")\n",
        "    print(f\"üìä ANALISI RISULTATI TUNING\")\n",
        "    print(f\"{'='*80}\\n\")\n",
        "\n",
        "    # Top N configurazioni\n",
        "    top_configs = tuning_summary.nlargest(top_n, 'mean_f1')\n",
        "\n",
        "    print(f\"üèÜ TOP {top_n} CONFIGURAZIONI (per F1 score):\")\n",
        "    print(f\"{'‚îÄ'*80}\")\n",
        "    for idx, (_, row) in enumerate(top_configs.iterrows(), 1):\n",
        "        print(f\"\\n{idx}. F1={row['mean_f1']:.4f} ¬± {row['std_f1']:.4f} | \"\n",
        "              f\"Acc={row['mean_acc']:.4f} ¬± {row['std_acc']:.4f}\")\n",
        "        print(f\"   Weights: H={row['high_scale']:.3f} | L={row['low_scale']:.3f} | N={row['no_scale']:.3f}\")\n",
        "        print(f\"   Model: hidden={row['hidden_size']}, layers={row['num_layers']}, dropout={row['dropout']:.2f}\")\n",
        "        print(f\"   Training: lr={row['lr']:.4f}, batch={row['batch_size']}, focal_gamma={row['focal_gamma']:.2f}\")\n",
        "\n",
        "    # Analisi correlazioni\n",
        "    print(f\"\\n{'‚îÄ'*80}\")\n",
        "    print(\"üìà CORRELAZIONI PARAMETRI vs F1 SCORE:\")\n",
        "    print(f\"{'‚îÄ'*80}\")\n",
        "\n",
        "    param_cols = ['high_scale', 'low_scale', 'no_scale', 'focal_gamma', 'label_smoothing',\n",
        "                  'hidden_size', 'num_layers', 'dropout', 'lr', 'batch_size']\n",
        "\n",
        "    correlations = []\n",
        "    for col in param_cols:\n",
        "        if col in tuning_summary.columns:\n",
        "            corr = tuning_summary[col].corr(tuning_summary['mean_f1'])\n",
        "            correlations.append((col, corr))\n",
        "\n",
        "    correlations.sort(key=lambda x: abs(x[1]), reverse=True)\n",
        "\n",
        "    for param, corr in correlations:\n",
        "        direction = \"‚Üë\" if corr > 0 else \"‚Üì\"\n",
        "        strength = \"forte\" if abs(corr) > 0.3 else \"moderata\" if abs(corr) > 0.15 else \"debole\"\n",
        "        print(f\"  {param:20s}: {corr:7.3f} {direction} ({strength})\")\n",
        "\n",
        "    # Statistiche per parametri categoriali\n",
        "    print(f\"\\n{'‚îÄ'*80}\")\n",
        "    print(\"üìä F1 MEDIO PER VALORE PARAMETRO:\")\n",
        "    print(f\"{'‚îÄ'*80}\")\n",
        "\n",
        "    for param in ['hidden_size', 'num_layers', 'batch_size', 'focal_gamma']:\n",
        "        if param in tuning_summary.columns:\n",
        "            grouped = tuning_summary.groupby(param)['mean_f1'].agg(['mean', 'std', 'count'])\n",
        "            grouped = grouped.sort_values('mean', ascending=False)\n",
        "            print(f\"\\n{param}:\")\n",
        "            for val, row in grouped.iterrows():\n",
        "                print(f\"  {val:10s}: F1={row['mean']:.4f} ¬± {row['std']:.4f} (n={int(row['count'])})\")\n",
        "\n",
        "    return top_configs\n",
        "\n",
        "\n",
        "def apply_best_config(tuning_summary: pd.DataFrame, tuning_results: List[Dict] = None):\n",
        "    \"\"\"\n",
        "    Applica la migliore configurazione trovata dal tuning.\n",
        "    \"\"\"\n",
        "    if tuning_summary.empty:\n",
        "        print(\"‚ö†Ô∏è  Nessun risultato disponibile.\")\n",
        "        return None\n",
        "\n",
        "    best_row = tuning_summary.nlargest(1, 'mean_f1').iloc[0]\n",
        "    best_trial = int(best_row['trial'])\n",
        "\n",
        "    print(f\"{'='*80}\")\n",
        "    print(f\"‚úÖ APPLICAZIONE MIGLIORE CONFIGURAZIONE\")\n",
        "    print(f\"{'='*80}\\n\")\n",
        "    print(f\"Best trial: {best_trial} | F1={best_row['mean_f1']:.4f} ¬± {best_row['std_f1']:.4f}\")\n",
        "    print(f\"\\nParametri da applicare:\")\n",
        "    print(f\"  Weight scales: H={best_row['high_scale']:.3f} | L={best_row['low_scale']:.3f} | N={best_row['no_scale']:.3f}\")\n",
        "    print(f\"  Model: hidden={best_row['hidden_size']}, layers={best_row['num_layers']}, dropout={best_row['dropout']:.2f}\")\n",
        "    print(f\"  Training: lr={best_row['lr']:.4f}, batch={best_row['batch_size']}, wd={best_row.get('weight_decay', 'N/A')}\")\n",
        "    print(f\"  Loss: focal_gamma={best_row['focal_gamma']:.2f}, label_smoothing={best_row.get('label_smoothing', 0.0):.2f}\")\n",
        "\n",
        "    # Trova la configurazione completa dal tuning_results\n",
        "    if tuning_results:\n",
        "        best_result = next((r for r in tuning_results if r['trial'] == best_trial), None)\n",
        "        if best_result and 'sampled_config' in best_result:\n",
        "            sampled_config = best_result['sampled_config']\n",
        "\n",
        "            # Aggiorna class weights\n",
        "            update_class_weights(\n",
        "                sampled_config['high_pain_weight_scale'],\n",
        "                sampled_config['low_pain_weight_scale'],\n",
        "                sampled_config['no_pain_weight_scale'],\n",
        "            )\n",
        "            print(f\"\\n‚úÖ Class weights aggiornati!\")\n",
        "\n",
        "            # Prepara configurazione per il modello finale\n",
        "            overrides = config_to_overrides(sampled_config)\n",
        "            overrides['tensorboard'] = True  # Riabilita per training finale\n",
        "\n",
        "            best_config = prepare_config(f\"BEST_TUNED_{best_trial}\", overrides)\n",
        "\n",
        "            print(f\"\\nüìù Per usare questa configurazione nel training finale:\")\n",
        "            print(f\"   best_tuned_config = prepare_config('BEST_TUNED', {overrides})\")\n",
        "            print(f\"\\n   Oppure modifica manualmente:\")\n",
        "            print(f\"   HIGH_PAIN_WEIGHT_SCALE = {sampled_config['high_pain_weight_scale']}\")\n",
        "            print(f\"   LOW_PAIN_WEIGHT_SCALE = {sampled_config['low_pain_weight_scale']}\")\n",
        "            print(f\"   NO_PAIN_WEIGHT_SCALE = {sampled_config['no_pain_weight_scale']}\")\n",
        "\n",
        "            return best_config, sampled_config\n",
        "\n",
        "    return None, None\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# USO: Dopo aver eseguito il tuning, usa queste funzioni:\n",
        "# ============================================================================\n",
        "# 1. Analizza i risultati:\n",
        "#    top_configs = analyze_tuning_results(tuning_summary, top_n=10)\n",
        "#\n",
        "# 2. Applica la migliore configurazione:\n",
        "#    best_config, best_sampled = apply_best_config(tuning_summary, tuning_results)\n",
        "#\n",
        "# 3. Esegui training finale con la migliore config:\n",
        "#    final_result = run_cross_validation(best_config, n_splits=5)\n",
        "\n",
        "print(\"üìã Funzioni di analisi pronte!\")\n",
        "print(\"   Dopo il tuning, usa: analyze_tuning_results() e apply_best_config()\")\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}