{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d0iHUb-QTBGM"
      },
      "source": [
        "# üè¥‚Äç‚ò†Ô∏è AN2DL25 Challenge 1 ‚Äî Pirate Pain Classification\n",
        "\n",
        "This notebook implements a full deep-learning pipeline for multivariate time-series classification of the Pirate Pain dataset. It is inspired by the Lecture 4 notebook (`Timeseries Classification (1).ipynb`) but adapted for the competition setting, including data preparation, model training (RNN/GRU/LSTM variants), evaluation, and test-time inference.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 478,
      "metadata": {
        "id": "yD8eSdFgTBGP"
      },
      "outputs": [],
      "source": [
        "# %%capture\n",
        "# !pip install -q -r requirements.txt\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 479,
      "metadata": {
        "id": "TNB0xa3YTBGR"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import random\n",
        "import math\n",
        "import copy\n",
        "from pathlib import Path\n",
        "from typing import Tuple, Dict, Optional, List, Any\n",
        "from datetime import datetime\n",
        "from itertools import product\n",
        "import inspect\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.metrics import (\n",
        "    f1_score,\n",
        "    classification_report,\n",
        "    confusion_matrix,\n",
        "    precision_recall_fscore_support,\n",
        ")\n",
        "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "import torch.nn.functional as F\n",
        "\n",
        "try:\n",
        "    from torch.amp import autocast, GradScaler\n",
        "except ImportError:  # pragma: no cover\n",
        "    from torch.cuda.amp import autocast, GradScaler\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 480,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JReu2QJbTBGR",
        "outputId": "62a744b8-07e5-4d16-f263-6b4b8f1c54d9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<>:11: SyntaxWarning: invalid escape sequence '\\ '\n",
            "<>:11: SyntaxWarning: invalid escape sequence '\\ '\n",
            "/tmp/ipython-input-1665548672.py:11: SyntaxWarning: invalid escape sequence '\\ '\n",
            "  BASE_DIR = Path('/content/drive/MyDrive/[2025-2026]\\ A2NDL/Challenge')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Usando DATA_DIR: /content/drive/MyDrive/[2025-2026] AN2DL/Challenge\n",
            "Running in Colab: True\n",
            "Device: cuda\n",
            "Data dir: /content/drive/MyDrive/[2025-2026] AN2DL/Challenge\n",
            "Output dir: /content/drive/MyDrive/[2025-2026]\\ A2NDL/Challenge/outputs\n"
          ]
        }
      ],
      "source": [
        "SEED = 2024\n",
        "\n",
        "try:\n",
        "    from google.colab import drive  # type: ignore\n",
        "    IN_COLAB = True\n",
        "except ImportError:  # pragma: no cover\n",
        "    IN_COLAB = False\n",
        "\n",
        "if IN_COLAB:\n",
        "    drive.mount('/content/drive', force_remount=False)\n",
        "    BASE_DIR = Path('/content/drive/MyDrive/[2025-2026]\\ A2NDL/Challenge')\n",
        "else:\n",
        "    BASE_DIR = Path('/Users/md101ta/Desktop/Pirates')\n",
        "\n",
        "DATA_DIR = (BASE_DIR / 'data').resolve()\n",
        "# subito dopo aver definito DATA_DIR in cella 3\n",
        "DATA_DIR_CANDIDATES = [\n",
        "    Path('/content/drive/MyDrive/[2025-2026] AN2DL/Challenge'),\n",
        "    Path('/content/drive/MyDrive/[2025-2026]\\\\ A2NDL/Challenge'),\n",
        "    BASE_DIR / 'data'\n",
        "]\n",
        "\n",
        "for candidate in DATA_DIR_CANDIDATES:\n",
        "    if candidate.exists():\n",
        "        DATA_DIR = candidate\n",
        "        break\n",
        "else:\n",
        "    raise FileNotFoundError('Nessuna DATA_DIR valida trovata')\n",
        "\n",
        "print(f'Usando DATA_DIR: {DATA_DIR}')\n",
        "OUTPUT_DIR = (BASE_DIR / 'outputs').resolve()\n",
        "OUTPUT_DIR.mkdir(exist_ok=True, parents=True)\n",
        "\n",
        "# Reproducibility\n",
        "os.environ['PYTHONHASHSEED'] = str(SEED)\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(SEED)\n",
        "    DEVICE = torch.device('cuda')\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "else:\n",
        "    DEVICE = torch.device('cpu')\n",
        "\n",
        "print(f'Running in Colab: {IN_COLAB}')\n",
        "print(f'Device: {DEVICE}')\n",
        "print(f'Data dir: {DATA_DIR}')\n",
        "print(f'Output dir: {OUTPUT_DIR}')\n",
        "\n",
        "_AUTocast_params = inspect.signature(autocast).parameters\n",
        "_GRADSCALER_PARAMS = inspect.signature(GradScaler).parameters\n",
        "\n",
        "def autocast_context():\n",
        "    enabled = DEVICE.type == 'cuda'\n",
        "    if 'device_type' in _AUTocast_params:\n",
        "        return autocast(device_type=DEVICE.type, enabled=enabled)\n",
        "    if 'device' in _AUTocast_params:\n",
        "        return autocast(DEVICE.type, enabled=enabled)\n",
        "    # fallback to legacy signature (enabled only)\n",
        "    return autocast(enabled=enabled)\n",
        "\n",
        "\n",
        "def create_grad_scaler():\n",
        "    enabled = DEVICE.type == 'cuda'\n",
        "    if 'device_type' in _GRADSCALER_PARAMS:\n",
        "        return GradScaler(device_type=DEVICE.type, enabled=enabled)\n",
        "    return GradScaler(enabled=enabled)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 481,
      "metadata": {
        "id": "Us5CnwHwTBGS"
      },
      "outputs": [],
      "source": [
        "LOG_DIR = (OUTPUT_DIR / 'logs').resolve()\n",
        "CHECKPOINT_DIR = (OUTPUT_DIR / 'checkpoints').resolve()\n",
        "LOG_DIR.mkdir(exist_ok=True, parents=True)\n",
        "CHECKPOINT_DIR.mkdir(exist_ok=True, parents=True)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 482,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-oigFvDtTBGS",
        "outputId": "fb576a58-1bf5-4976-a84d-b00b4081c194"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(105760, 40) (661, 2) (211840, 40)\n"
          ]
        }
      ],
      "source": [
        "from typing import Tuple\n",
        "\n",
        "assert DATA_DIR.exists(), f\"DATA_DIR non esiste: {DATA_DIR}\"\n",
        "\n",
        "def load_data(data_dir: Path) -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n",
        "    X_train = pd.read_csv(data_dir / 'pirate_pain_train.csv')\n",
        "    y_train = pd.read_csv(data_dir / 'pirate_pain_train_labels.csv')\n",
        "    X_test  = pd.read_csv(data_dir / 'pirate_pain_test.csv')\n",
        "    return X_train, y_train, X_test\n",
        "\n",
        "# Carica i dati\n",
        "X_train_raw, y_train, X_test_raw = load_data(DATA_DIR)\n",
        "print(X_train_raw.shape, y_train.shape, X_test_raw.shape)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 483,
      "metadata": {
        "id": "_W_AicMnQLiH",
        "outputId": "bf5c4a47-043e-4678-cbb9-62266e0fab83",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Salvato report: /content/drive/MyDrive/[2025-2026]\\ A2NDL/Challenge/outputs/reports/feature_describe.csv\n",
            "Salvato report: /content/drive/MyDrive/[2025-2026]\\ A2NDL/Challenge/outputs/reports/feature_missing.csv\n",
            "Salvato report: /content/drive/MyDrive/[2025-2026]\\ A2NDL/Challenge/outputs/reports/feature_variance.csv\n",
            "Salvato report: /content/drive/MyDrive/[2025-2026]\\ A2NDL/Challenge/outputs/reports/feature_correlation.csv\n",
            "Salvato report: /content/drive/MyDrive/[2025-2026]\\ A2NDL/Challenge/outputs/reports/feature_outlier_ratio.csv\n",
            "Profiling completato. Totale feature analizzate: 38\n"
          ]
        }
      ],
      "source": [
        "# -- Data profiling & feature diagnostics -------------------------------------------------\n",
        "REPORT_DIR = (OUTPUT_DIR / 'reports').resolve()\n",
        "REPORT_DIR.mkdir(exist_ok=True, parents=True)\n",
        "\n",
        "X_train_raw = X_train_raw.sort_values(['sample_index', 'time']).reset_index(drop=True)\n",
        "X_test_raw = X_test_raw.sort_values(['sample_index', 'time']).reset_index(drop=True)\n",
        "\n",
        "FEATURE_BASE_COLUMNS = [col for col in X_train_raw.columns if col not in ['sample_index', 'time']]\n",
        "NUMERIC_FEATURE_BASE_COLUMNS = X_train_raw[FEATURE_BASE_COLUMNS].select_dtypes(include=[np.number]).columns.tolist()\n",
        "feature_audit_entries: List[Dict[str, Any]] = []\n",
        "\n",
        "if not NUMERIC_FEATURE_BASE_COLUMNS:\n",
        "    raise RuntimeError('Nessuna feature numerica trovata per il profiling. Verifica il caricamento dati.')\n",
        "\n",
        "def save_report(df: pd.DataFrame, filename: str):\n",
        "    output_path = REPORT_DIR / filename\n",
        "    df.to_csv(output_path)\n",
        "    print(f\"Salvato report: {output_path}\")\n",
        "\n",
        "# Statistiche descrittive\n",
        "feature_describe = X_train_raw[NUMERIC_FEATURE_BASE_COLUMNS].describe().T\n",
        "save_report(feature_describe, 'feature_describe.csv')\n",
        "\n",
        "# Missing values\n",
        "missing_info = pd.DataFrame({\n",
        "    'missing_count': X_train_raw[FEATURE_BASE_COLUMNS].isna().sum(),\n",
        "    'missing_ratio': X_train_raw[FEATURE_BASE_COLUMNS].isna().mean(),\n",
        "})\n",
        "save_report(missing_info, 'feature_missing.csv')\n",
        "\n",
        "# Varianza\n",
        "variance_info = X_train_raw[NUMERIC_FEATURE_BASE_COLUMNS].var().to_frame(name='variance')\n",
        "save_report(variance_info, 'feature_variance.csv')\n",
        "\n",
        "# Correlazioni (assolute)\n",
        "corr_matrix = X_train_raw[NUMERIC_FEATURE_BASE_COLUMNS].corr().abs()\n",
        "save_report(corr_matrix, 'feature_correlation.csv')\n",
        "\n",
        "# Outlier ratio (valori oltre 5 sigma)\n",
        "feature_means = X_train_raw[NUMERIC_FEATURE_BASE_COLUMNS].mean()\n",
        "feature_stds = X_train_raw[NUMERIC_FEATURE_BASE_COLUMNS].std() + 1e-8\n",
        "z_scores = np.abs((X_train_raw[NUMERIC_FEATURE_BASE_COLUMNS] - feature_means) / feature_stds)\n",
        "outlier_ratio = (z_scores > 5).sum().div(len(X_train_raw)).to_frame(name='outlier_ratio')\n",
        "save_report(outlier_ratio, 'feature_outlier_ratio.csv')\n",
        "\n",
        "print('Profiling completato. Totale feature analizzate:', len(FEATURE_BASE_COLUMNS))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 484,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 594
        },
        "id": "G1_qoIZ2TBGT",
        "outputId": "3aa84225-d21c-427c-9338-eb37bf0186cc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Removing features: ['joint_13', 'joint_14', 'joint_15', 'joint_16', 'joint_23', 'joint_24', 'joint_30', 'n_eyes', 'n_hands']\n",
            "Salvato report: /content/drive/MyDrive/[2025-2026]\\ A2NDL/Challenge/outputs/reports/feature_removal_log.csv\n",
            "Time steps (raw): 160 | Window size: 25 | Features: 39 | Classes: 3\n",
            "Category mappings: {'n_legs': {'one+peg_leg': 0, 'two': 1}, 'n_hands': {'one+hook_hand': 0, 'two': 1}, 'n_eyes': {'one+eye_patch': 0, 'two': 1}}\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "CATEGORICAL_COLUMNS = ['n_legs', 'n_hands', 'n_eyes']\n",
        "CATEGORY_MAPPINGS: Dict[str, Dict[str, int]] = {}\n",
        "\n",
        "for col in CATEGORICAL_COLUMNS:\n",
        "    uniques = pd.concat([X_train_raw[col], X_test_raw[col]]).dropna().unique()\n",
        "    mapping = {value: idx for idx, value in enumerate(sorted(uniques))}\n",
        "    CATEGORY_MAPPINGS[col] = mapping\n",
        "    X_train_raw[col] = X_train_raw[col].map(mapping).astype(np.int32)\n",
        "    X_test_raw[col] = X_test_raw[col].map(mapping).astype(np.int32)\n",
        "\n",
        "FEATURE_COLUMNS = [col for col in X_train_raw.columns if col not in ['sample_index', 'time']]\n",
        "\n",
        "LOW_VARIANCE_THRESHOLD = 2e-3\n",
        "LOW_MAG_THRESHOLD = 1e-3\n",
        "HIGH_CORR_THRESHOLD = 0.98\n",
        "\n",
        "feature_std = X_train_raw[FEATURE_COLUMNS].std()\n",
        "feature_abs_max = X_train_raw[FEATURE_COLUMNS].abs().max()\n",
        "\n",
        "# Helper per loggare le feature rimosse\n",
        "\n",
        "def log_feature_removal(feature: str, reason: str, metric_value: Optional[float] = None, notes: Optional[str] = None):\n",
        "    entry = {\n",
        "        'feature': feature,\n",
        "        'reason': reason,\n",
        "        'metric': metric_value,\n",
        "        'notes': notes,\n",
        "    }\n",
        "    feature_audit_entries.append(entry)\n",
        "\n",
        "# Colonne costanti\n",
        "constant_features = [col for col in FEATURE_COLUMNS if X_train_raw[col].nunique(dropna=False) <= 1]\n",
        "for feature in constant_features:\n",
        "    log_feature_removal(feature, 'constant_value', X_train_raw[feature].iloc[0])\n",
        "\n",
        "# Bassa varianza\n",
        "low_variance = feature_std[feature_std <= LOW_VARIANCE_THRESHOLD]\n",
        "for feature, value in low_variance.items():\n",
        "    log_feature_removal(feature, 'low_variance', float(value))\n",
        "\n",
        "# Bassa magnitudo\n",
        "low_magnitude = feature_abs_max[feature_abs_max <= LOW_MAG_THRESHOLD]\n",
        "for feature, value in low_magnitude.items():\n",
        "    log_feature_removal(feature, 'low_magnitude', float(value))\n",
        "\n",
        "# Alta correlazione\n",
        "corr_matrix = X_train_raw[FEATURE_COLUMNS].corr().abs()\n",
        "upper_mask = np.triu(np.ones_like(corr_matrix, dtype=bool), k=1)\n",
        "upper_corr = corr_matrix.where(upper_mask)\n",
        "high_corr_features = set()\n",
        "for col in upper_corr.columns:\n",
        "    partners = upper_corr.index[upper_corr[col] >= HIGH_CORR_THRESHOLD].tolist()\n",
        "    if partners:\n",
        "        high_corr_features.add(col)\n",
        "        joined_partners = ','.join(partners)\n",
        "        log_feature_removal(col, 'high_correlation', notes=f'correlated_with={joined_partners}')\n",
        "\n",
        "# Outlier-dominated features (opzionale, threshold conservativo)\n",
        "outlier_flagged = outlier_ratio[outlier_ratio['outlier_ratio'] >= 0.75].index.tolist()\n",
        "for feature in outlier_flagged:\n",
        "    log_feature_removal(feature, 'extreme_outliers', float(outlier_ratio.loc[feature, 'outlier_ratio']))\n",
        "\n",
        "features_to_drop = sorted(set(constant_features) | set(low_variance.index) | set(low_magnitude.index) | high_corr_features | set(outlier_flagged))\n",
        "if features_to_drop:\n",
        "    print(\"Removing features:\", features_to_drop)\n",
        "    X_train_raw = X_train_raw.drop(columns=features_to_drop)\n",
        "    X_test_raw = X_test_raw.drop(columns=features_to_drop)\n",
        "    FEATURE_COLUMNS = [col for col in FEATURE_COLUMNS if col not in features_to_drop]\n",
        "\n",
        "    removal_df = pd.DataFrame(feature_audit_entries)\n",
        "    if not removal_df.empty:\n",
        "        save_report(removal_df, 'feature_removal_log.csv')\n",
        "else:\n",
        "    print('Nessuna feature rimossa con i criteri impostati.')\n",
        "    removal_df = pd.DataFrame(feature_audit_entries)\n",
        "    save_report(removal_df, 'feature_removal_log.csv')\n",
        "\n",
        "FULL_TIME_STEPS = X_train_raw['time'].nunique()\n",
        "WINDOW_SIZE = 25\n",
        "WINDOW_STRIDE = 15\n",
        "EVAL_WINDOW_SIZE = WINDOW_SIZE\n",
        "EVAL_WINDOW_STRIDE = WINDOW_STRIDE\n",
        "EVAL_AGGREGATION = 'logsumexp'\n",
        "if WINDOW_SIZE > FULL_TIME_STEPS:\n",
        "    raise ValueError(\n",
        "        f'Requested window size ({WINDOW_SIZE}) exceeds the available time steps ({FULL_TIME_STEPS}).'\n",
        "    )\n",
        "TIME_STEPS = FULL_TIME_STEPS\n",
        "\n",
        "TIME_DIVISOR = max(FULL_TIME_STEPS - 1, 1)\n",
        "TIME_FEATURE_COLUMNS = ['time_fraction', 'time_sin', 'time_cos', 'time_is_start', 'time_is_end']\n",
        "for df in (X_train_raw, X_test_raw):\n",
        "    frac = (df['time'] / TIME_DIVISOR).astype(np.float32)\n",
        "    df['time_fraction'] = frac\n",
        "    df['time_sin'] = np.sin(2 * np.pi * frac).astype(np.float32)\n",
        "    df['time_cos'] = np.cos(2 * np.pi * frac).astype(np.float32)\n",
        "    df['time_is_start'] = (df['time'] == 0).astype(np.float32)\n",
        "    df['time_is_end'] = (df['time'] == FULL_TIME_STEPS - 1).astype(np.float32)\n",
        "\n",
        "for col in TIME_FEATURE_COLUMNS:\n",
        "    if col not in FEATURE_COLUMNS:\n",
        "        FEATURE_COLUMNS.append(col)\n",
        "\n",
        "TEMPORAL_WINDOW_SIZES = (5, 15)\n",
        "VALUE_FEATURE_COLUMNS = [col for col in FEATURE_COLUMNS if col not in (TIME_FEATURE_COLUMNS + CATEGORICAL_COLUMNS)]\n",
        "\n",
        "def add_temporal_statistics(df: pd.DataFrame):\n",
        "    if not VALUE_FEATURE_COLUMNS:\n",
        "        return\n",
        "    df_signal = df[VALUE_FEATURE_COLUMNS].abs().mean(axis=1)\n",
        "    df['signal_abs_mean'] = df_signal\n",
        "    if 'signal_abs_mean' not in FEATURE_COLUMNS:\n",
        "        FEATURE_COLUMNS.append('signal_abs_mean')\n",
        "    for window in TEMPORAL_WINDOW_SIZES:\n",
        "        rolling_mean = (\n",
        "            df.groupby('sample_index')['signal_abs_mean']\n",
        "              .transform(lambda x: x.rolling(window, min_periods=1, center=True).mean())\n",
        "        )\n",
        "        rolling_std = (\n",
        "            df.groupby('sample_index')['signal_abs_mean']\n",
        "              .transform(lambda x: x.rolling(window, min_periods=1, center=True).std().fillna(0.0))\n",
        "        )\n",
        "        df[f'signal_mean_w{window}'] = rolling_mean\n",
        "        df[f'signal_std_w{window}'] = rolling_std\n",
        "        for feature in (f'signal_mean_w{window}', f'signal_std_w{window}'):\n",
        "            if feature not in FEATURE_COLUMNS:\n",
        "                FEATURE_COLUMNS.append(feature)\n",
        "\n",
        "for dataset in (X_train_raw, X_test_raw):\n",
        "    add_temporal_statistics(dataset)\n",
        "\n",
        "NUM_FEATURES = len(FEATURE_COLUMNS)\n",
        "NUM_CLASSES = y_train['label'].nunique()\n",
        "print(\n",
        "    f'Time steps (raw): {FULL_TIME_STEPS} | Window size: {WINDOW_SIZE} | Features: {NUM_FEATURES} | Classes: {NUM_CLASSES}'\n",
        ")\n",
        "print('Category mappings:', CATEGORY_MAPPINGS)\n",
        "\n",
        "y_train['label'].value_counts().plot(kind='bar', title='Class distribution')\n",
        "plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kB_x6eF6QLiJ",
        "outputId": "f9b856cb-3caa-4a32-e173-2ebbe8ec2357",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Feature pain_survey_1: miglior lag ‚âà 6 con autocorr 0.008\n"
          ]
        }
      ],
      "source": [
        "# -- Analisi autocorrelazione per suggerire window size ----------------------\n",
        "def compute_autocorr_for_feature(feature: str, max_lag: int = 60) -> Tuple[List[int], List[float]]:\n",
        "    pivot = (\n",
        "        X_train_raw\n",
        "        .pivot(index='sample_index', columns='time', values=feature)\n",
        "        .sort_index(axis=1)\n",
        "    )\n",
        "    lags = list(range(1, max_lag + 1))\n",
        "    autocorr_values: List[float] = []\n",
        "    for lag in lags:\n",
        "        ac_series = pivot.apply(lambda row: row.autocorr(lag=lag), axis=1).dropna()\n",
        "        autocorr_values.append(ac_series.mean() if not ac_series.empty else 0.0)\n",
        "    return lags, autocorr_values\n",
        "\n",
        "AUTO_CORR_FEATURES = FEATURE_COLUMNS[:min(3, len(FEATURE_COLUMNS))]\n",
        "MAX_AUTOCORR_LAG = 60\n",
        "plt.figure(figsize=(10, 6))\n",
        "for feature in AUTO_CORR_FEATURES:\n",
        "    lags, autocorr_vals = compute_autocorr_for_feature(feature, MAX_AUTOCORR_LAG)\n",
        "    plt.plot(lags, autocorr_vals, label=feature)\n",
        "    top_lag = lags[int(np.argmax(autocorr_vals))]\n",
        "    print(f\"Feature {feature}: miglior lag ‚âà {top_lag} con autocorr {max(autocorr_vals):.3f}\")\n",
        "\n",
        "plt.title('Autocorrelazione media per feature selezionate')\n",
        "plt.xlabel('Lag')\n",
        "plt.ylabel('Autocorrelation')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NknJpSLWQLiJ"
      },
      "outputs": [],
      "source": [
        "# -- Sanity checks per garantire coerenza pipeline ---------------------------\n",
        "def run_quick_sanity_checks():\n",
        "    train_cols = set(X_train_raw.columns)\n",
        "    test_cols = set(X_test_raw.columns)\n",
        "    assert train_cols == test_cols, 'Train/Test hanno colonne diverse'\n",
        "    assert len(FEATURE_COLUMNS) == len(set(FEATURE_COLUMNS)), 'FEATURE_COLUMNS contiene duplicati'\n",
        "    assert all(col in train_cols for col in FEATURE_COLUMNS), 'Colonne feature non presenti nel dataset'\n",
        "    assert NUM_FEATURES == len(FEATURE_COLUMNS), 'NUM_FEATURES non allineato'\n",
        "    print('Sanity checks superati: dataset e feature set coerenti.')\n",
        "\n",
        "run_quick_sanity_checks()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m7ieHeHyTBGU"
      },
      "outputs": [],
      "source": [
        "LABEL2IDX = {label: idx for idx, label in enumerate(sorted(y_train['label'].unique()))}\n",
        "IDX2LABEL = {idx: label for label, idx in LABEL2IDX.items()}\n",
        "print('Label mapping:', LABEL2IDX)\n",
        "\n",
        "HIGH_PAIN_LABELS = [label for label in LABEL2IDX if 'high' in label.lower()]\n",
        "LOW_PAIN_LABELS = [label for label in LABEL2IDX if 'low' in label.lower()]\n",
        "NO_PAIN_LABELS = [label for label in LABEL2IDX if 'no' in label.lower()]\n",
        "HIGH_PAIN_IDX = {LABEL2IDX[label] for label in HIGH_PAIN_LABELS}\n",
        "LOW_PAIN_IDX = {LABEL2IDX[label] for label in LOW_PAIN_LABELS}\n",
        "NO_PAIN_IDX = {LABEL2IDX[label] for label in NO_PAIN_LABELS}\n",
        "print('High-pain classes for augmentation:', HIGH_PAIN_LABELS or 'None')\n",
        "print('Low-pain classes for augmentation:', LOW_PAIN_LABELS or 'None')\n",
        "print('No-pain classes for augmentation:', NO_PAIN_IDX or 'None')\n",
        "\n",
        "HIGH_PAIN_OVERSAMPLE = 2 if HIGH_PAIN_IDX else 1\n",
        "LOW_PAIN_OVERSAMPLE = 2 if LOW_PAIN_IDX else 1\n",
        "HIGH_PAIN_WEIGHT_SCALE = 0.25\n",
        "LOW_PAIN_WEIGHT_SCALE = 0.45\n",
        "NO_PAIN_WEIGHT_SCALE = 1.9\n",
        "HIGH_PAIN_AUGMENTATION_PARAMS = {\n",
        "    'jitter_std': 0.03,\n",
        "    'scale_range': (0.90, 1.10),\n",
        "    'time_mask_prob': 0.30,\n",
        "    'time_mask_ratio': 0.12,\n",
        "    'time_shift_range': 4,\n",
        "    'time_flip_prob': 0.10,\n",
        "}\n",
        "LOW_PAIN_AUGMENTATION_PARAMS = {\n",
        "    'jitter_std': 0.02,\n",
        "    'scale_range': (0.95, 1.05),\n",
        "    'time_mask_prob': 0.20,\n",
        "    'time_mask_ratio': 0.08,\n",
        "    'time_shift_range': 3,\n",
        "    'time_flip_prob': 0.05,\n",
        "}\n",
        "AUGMENTATION_PARAMS = HIGH_PAIN_AUGMENTATION_PARAMS\n",
        "\n",
        "\n",
        "def pivot_timeseries(df: pd.DataFrame) -> np.ndarray:\n",
        "    pivoted = (\n",
        "        df.pivot(index='sample_index', columns='time', values=FEATURE_COLUMNS)\n",
        "          .sort_index(axis=0)\n",
        "          .sort_index(axis=1, level=1)\n",
        "    )\n",
        "    data = pivoted.to_numpy().reshape(-1, TIME_STEPS, NUM_FEATURES)\n",
        "    return data\n",
        "\n",
        "\n",
        "X_train_np = pivot_timeseries(X_train_raw)\n",
        "X_test_np = pivot_timeseries(X_test_raw)\n",
        "y_train_idx = y_train.set_index('sample_index').loc[pd.unique(X_train_raw['sample_index'])]['label'].map(LABEL2IDX).to_numpy()\n",
        "\n",
        "print(X_train_np.shape, y_train_idx.shape, X_test_np.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "utrbBxCPTBGU"
      },
      "outputs": [],
      "source": [
        "class_counts = np.bincount(y_train_idx, minlength=NUM_CLASSES)\n",
        "class_weights = class_counts.sum() / (class_counts + 1e-6)\n",
        "class_weights = class_weights / class_weights.mean()\n",
        "if HIGH_PAIN_IDX:\n",
        "    for idx in HIGH_PAIN_IDX:\n",
        "        class_weights[idx] *= HIGH_PAIN_WEIGHT_SCALE\n",
        "if LOW_PAIN_IDX:\n",
        "    for idx in LOW_PAIN_IDX:\n",
        "        class_weights[idx] *= LOW_PAIN_WEIGHT_SCALE\n",
        "if NO_PAIN_IDX:\n",
        "    for idx in NO_PAIN_IDX:\n",
        "        class_weights[idx] *= NO_PAIN_WEIGHT_SCALE\n",
        "CLASS_COUNTS = class_counts\n",
        "CLASS_WEIGHTS = torch.tensor(class_weights, dtype=torch.float32)\n",
        "print('Class counts:', class_counts)\n",
        "print('Class weights (normalized):', np.round(class_weights, 3))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ufAfwBqxTBGV"
      },
      "outputs": [],
      "source": [
        "def compute_normalization_stats(data: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:\n",
        "    # data shape: (N, T, F)\n",
        "    mean = data.reshape(-1, NUM_FEATURES).mean(axis=0)\n",
        "    std = data.reshape(-1, NUM_FEATURES).std(axis=0) + 1e-6\n",
        "    return mean, std\n",
        "\n",
        "\n",
        "def normalize(data: np.ndarray, mean: np.ndarray, std: np.ndarray) -> np.ndarray:\n",
        "    return (data - mean) / std\n",
        "\n",
        "\n",
        "feat_mean, feat_std = compute_normalization_stats(X_train_np)\n",
        "X_train_np = normalize(X_train_np, feat_mean, feat_std)\n",
        "X_test_np = normalize(X_test_np, feat_mean, feat_std)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cPxW7DukTBGV"
      },
      "outputs": [],
      "source": [
        "def make_dataloader_from_arrays(\n",
        "    X: np.ndarray,\n",
        "    y: Optional[np.ndarray],\n",
        "    batch_size: int,\n",
        "    shuffle: bool,\n",
        "    mode: str,\n",
        "    oversample_factor: int = 1,\n",
        "    augmentation_params: Optional[Dict[str, float]] = None,\n",
        "    window_size: Optional[int] = None,\n",
        "    window_stride: Optional[int] = None,\n",
        ") -> DataLoader:\n",
        "    dataset = TimeSeriesDataset(\n",
        "        X,\n",
        "        y,\n",
        "        window_size=window_size,\n",
        "        window_stride=window_stride,\n",
        "        mode=mode,\n",
        "        high_pain_targets=HIGH_PAIN_IDX,\n",
        "        oversample_factor=oversample_factor,\n",
        "        augmentation_params=augmentation_params,\n",
        "    )\n",
        "    return DataLoader(dataset, batch_size=batch_size, shuffle=shuffle, drop_last=False)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ruF9R0TzTBGV"
      },
      "outputs": [],
      "source": [
        "class TimeSeriesDataset(Dataset):\n",
        "    def __init__(\n",
        "        self,\n",
        "        data: np.ndarray,\n",
        "        labels: Optional[np.ndarray] = None,\n",
        "        *,\n",
        "        window_size: Optional[int] = None,\n",
        "        window_stride: Optional[int] = None,\n",
        "        mode: str = 'train',\n",
        "        high_pain_targets: Optional[set] = None,\n",
        "        oversample_factor: int = 1,\n",
        "        augmentation_params: Optional[Dict[str, float]] = None,\n",
        "    ):\n",
        "        self.data = torch.tensor(data, dtype=torch.float32)\n",
        "        self.labels = None if labels is None else torch.tensor(labels, dtype=torch.long)\n",
        "        self.window_size = window_size\n",
        "        self.window_stride = max(1, window_stride or 1)\n",
        "        self.mode = mode\n",
        "        self.high_pain_targets = set(high_pain_targets or [])\n",
        "        self.augmentation_params = augmentation_params or {}\n",
        "        self.time_steps = self.data.shape[1]\n",
        "\n",
        "        if mode not in {'train', 'valid', 'test'}:\n",
        "            raise ValueError(f\"Unsupported mode '{mode}'. Use 'train', 'valid', or 'test'.\")\n",
        "        if self.window_size is not None and self.window_size > self.time_steps:\n",
        "            raise ValueError(\n",
        "                f'Window size {self.window_size} exceeds series length {self.time_steps}.'\n",
        "            )\n",
        "\n",
        "        self.indices = self._build_indices(max(1, oversample_factor))\n",
        "\n",
        "    def _build_indices(self, oversample_factor: int) -> List[int]:\n",
        "        base_indices = list(range(len(self.data)))\n",
        "        if (\n",
        "            self.labels is None\n",
        "            or not self.high_pain_targets\n",
        "            or oversample_factor <= 1\n",
        "        ):\n",
        "            return base_indices\n",
        "\n",
        "        high_indices = [idx for idx in base_indices if int(self.labels[idx].item()) in self.high_pain_targets]\n",
        "        if not high_indices:\n",
        "            return base_indices\n",
        "\n",
        "        extra_indices: List[int] = []\n",
        "        for _ in range(oversample_factor - 1):\n",
        "            extra_indices.extend(high_indices)\n",
        "        return base_indices + extra_indices\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        return len(self.indices)\n",
        "\n",
        "    def __getitem__(self, idx: int):\n",
        "        real_idx = self.indices[idx]\n",
        "        series = self.data[real_idx]\n",
        "        label = None if self.labels is None else self.labels[real_idx]\n",
        "\n",
        "        if self.window_size is not None and self.mode == 'train':\n",
        "            series = self._select_window(series, random_selection=True)\n",
        "        elif self.window_size is not None and self.mode != 'train':\n",
        "            series = self._select_window(series, random_selection=False)\n",
        "        else:\n",
        "            series = series.clone()\n",
        "\n",
        "        if (\n",
        "            self.mode == 'train'\n",
        "            and label is not None\n",
        "            and int(label.item()) in self.high_pain_targets\n",
        "            and self.augmentation_params\n",
        "        ):\n",
        "            series = self._augment(series)\n",
        "\n",
        "        if label is None:\n",
        "            return series\n",
        "        return series, label\n",
        "\n",
        "    def _select_window(self, series: torch.Tensor, *, random_selection: bool) -> torch.Tensor:\n",
        "        window = self.window_size or series.shape[0]\n",
        "        if window >= series.shape[0]:\n",
        "            return series.clone()\n",
        "\n",
        "        max_start = series.shape[0] - window\n",
        "        if max_start <= 0:\n",
        "            start = 0\n",
        "        else:\n",
        "            stride = max(1, self.window_stride)\n",
        "            positions = list(range(0, max_start + 1, stride))\n",
        "            if positions[-1] != max_start:\n",
        "                positions.append(max_start)\n",
        "            if random_selection:\n",
        "                start = random.choice(positions)\n",
        "            else:\n",
        "                start = positions[len(positions) // 2]\n",
        "        end = start + window\n",
        "        return series[start:end].clone()\n",
        "\n",
        "    def _augment(self, series: torch.Tensor) -> torch.Tensor:\n",
        "        augmented = series.clone()\n",
        "        jitter_std = float(self.augmentation_params.get('jitter_std', 0.0))\n",
        "        if jitter_std > 0:\n",
        "            augmented = augmented + torch.randn_like(augmented) * jitter_std\n",
        "\n",
        "        scale_range = self.augmentation_params.get('scale_range')\n",
        "        if scale_range:\n",
        "            low, high = scale_range\n",
        "            scale = random.uniform(low, high)\n",
        "            augmented = augmented * scale\n",
        "\n",
        "        time_shift_range = int(self.augmentation_params.get('time_shift_range', 0))\n",
        "        if time_shift_range > 0:\n",
        "            shift = random.randint(-time_shift_range, time_shift_range)\n",
        "            if shift != 0:\n",
        "                augmented = torch.roll(augmented, shifts=shift, dims=0)\n",
        "\n",
        "        if random.random() < float(self.augmentation_params.get('time_flip_prob', 0.0)):\n",
        "            augmented = torch.flip(augmented, dims=[0])\n",
        "\n",
        "        time_mask_prob = float(self.augmentation_params.get('time_mask_prob', 0.0))\n",
        "        time_mask_ratio = float(self.augmentation_params.get('time_mask_ratio', 0.0))\n",
        "        if time_mask_prob > 0 and time_mask_ratio > 0 and random.random() < time_mask_prob:\n",
        "            mask_len = max(1, int(augmented.shape[0] * time_mask_ratio))\n",
        "            if mask_len < augmented.shape[0]:\n",
        "                start = random.randint(0, augmented.shape[0] - mask_len)\n",
        "                augmented[start:start + mask_len] = 0.0\n",
        "\n",
        "        return augmented\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sez_5vaUTBGW"
      },
      "outputs": [],
      "source": [
        "class FocalLoss(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        weight: Optional[torch.Tensor] = None,\n",
        "        gamma: float = 2.0,\n",
        "        reduction: str = 'mean',\n",
        "    ):\n",
        "        super().__init__()\n",
        "        if reduction not in {'none', 'mean', 'sum'}:\n",
        "            raise ValueError(f\"Unsupported reduction '{reduction}'.\")\n",
        "        if weight is not None:\n",
        "            self.register_buffer('weight', weight)\n",
        "            self._use_weight = True\n",
        "        else:\n",
        "            self.register_buffer('weight', torch.tensor([], dtype=torch.float32), persistent=False)\n",
        "            self._use_weight = False\n",
        "        self.gamma = gamma\n",
        "        self.reduction = reduction\n",
        "\n",
        "    def forward(self, logits: torch.Tensor, targets: torch.Tensor) -> torch.Tensor:\n",
        "        if logits.shape[0] != targets.shape[0]:\n",
        "            raise ValueError('Logits and targets must have matching batch dimension.')\n",
        "        targets = targets.long().unsqueeze(1)\n",
        "        log_probs = F.log_softmax(logits, dim=-1)\n",
        "        probs = log_probs.exp()\n",
        "        log_pt = log_probs.gather(1, targets)\n",
        "        pt = probs.gather(1, targets).clamp(min=1e-6, max=1.0)\n",
        "        focal_factor = (1.0 - pt) ** self.gamma\n",
        "        loss = -focal_factor * log_pt\n",
        "        if self._use_weight:\n",
        "            class_weights = self.weight[targets.squeeze(1)]\n",
        "            loss = loss * class_weights.unsqueeze(1)\n",
        "        loss = loss.squeeze(1)\n",
        "        if self.reduction == 'mean':\n",
        "            return loss.mean()\n",
        "        if self.reduction == 'sum':\n",
        "            return loss.sum()\n",
        "        return loss\n",
        "\n",
        "\n",
        "def build_loss_fn(config: Dict) -> nn.Module:\n",
        "    loss_type = config.get('loss_type', 'focal')\n",
        "    use_class_weights = config.get('use_class_weights', True)\n",
        "    weight_tensor = None\n",
        "    if use_class_weights and 'CLASS_WEIGHTS' in globals():\n",
        "        weight_tensor = CLASS_WEIGHTS.to(DEVICE)\n",
        "    if loss_type == 'focal':\n",
        "        gamma = config.get('focal_gamma', 2.0)\n",
        "        return FocalLoss(weight=weight_tensor, gamma=gamma)\n",
        "    if loss_type == 'weighted_ce':\n",
        "        return nn.CrossEntropyLoss(weight=weight_tensor)\n",
        "    if loss_type == 'cross_entropy':\n",
        "        return nn.CrossEntropyLoss()\n",
        "    raise ValueError(f\"Unsupported loss_type '{loss_type}'.\")\n",
        "\n",
        "\n",
        "def extract_sliding_windows(\n",
        "    series: torch.Tensor,\n",
        "    window_size: Optional[int],\n",
        "    stride: Optional[int],\n",
        ") -> torch.Tensor:\n",
        "    if window_size is None or window_size >= series.shape[0]:\n",
        "        return series.unsqueeze(0)\n",
        "    stride = max(1, stride or 1)\n",
        "    length = series.shape[0]\n",
        "    max_start = max(0, length - window_size)\n",
        "    positions = list(range(0, max_start + 1, stride))\n",
        "    if positions[-1] != max_start:\n",
        "        positions.append(max_start)\n",
        "    windows = [series[start:start + window_size] for start in positions]\n",
        "    return torch.stack(windows, dim=0)\n",
        "\n",
        "\n",
        "def forward_with_sliding_windows(\n",
        "    model: nn.Module,\n",
        "    inputs: torch.Tensor,\n",
        "    window_size: Optional[int],\n",
        "    stride: Optional[int],\n",
        "    aggregation: str = 'max',\n",
        ") -> torch.Tensor:\n",
        "    if window_size is None or window_size >= inputs.shape[1]:\n",
        "        return model(inputs)\n",
        "\n",
        "    all_windows: List[torch.Tensor] = []\n",
        "    window_counts: List[int] = []\n",
        "    for sample in inputs:\n",
        "        sample_windows = extract_sliding_windows(sample, window_size, stride)\n",
        "        all_windows.append(sample_windows)\n",
        "        window_counts.append(sample_windows.shape[0])\n",
        "\n",
        "    stacked_windows = torch.cat(all_windows, dim=0)\n",
        "    logits = model(stacked_windows)\n",
        "    chunks = logits.split(window_counts, dim=0)\n",
        "\n",
        "    aggregated_logits: List[torch.Tensor] = []\n",
        "    aggregation = aggregation.lower()\n",
        "    for chunk in chunks:\n",
        "        if aggregation == 'max':\n",
        "            aggregated_logits.append(chunk.max(dim=0).values)\n",
        "        elif aggregation == 'mean':\n",
        "            aggregated_logits.append(chunk.mean(dim=0))\n",
        "        elif aggregation == 'logsumexp':\n",
        "            aggregated_logits.append(torch.logsumexp(chunk, dim=0))\n",
        "        else:\n",
        "            raise ValueError(f\"Unsupported aggregation '{aggregation}'.\")\n",
        "\n",
        "    return torch.stack(aggregated_logits, dim=0)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hHmie09aTBGW"
      },
      "outputs": [],
      "source": [
        "def create_dataloaders(\n",
        "    X: np.ndarray,\n",
        "    y: np.ndarray,\n",
        "    valid_size: float = 0.2,\n",
        "    batch_size: int = 128,\n",
        "    oversample_factor: int = HIGH_PAIN_OVERSAMPLE,\n",
        "    window_size: Optional[int] = WINDOW_SIZE,\n",
        "    window_stride: Optional[int] = WINDOW_STRIDE,\n",
        "    augmentation_params: Optional[Dict[str, float]] = AUGMENTATION_PARAMS,\n",
        "):\n",
        "    X_train, X_valid, y_train, y_valid = train_test_split(\n",
        "        X, y, test_size=valid_size, random_state=SEED, stratify=y\n",
        "    )\n",
        "\n",
        "    train_loader = make_dataloader_from_arrays(\n",
        "        X_train,\n",
        "        y_train,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=True,\n",
        "        mode='train',\n",
        "        oversample_factor=oversample_factor,\n",
        "        augmentation_params=augmentation_params,\n",
        "        window_size=window_size,\n",
        "        window_stride=window_stride,\n",
        "    )\n",
        "    valid_loader = make_dataloader_from_arrays(\n",
        "        X_valid,\n",
        "        y_valid,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=False,\n",
        "        mode='valid',\n",
        "        window_size=None,\n",
        "        window_stride=None,\n",
        "    )\n",
        "    return train_loader, valid_loader, (X_train, y_train, X_valid, y_valid)\n",
        "\n",
        "\n",
        "BATCH_SIZE = 128\n",
        "train_loader, valid_loader, (X_train_split, y_train_split, X_valid_split, y_valid_split) = create_dataloaders(\n",
        "    X_train_np,\n",
        "    y_train_idx,\n",
        "    batch_size=BATCH_SIZE,\n",
        ")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EVRDa4oMTBGW"
      },
      "outputs": [],
      "source": [
        "class RecurrentBackbone(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        input_size: int,\n",
        "        hidden_size: int = 128,\n",
        "        num_layers: int = 2,\n",
        "        dropout: float = 0.2,\n",
        "        rnn_type: str = 'lstm',\n",
        "        bidirectional: bool = True,\n",
        "        conv_layers: int = 0,\n",
        "        conv_channels: Optional[int] = None,\n",
        "        conv_kernel_size: int = 3,\n",
        "        conv_dropout: float = 0.1,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        if conv_kernel_size % 2 == 0:\n",
        "            raise ValueError('conv_kernel_size should be odd to preserve temporal dimension.')\n",
        "        rnn_cls = {\n",
        "            'rnn': nn.RNN,\n",
        "            'gru': nn.GRU,\n",
        "            'lstm': nn.LSTM,\n",
        "        }[rnn_type.lower()]\n",
        "        self.rnn_type = rnn_type.lower()\n",
        "        self.bidirectional = bidirectional\n",
        "\n",
        "        self.conv_layers = conv_layers\n",
        "        self.conv = None\n",
        "        rnn_input_size = input_size\n",
        "        if conv_layers > 0:\n",
        "            conv_blocks: List[nn.Module] = []\n",
        "            in_channels = input_size\n",
        "            out_channels = conv_channels or input_size\n",
        "            for layer_idx in range(conv_layers):\n",
        "                conv_blocks.append(\n",
        "                    nn.Conv1d(\n",
        "                        in_channels,\n",
        "                        out_channels,\n",
        "                        kernel_size=conv_kernel_size,\n",
        "                        padding=conv_kernel_size // 2,\n",
        "                        bias=False,\n",
        "                    )\n",
        "                )\n",
        "                conv_blocks.append(nn.BatchNorm1d(out_channels))\n",
        "                conv_blocks.append(nn.ReLU())\n",
        "                if conv_dropout > 0:\n",
        "                    conv_blocks.append(nn.Dropout(conv_dropout))\n",
        "                in_channels = out_channels\n",
        "            self.conv = nn.Sequential(*conv_blocks)\n",
        "            rnn_input_size = out_channels\n",
        "\n",
        "        self.rnn = rnn_cls(\n",
        "            input_size=rnn_input_size,\n",
        "            hidden_size=hidden_size,\n",
        "            num_layers=num_layers,\n",
        "            dropout=dropout if num_layers > 1 else 0.0,\n",
        "            batch_first=True,\n",
        "            bidirectional=bidirectional,\n",
        "        )\n",
        "        proj_input = hidden_size * (2 if bidirectional else 1)\n",
        "        self.head = nn.Sequential(\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(proj_input, hidden_size),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(hidden_size, NUM_CLASSES),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        if self.conv is not None:\n",
        "            # (batch, time, feature) -> (batch, feature, time)\n",
        "            x = x.transpose(1, 2)\n",
        "            x = self.conv(x)\n",
        "            x = x.transpose(1, 2)\n",
        "        out, _ = self.rnn(x)\n",
        "        # use last time-step hidden state\n",
        "        last = out[:, -1, :]\n",
        "        return self.head(last)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uFlPWsSPTBGX"
      },
      "outputs": [],
      "source": [
        "def compute_classification_metrics(preds: np.ndarray, targets: np.ndarray) -> Dict[str, float]:\n",
        "    accuracy = float((preds == targets).mean())\n",
        "    precision, recall, f1, _ = precision_recall_fscore_support(\n",
        "        targets,\n",
        "        preds,\n",
        "        average='macro',\n",
        "        zero_division=0,\n",
        "    )\n",
        "    return {\n",
        "        'accuracy': accuracy,\n",
        "        'precision': float(precision),\n",
        "        'recall': float(recall),\n",
        "        'f1': float(f1),\n",
        "    }\n",
        "\n",
        "\n",
        "def train_one_epoch(\n",
        "    model: nn.Module,\n",
        "    loader: DataLoader,\n",
        "    criterion: nn.Module,\n",
        "    optimizer: torch.optim.Optimizer,\n",
        "    scaler: GradScaler,\n",
        "    max_grad_norm: float = 5.0,\n",
        ") -> Tuple[float, Dict[str, float]]:\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    preds_all, targets_all = [], []\n",
        "\n",
        "    for inputs, targets in loader:\n",
        "        inputs = inputs.to(DEVICE)\n",
        "        targets = targets.to(DEVICE)\n",
        "        optimizer.zero_grad(set_to_none=True)\n",
        "\n",
        "        with autocast_context():\n",
        "            logits = model(inputs)\n",
        "            loss = criterion(logits, targets)\n",
        "\n",
        "        scaler.scale(loss).backward()\n",
        "        if max_grad_norm is not None:\n",
        "            scaler.unscale_(optimizer)\n",
        "            nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "\n",
        "        running_loss += loss.item() * inputs.size(0)\n",
        "        preds_all.append(torch.argmax(logits.detach(), dim=1).cpu())\n",
        "        targets_all.append(targets.cpu())\n",
        "\n",
        "    preds_np = torch.cat(preds_all).numpy()\n",
        "    targets_np = torch.cat(targets_all).numpy()\n",
        "    metrics = compute_classification_metrics(preds_np, targets_np)\n",
        "    avg_loss = running_loss / len(loader.dataset)\n",
        "    return avg_loss, metrics\n",
        "\n",
        "\n",
        "def evaluate_epoch(\n",
        "    model: nn.Module,\n",
        "    loader: DataLoader,\n",
        "    criterion: nn.Module,\n",
        "    eval_window_size: Optional[int],\n",
        "    eval_window_stride: Optional[int],\n",
        "    aggregation: str,\n",
        ") -> Tuple[float, Dict[str, float], np.ndarray, np.ndarray]:\n",
        "    model.eval()\n",
        "    running_loss = 0.0\n",
        "    preds_all, targets_all = [], []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, targets in loader:\n",
        "            inputs = inputs.to(DEVICE)\n",
        "            targets = targets.to(DEVICE)\n",
        "            with autocast_context():\n",
        "                logits = forward_with_sliding_windows(\n",
        "                    model,\n",
        "                    inputs,\n",
        "                    eval_window_size,\n",
        "                    eval_window_stride,\n",
        "                    aggregation,\n",
        "                )\n",
        "                loss = criterion(logits, targets)\n",
        "\n",
        "            running_loss += loss.item() * targets.size(0)\n",
        "            preds_all.append(torch.argmax(logits, dim=1).cpu())\n",
        "            targets_all.append(targets.cpu())\n",
        "\n",
        "    preds_np = torch.cat(preds_all).numpy()\n",
        "    targets_np = torch.cat(targets_all).numpy()\n",
        "    metrics = compute_classification_metrics(preds_np, targets_np)\n",
        "    avg_loss = running_loss / len(loader.dataset)\n",
        "    return avg_loss, metrics, preds_np, targets_np\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i8CvXdcDTBGX"
      },
      "outputs": [],
      "source": [
        "def fit_model(\n",
        "    config: Dict,\n",
        "    train_loader: DataLoader,\n",
        "    valid_loader: DataLoader,\n",
        "    run_name: str,\n",
        "    tensorboard: bool = True,\n",
        ") -> Tuple[nn.Module, Dict[str, List[float]], Dict]:\n",
        "    model = RecurrentBackbone(\n",
        "        input_size=NUM_FEATURES,\n",
        "        hidden_size=config['hidden_size'],\n",
        "        num_layers=config['num_layers'],\n",
        "        dropout=config['dropout'],\n",
        "        rnn_type=config['rnn_type'],\n",
        "        bidirectional=config.get('bidirectional', False),\n",
        "        conv_layers=config.get('conv_layers', 0),\n",
        "        conv_channels=config.get('conv_channels'),\n",
        "        conv_kernel_size=config.get('conv_kernel_size', 3),\n",
        "        conv_dropout=config.get('conv_dropout', 0.1),\n",
        "    ).to(DEVICE)\n",
        "\n",
        "    criterion = build_loss_fn(config).to(DEVICE)\n",
        "    optimizer = torch.optim.AdamW(\n",
        "        model.parameters(),\n",
        "        lr=config['lr'],\n",
        "        weight_decay=config.get('weight_decay', 0.0),\n",
        "    )\n",
        "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
        "        optimizer,\n",
        "        mode='max',\n",
        "        factor=config.get('scheduler_factor', 0.5),\n",
        "        patience=config.get('scheduler_patience', 3),\n",
        "    )\n",
        "    scaler = create_grad_scaler()\n",
        "\n",
        "    history: Dict[str, List[float]] = {\n",
        "        'train_loss': [],\n",
        "        'train_accuracy': [],\n",
        "        'train_f1': [],\n",
        "        'train_precision': [],\n",
        "        'train_recall': [],\n",
        "        'valid_loss': [],\n",
        "        'valid_accuracy': [],\n",
        "        'valid_f1': [],\n",
        "        'valid_precision': [],\n",
        "        'valid_recall': [],\n",
        "        'lr': [],\n",
        "    }\n",
        "\n",
        "    run_log_dir = (LOG_DIR / run_name).resolve()\n",
        "    writer = SummaryWriter(run_log_dir.as_posix()) if tensorboard else None\n",
        "\n",
        "    best_metric = -np.inf\n",
        "    best_state: Optional[Dict] = None\n",
        "    patience = config.get('patience', 10)\n",
        "    patience_counter = 0\n",
        "    checkpoint_path = (CHECKPOINT_DIR / f'{run_name}.pt').resolve()\n",
        "\n",
        "    eval_window_size = config.get('eval_window_size', EVAL_WINDOW_SIZE)\n",
        "    eval_window_stride = config.get('eval_window_stride', EVAL_WINDOW_STRIDE)\n",
        "    eval_aggregation = config.get('eval_aggregation', EVAL_AGGREGATION)\n",
        "\n",
        "    for epoch in range(1, config['epochs'] + 1):\n",
        "        train_loss, train_metrics = train_one_epoch(\n",
        "            model,\n",
        "            train_loader,\n",
        "            criterion,\n",
        "            optimizer,\n",
        "            scaler,\n",
        "            max_grad_norm=config.get('max_grad_norm', 5.0),\n",
        "        )\n",
        "        valid_loss, valid_metrics, preds, targets = evaluate_epoch(\n",
        "            model,\n",
        "            valid_loader,\n",
        "            criterion,\n",
        "            eval_window_size,\n",
        "            eval_window_stride,\n",
        "            eval_aggregation,\n",
        "        )\n",
        "        scheduler.step(valid_metrics['f1'])\n",
        "\n",
        "        current_lr = optimizer.param_groups[0]['lr']\n",
        "\n",
        "        history['train_loss'].append(train_loss)\n",
        "        history['train_accuracy'].append(train_metrics['accuracy'])\n",
        "        history['train_f1'].append(train_metrics['f1'])\n",
        "        history['train_precision'].append(train_metrics['precision'])\n",
        "        history['train_recall'].append(train_metrics['recall'])\n",
        "        history['valid_loss'].append(valid_loss)\n",
        "        history['valid_accuracy'].append(valid_metrics['accuracy'])\n",
        "        history['valid_f1'].append(valid_metrics['f1'])\n",
        "        history['valid_precision'].append(valid_metrics['precision'])\n",
        "        history['valid_recall'].append(valid_metrics['recall'])\n",
        "        history['lr'].append(current_lr)\n",
        "\n",
        "        if writer is not None:\n",
        "            writer.add_scalar('Loss/train', train_loss, epoch)\n",
        "            writer.add_scalar('Loss/valid', valid_loss, epoch)\n",
        "            writer.add_scalar('F1/train', train_metrics['f1'], epoch)\n",
        "            writer.add_scalar('F1/valid', valid_metrics['f1'], epoch)\n",
        "            writer.add_scalar('Accuracy/train', train_metrics['accuracy'], epoch)\n",
        "            writer.add_scalar('Accuracy/valid', valid_metrics['accuracy'], epoch)\n",
        "            writer.add_scalar('LearningRate', current_lr, epoch)\n",
        "\n",
        "        msg = (\n",
        "            f\"Epoch {epoch:03d} | \"\n",
        "            f\"train_loss={train_loss:.4f} acc={train_metrics['accuracy']:.3f} f1={train_metrics['f1']:.3f} | \"\n",
        "            f\"valid_loss={valid_loss:.4f} acc={valid_metrics['accuracy']:.3f} f1={valid_metrics['f1']:.3f} | \"\n",
        "            f\"lr={current_lr:.2e}\"\n",
        "        )\n",
        "        print(msg)\n",
        "\n",
        "        if valid_metrics['f1'] > best_metric + config.get('min_improvement', 0.0):\n",
        "            best_metric = valid_metrics['f1']\n",
        "            patience_counter = 0\n",
        "            best_state = {\n",
        "                'epoch': epoch,\n",
        "                'model_state': copy.deepcopy(model.state_dict()),\n",
        "                'optimizer_state': copy.deepcopy(optimizer.state_dict()),\n",
        "                'metrics': valid_metrics,\n",
        "                'train_metrics': train_metrics,\n",
        "                'preds': preds,\n",
        "                'targets': targets,\n",
        "            }\n",
        "            torch.save(best_state['model_state'], checkpoint_path)\n",
        "        else:\n",
        "            patience_counter += 1\n",
        "            if patience_counter >= patience:\n",
        "                print(f\"Early stopping triggered at epoch {epoch}. Best f1={best_metric:.4f}.\")\n",
        "                break\n",
        "\n",
        "    if writer is not None:\n",
        "        writer.close()\n",
        "\n",
        "    if best_state is None:\n",
        "        best_state = {\n",
        "            'epoch': config['epochs'],\n",
        "            'model_state': copy.deepcopy(model.state_dict()),\n",
        "            'optimizer_state': copy.deepcopy(optimizer.state_dict()),\n",
        "            'metrics': valid_metrics,\n",
        "            'train_metrics': train_metrics,\n",
        "            'preds': preds,\n",
        "            'targets': targets,\n",
        "        }\n",
        "        torch.save(best_state['model_state'], checkpoint_path)\n",
        "\n",
        "    model.load_state_dict(best_state['model_state'])\n",
        "    best_state.update(\n",
        "        {\n",
        "            'run_name': run_name,\n",
        "            'config': copy.deepcopy(config),\n",
        "            'history': history,\n",
        "            'checkpoint_path': checkpoint_path,\n",
        "            'best_f1': best_metric,\n",
        "        }\n",
        "    )\n",
        "    return model, history, best_state\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ltbq2G3RTBGX"
      },
      "outputs": [],
      "source": [
        "def run_cross_validation(\n",
        "    config: Dict,\n",
        "    n_splits: int = 5,\n",
        "    shuffle: bool = True,\n",
        "    random_state: int = SEED,\n",
        ") -> List[Dict]:\n",
        "    skf = StratifiedKFold(n_splits=n_splits, shuffle=shuffle, random_state=random_state)\n",
        "    fold_results: List[Dict] = []\n",
        "\n",
        "    print(f\"\\n>>> Starting {n_splits}-fold CV for {config['run_name']} ({config['rnn_type'].upper()})\")\n",
        "    for fold_idx, (train_idx, valid_idx) in enumerate(skf.split(X_train_np, y_train_idx), start=1):\n",
        "        fold_config = copy.deepcopy(config)\n",
        "        fold_config['run_name'] = f\"{config['run_name']}_fold{fold_idx}\"\n",
        "\n",
        "        X_tr, y_tr = X_train_np[train_idx], y_train_idx[train_idx]\n",
        "        X_val, y_val = X_train_np[valid_idx], y_train_idx[valid_idx]\n",
        "\n",
        "        train_loader = make_dataloader_from_arrays(\n",
        "            X_tr,\n",
        "            y_tr,\n",
        "            batch_size=fold_config['batch_size'],\n",
        "            shuffle=True,\n",
        "            mode='train',\n",
        "            oversample_factor=fold_config.get('oversample_factor', HIGH_PAIN_OVERSAMPLE),\n",
        "            augmentation_params=fold_config.get('augmentation_params', AUGMENTATION_PARAMS),\n",
        "            window_size=fold_config.get('window_size', WINDOW_SIZE),\n",
        "            window_stride=fold_config.get('window_stride', WINDOW_STRIDE),\n",
        "        )\n",
        "        valid_loader = make_dataloader_from_arrays(\n",
        "            X_val,\n",
        "            y_val,\n",
        "            batch_size=fold_config['batch_size'],\n",
        "            shuffle=False,\n",
        "            mode='valid',\n",
        "            window_size=None,\n",
        "            window_stride=None,\n",
        "        )\n",
        "\n",
        "        model, history, best_state = fit_model(\n",
        "            fold_config,\n",
        "            train_loader,\n",
        "            valid_loader,\n",
        "            run_name=fold_config['run_name'],\n",
        "            tensorboard=fold_config.get('tensorboard', True),\n",
        "        )\n",
        "        best_state['fold'] = fold_idx\n",
        "        best_state['model'] = model\n",
        "        best_state['history'] = history\n",
        "        fold_results.append(best_state)\n",
        "\n",
        "        metrics = best_state['metrics']\n",
        "        print(\n",
        "            f\"Fold {fold_idx}/{n_splits} | best F1={metrics['f1']:.4f} | \"\n",
        "            f\"Accuracy={metrics['accuracy']:.4f}\"\n",
        "        )\n",
        "\n",
        "    return fold_results\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4MCuWUYeTBGX"
      },
      "outputs": [],
      "source": [
        "def summarize_cv_results(cv_results: List[Dict]) -> pd.DataFrame:\n",
        "    rows = []\n",
        "    for res in cv_results:\n",
        "        cfg = res['config']\n",
        "        metrics = res['metrics']\n",
        "        rows.append({\n",
        "            'run_name': res['run_name'],\n",
        "            'fold': res['fold'],\n",
        "            'rnn_type': cfg['rnn_type'],\n",
        "            'bidirectional': cfg.get('bidirectional', False),\n",
        "            'hidden_size': cfg['hidden_size'],\n",
        "            'num_layers': cfg['num_layers'],\n",
        "            'dropout': cfg['dropout'],\n",
        "            'f1': metrics['f1'],\n",
        "            'accuracy': metrics['accuracy'],\n",
        "            'precision': metrics['precision'],\n",
        "            'recall': metrics['recall'],\n",
        "            'checkpoint': str(res['checkpoint_path']),\n",
        "            'log_dir': str(LOG_DIR / res['run_name']),\n",
        "        })\n",
        "\n",
        "    df = pd.DataFrame(rows)\n",
        "    agg = df.groupby(['rnn_type', 'bidirectional'])[['f1', 'accuracy', 'precision', 'recall']].agg(['mean', 'std'])\n",
        "    agg.columns = ['_'.join(col) for col in agg.columns]\n",
        "    agg = agg.reset_index()\n",
        "    return df, agg\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "plM-wtoUTBGX"
      },
      "outputs": [],
      "source": [
        "def prepare_config(name: str, overrides: Dict) -> Dict:\n",
        "    base_config = {\n",
        "        'run_name': name,\n",
        "        'rnn_type': 'gru',\n",
        "        'hidden_size': 128,\n",
        "        'num_layers': 2,\n",
        "        'dropout': 0.4,\n",
        "        'bidirectional': True,\n",
        "        'lr': 2e-3,\n",
        "        'weight_decay': 1e-4,\n",
        "        'epochs': 200,\n",
        "        'batch_size': 64,\n",
        "        'valid_size': 0.1,\n",
        "        'patience': 20,\n",
        "        'max_grad_norm': 5.0,\n",
        "        'scheduler_factor': 0.5,\n",
        "        'scheduler_patience': 5,\n",
        "        'min_improvement': 5e-4,\n",
        "        'tensorboard': True,\n",
        "        'oversample_factor': HIGH_PAIN_OVERSAMPLE,\n",
        "        'window_size': WINDOW_SIZE,\n",
        "        'window_stride': WINDOW_STRIDE,\n",
        "        'eval_window_size': EVAL_WINDOW_SIZE,\n",
        "        'eval_window_stride': EVAL_WINDOW_STRIDE,\n",
        "        'eval_aggregation': EVAL_AGGREGATION,\n",
        "        'loss_type': 'focal',\n",
        "        'use_class_weights': True,\n",
        "        'focal_gamma': 0.75,\n",
        "        'augmentation_params': AUGMENTATION_PARAMS,\n",
        "        'conv_layers': 2,\n",
        "        'conv_channels': min(NUM_FEATURES, 128),\n",
        "        'conv_kernel_size': 5,\n",
        "        'conv_dropout': 0.1,\n",
        "    }\n",
        "    config = copy.deepcopy(base_config)\n",
        "    config.update(overrides)\n",
        "    return config\n",
        "\n",
        "\n",
        "def run_experiment(config: Dict) -> Dict:\n",
        "    run_name = config.get('run_name') or f\"{config['rnn_type'].upper()}_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
        "    config = copy.deepcopy(config)\n",
        "    config['run_name'] = run_name\n",
        "\n",
        "    train_loader, valid_loader, (X_tr, y_tr, X_val, y_val) = create_dataloaders(\n",
        "        X_train_np,\n",
        "        y_train_idx,\n",
        "        valid_size=config.get('valid_size', 0.2),\n",
        "        batch_size=config['batch_size'],\n",
        "        oversample_factor=config.get('oversample_factor', HIGH_PAIN_OVERSAMPLE),\n",
        "        window_size=config.get('window_size', WINDOW_SIZE),\n",
        "        window_stride=config.get('window_stride', WINDOW_STRIDE),\n",
        "        augmentation_params=config.get('augmentation_params', AUGMENTATION_PARAMS),\n",
        "    )\n",
        "\n",
        "    model, history, best_state = fit_model(\n",
        "        config,\n",
        "        train_loader,\n",
        "        valid_loader,\n",
        "        run_name=run_name,\n",
        "        tensorboard=config.get('tensorboard', True),\n",
        "    )\n",
        "\n",
        "    best_state['data_split'] = {\n",
        "        'X_train': X_tr,\n",
        "        'y_train': y_tr,\n",
        "        'X_valid': X_val,\n",
        "        'y_valid': y_val,\n",
        "    }\n",
        "    best_state['model'] = model\n",
        "    best_state['history'] = history\n",
        "    return best_state\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rCrkiQqCTBGY"
      },
      "outputs": [],
      "source": [
        "EXPERIMENT_CONFIGS = [\n",
        "\n",
        "    prepare_config('GRU_BI', {'rnn_type': 'gru', 'bidirectional': True}),\n",
        "    prepare_config('GRU_SINGLE', {'rnn_type': 'gru', 'bidirectional': False}),\n",
        "\n",
        "]\n",
        "\n",
        "experiment_results: List[Dict] = []\n",
        "for cfg in EXPERIMENT_CONFIGS:\n",
        "    print(f\"\\n=== Running experiment: {cfg['run_name']} ({cfg['rnn_type'].upper()} - {'BI' if cfg['bidirectional'] else 'UNI'}) ===\")\n",
        "    result = run_experiment(cfg)\n",
        "    experiment_results.append(result)\n",
        "    print(\n",
        "        f\"Best validation F1: {result['best_f1']:.4f} at epoch {result['epoch']} | \"\n",
        "        f\"Accuracy: {result['metrics']['accuracy']:.4f}\"\n",
        "    )\n",
        "\n",
        "BEST_FOR_CV = prepare_config('GRU_BI', {'bidirectional': True})\n",
        "cv_results = run_cross_validation(BEST_FOR_CV, n_splits=5)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aE9xeS7dTBGY"
      },
      "outputs": [],
      "source": [
        "def gather_results(source_names: List[str]) -> List[Dict]:\n",
        "    collected: List[Dict] = []\n",
        "    for name in source_names:\n",
        "        if name in globals():\n",
        "            value = globals()[name]\n",
        "            if isinstance(value, list) and value:\n",
        "                collected.extend(value)\n",
        "    return collected\n",
        "\n",
        "\n",
        "def build_summary_table(result_sources: List[str]) -> Tuple[pd.DataFrame, List[Dict]]:\n",
        "    collected_results = gather_results(result_sources)\n",
        "    if not collected_results:\n",
        "        raise RuntimeError('No experiment results available. Run the training/sweep cells first.')\n",
        "\n",
        "    summary_rows = []\n",
        "    for res in collected_results:\n",
        "        cfg = res['config']\n",
        "        metrics = res['metrics']\n",
        "        summary_rows.append({\n",
        "            'run_name': res['run_name'],\n",
        "            'fold': res.get('fold'),\n",
        "            'rnn_type': cfg['rnn_type'],\n",
        "            'bidirectional': cfg.get('bidirectional', False),\n",
        "            'hidden_size': cfg['hidden_size'],\n",
        "            'num_layers': cfg['num_layers'],\n",
        "            'dropout': cfg['dropout'],\n",
        "            'lr': cfg['lr'],\n",
        "            'weight_decay': cfg.get('weight_decay', 0.0),\n",
        "            'best_epoch': res['epoch'],\n",
        "            'best_f1': res['best_f1'],\n",
        "            'accuracy': metrics['accuracy'],\n",
        "            'precision': metrics['precision'],\n",
        "            'recall': metrics['recall'],\n",
        "            'checkpoint': str(res['checkpoint_path']),\n",
        "            'log_dir': str(LOG_DIR / res['run_name']),\n",
        "            'is_cv': res.get('fold') is not None,\n",
        "        })\n",
        "\n",
        "    summary_df = pd.DataFrame(summary_rows)\n",
        "    summary_df = summary_df.sort_values(by='best_f1', ascending=False).reset_index(drop=True)\n",
        "    return summary_df, collected_results\n",
        "\n",
        "\n",
        "RESULT_SOURCES = ['experiment_results', 'sweep_results', 'cv_results', 'auto_cv_results']\n",
        "summary_table, all_results = build_summary_table(RESULT_SOURCES)\n",
        "summary_table\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yC993yBuTBGY"
      },
      "outputs": [],
      "source": [
        "for _, row in summary_table.iterrows():\n",
        "    print(\n",
        "        f\"Run: {row['run_name']} | Model: {row['rnn_type'].upper()} | \"\n",
        "        f\"Bidirectional: {row['bidirectional']} | F1: {row['best_f1']:.4f}\"\n",
        "    )\n",
        "    print(f\"  Logs: {row['log_dir']}\")\n",
        "\n",
        "# Per TensorBoard combinato (eseguire su Colab / locale):\n",
        "# %tensorboard --logdir \"{LOG_DIR.as_posix()}\"\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pO9BAXqcTBGY"
      },
      "outputs": [],
      "source": [
        "cv_folds_df, cv_summary = summarize_cv_results(cv_results)\n",
        "cv_summary\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OR95BKT0TBGZ"
      },
      "outputs": [],
      "source": [
        "if 'all_results' not in globals() or not all_results:\n",
        "    raise RuntimeError('No experiment results available. Run the training/sweep cells first.')\n",
        "\n",
        "best_run = max(all_results, key=lambda x: x['best_f1'])\n",
        "best_model = best_run['model']\n",
        "best_history = best_run['history']\n",
        "print(\n",
        "    f\"Selected best run: {best_run['run_name']} | \"\n",
        "    f\"F1={best_run['best_f1']:.4f} | Accuracy={best_run['metrics']['accuracy']:.4f}\"\n",
        ")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Psxajjq9TBGZ"
      },
      "outputs": [],
      "source": [
        "def plot_history(history: Dict[str, List[float]], title: str = 'Learning Curves'):\n",
        "    epochs = range(1, len(history['train_loss']) + 1)\n",
        "    fig, axs = plt.subplots(1, 3, figsize=(18, 4))\n",
        "\n",
        "    axs[0].plot(epochs, history['train_loss'], label='Train')\n",
        "    axs[0].plot(epochs, history['valid_loss'], label='Valid')\n",
        "    axs[0].set_title('Loss')\n",
        "    axs[0].set_xlabel('Epoch')\n",
        "    axs[0].legend()\n",
        "\n",
        "    axs[1].plot(epochs, history['train_accuracy'], label='Train')\n",
        "    axs[1].plot(epochs, history['valid_accuracy'], label='Valid')\n",
        "    axs[1].set_title('Accuracy')\n",
        "    axs[1].set_xlabel('Epoch')\n",
        "    axs[1].legend()\n",
        "\n",
        "    axs[2].plot(epochs, history['train_f1'], label='Train F1')\n",
        "    axs[2].plot(epochs, history['valid_f1'], label='Valid F1')\n",
        "    axs[2].set_title('Macro F1')\n",
        "    axs[2].set_xlabel('Epoch')\n",
        "    axs[2].legend()\n",
        "\n",
        "    plt.suptitle(title)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "plot_history(best_history, title=f\"Learning Curves ‚Äî {best_run['run_name']}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3jnk70CJTBGZ"
      },
      "outputs": [],
      "source": [
        "best_preds = best_run['preds']\n",
        "best_targets = best_run['targets']\n",
        "print(f\"Best validation macro F1: {best_run['best_f1']:.3f}\")\n",
        "print(\n",
        "    classification_report(\n",
        "        best_targets,\n",
        "        best_preds,\n",
        "        target_names=[IDX2LABEL[i] for i in range(NUM_CLASSES)],\n",
        "    )\n",
        ")\n",
        "\n",
        "cf = confusion_matrix(best_targets, best_preds)\n",
        "plt.figure(figsize=(6, 5))\n",
        "sns.heatmap(\n",
        "    cf,\n",
        "    annot=True,\n",
        "    fmt='d',\n",
        "    cmap='Blues',\n",
        "    xticklabels=[IDX2LABEL[i] for i in range(NUM_CLASSES)],\n",
        "    yticklabels=[IDX2LABEL[i] for i in range(NUM_CLASSES)],\n",
        ")\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('True')\n",
        "plt.title(f\"Validation Confusion Matrix ‚Äî {best_run['run_name']}\")\n",
        "plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ALogtk9WTBGZ"
      },
      "outputs": [],
      "source": [
        "test_dataset = TimeSeriesDataset(\n",
        "    X_test_np,\n",
        "    labels=None,\n",
        "    window_size=None,\n",
        "    mode='test',\n",
        "    high_pain_targets=HIGH_PAIN_IDX,\n",
        ")\n",
        "test_loader = DataLoader(test_dataset, batch_size=256, shuffle=False, drop_last=False)\n",
        "\n",
        "best_model.eval()\n",
        "test_preds = []\n",
        "best_config = best_run['config']\n",
        "eval_window_size = best_config.get('eval_window_size', EVAL_WINDOW_SIZE)\n",
        "eval_window_stride = best_config.get('eval_window_stride', EVAL_WINDOW_STRIDE)\n",
        "eval_aggregation = best_config.get('eval_aggregation', EVAL_AGGREGATION)\n",
        "with torch.no_grad():\n",
        "    for inputs in test_loader:\n",
        "        inputs = inputs.to(DEVICE)\n",
        "        with autocast_context():\n",
        "            logits = forward_with_sliding_windows(\n",
        "                best_model,\n",
        "                inputs,\n",
        "                eval_window_size,\n",
        "                eval_window_stride,\n",
        "                eval_aggregation,\n",
        "            )\n",
        "        test_preds.append(torch.argmax(logits, dim=1).cpu().numpy())\n",
        "\n",
        "test_preds = np.concatenate(test_preds)\n",
        "test_labels = [IDX2LABEL[idx] for idx in test_preds]\n",
        "\n",
        "submission = pd.DataFrame({\n",
        "    'sample_index': pd.unique(X_test_raw['sample_index']),\n",
        "    'label': test_labels,\n",
        "})\n",
        "submission_filename = OUTPUT_DIR / f\"submission_{best_run['run_name'].lower()}.csv\"\n",
        "submission.to_csv(submission_filename, index=False)\n",
        "print(f\"Saved submission to {submission_filename}\")\n",
        "submission.head()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_Ip0Gs2ATBGZ"
      },
      "outputs": [],
      "source": [
        "%tensorboard --logdir \"/content/drive/MyDrive/[2025-2026]\\ AN2DL/Challenge/outputs/logs\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "64lgieJwTBGZ"
      },
      "source": [
        "## Next Steps\n",
        "\n",
        "- Espandere `EXPERIMENT_CONFIGS` con ricerche random/grid su hidden size, depth, dropout, learning rate e scheduler per automatizzare l'hyperparameter tuning.\n",
        "- Utilizzare `run_cross_validation` su pi√π configurazioni e confrontare le metriche aggregate in `cv_summary`, esportando i risultati (CSV/LaTeX) per il report finale.\n",
        "- Monitorare tutti i run con `%tensorboard --logdir outputs/logs`, salvando screenshot delle curve principali e confrontando tempi/risorse.\n",
        "- Integrare tecniche di regularizzazione avanzate (label smoothing, mixup temporale, stochastic weight averaging) o layer di attention/pooling.\n",
        "- Costruire ensemble sui checkpoint migliori (media delle probabilit√† o voting) prima della submission Kaggle definitiva.\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}