{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ðŸ´â€â˜ ï¸ AN2DL25 Challenge 1 â€” Pirate Pain Classification\n",
        "\n",
        "This notebook implements a full deep-learning pipeline for multivariate time-series classification of the Pirate Pain dataset. It is inspired by the Lecture 4 notebook (`Timeseries Classification (1).ipynb`) but adapted for the competition setting, including data preparation, model training (RNN/GRU/LSTM variants), evaluation, and test-time inference.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# %%capture\n",
        "# !pip install -q -r requirements.txt\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import random\n",
        "import math\n",
        "import copy\n",
        "from pathlib import Path\n",
        "from typing import Tuple, Dict, Optional, List\n",
        "from datetime import datetime\n",
        "from itertools import product\n",
        "import inspect\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.metrics import (\n",
        "    f1_score,\n",
        "    classification_report,\n",
        "    confusion_matrix,\n",
        "    precision_recall_fscore_support,\n",
        ")\n",
        "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "\n",
        "try:\n",
        "    from torch.amp import autocast, GradScaler\n",
        "except ImportError:  # pragma: no cover\n",
        "    from torch.cuda.amp import autocast, GradScaler\n",
        "\n",
        "\n",
        "def set_seed(seed: int = 42):\n",
        "    random.seed(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "SEED = 42\n",
        "\n",
        "try:\n",
        "    from google.colab import drive  # type: ignore\n",
        "    IN_COLAB = True\n",
        "except ImportError:  # pragma: no cover\n",
        "    IN_COLAB = False\n",
        "\n",
        "if IN_COLAB:\n",
        "    drive.mount('/content/drive', force_remount=False)\n",
        "    BASE_DIR = Path('/content/drive/MyDrive/[2025-2026]\\ A2NDL/Challenge')\n",
        "else:\n",
        "    BASE_DIR = Path('/Users/md101ta/Desktop/Pirates')\n",
        "\n",
        "DATA_DIR = (BASE_DIR / 'data').resolve()\n",
        "OUTPUT_DIR = (BASE_DIR / 'outputs').resolve()\n",
        "OUTPUT_DIR.mkdir(exist_ok=True, parents=True)\n",
        "\n",
        "# Reproducibility\n",
        "os.environ['PYTHONHASHSEED'] = str(SEED)\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(SEED)\n",
        "    DEVICE = torch.device('cuda')\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "else:\n",
        "    DEVICE = torch.device('cpu')\n",
        "\n",
        "print(f'Running in Colab: {IN_COLAB}')\n",
        "print(f'Device: {DEVICE}')\n",
        "print(f'Data dir: {DATA_DIR}')\n",
        "print(f'Output dir: {OUTPUT_DIR}')\n",
        "\n",
        "_AUTocast_params = inspect.signature(autocast).parameters\n",
        "_GRADSCALER_PARAMS = inspect.signature(GradScaler).parameters\n",
        "\n",
        "def autocast_context():\n",
        "    enabled = DEVICE.type == 'cuda'\n",
        "    if 'device_type' in _AUTocast_params:\n",
        "        return autocast(device_type=DEVICE.type, enabled=enabled)\n",
        "    if 'device' in _AUTocast_params:\n",
        "        return autocast(DEVICE.type, enabled=enabled)\n",
        "    # fallback to legacy signature (enabled only)\n",
        "    return autocast(enabled=enabled)\n",
        "\n",
        "\n",
        "def create_grad_scaler():\n",
        "    enabled = DEVICE.type == 'cuda'\n",
        "    if 'device_type' in _GRADSCALER_PARAMS:\n",
        "        return GradScaler(device_type=DEVICE.type, enabled=enabled)\n",
        "    return GradScaler(enabled=enabled)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "LOG_DIR = (OUTPUT_DIR / 'logs').resolve()\n",
        "CHECKPOINT_DIR = (OUTPUT_DIR / 'checkpoints').resolve()\n",
        "LOG_DIR.mkdir(exist_ok=True, parents=True)\n",
        "CHECKPOINT_DIR.mkdir(exist_ok=True, parents=True)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "from typing import Tuple\n",
        "\n",
        "# Percorso al dataset su Google Drive\n",
        "DATA_DIR = Path('/content/drive/MyDrive/[2025-2026] AN2DL/Challenge')\n",
        "\n",
        "def load_data(data_dir: Path) -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n",
        "    X_train = pd.read_csv(data_dir / 'pirate_pain_train.csv')\n",
        "    y_train = pd.read_csv(data_dir / 'pirate_pain_train_labels.csv')\n",
        "    X_test  = pd.read_csv(data_dir / 'pirate_pain_test.csv')\n",
        "    return X_train, y_train, X_test\n",
        "\n",
        "# Carica i dati\n",
        "X_train_raw, y_train, X_test_raw = load_data(DATA_DIR)\n",
        "print(X_train_raw.shape, y_train.shape, X_test_raw.shape)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "CATEGORICAL_COLUMNS = ['n_legs', 'n_hands', 'n_eyes']\n",
        "CATEGORY_MAPPINGS: Dict[str, Dict[str, int]] = {}\n",
        "\n",
        "for col in CATEGORICAL_COLUMNS:\n",
        "    uniques = pd.concat([X_train_raw[col], X_test_raw[col]]).dropna().unique()\n",
        "    mapping = {value: idx for idx, value in enumerate(sorted(uniques))}\n",
        "    CATEGORY_MAPPINGS[col] = mapping\n",
        "    X_train_raw[col] = X_train_raw[col].map(mapping).astype(np.int32)\n",
        "    X_test_raw[col] = X_test_raw[col].map(mapping).astype(np.int32)\n",
        "\n",
        "FEATURE_COLUMNS = [col for col in X_train_raw.columns if col not in ['sample_index', 'time']]\n",
        "TIME_STEPS = X_train_raw['time'].nunique()\n",
        "NUM_FEATURES = len(FEATURE_COLUMNS)\n",
        "NUM_CLASSES = y_train['label'].nunique()\n",
        "print(f'Time steps: {TIME_STEPS} | Features: {NUM_FEATURES} | Classes: {NUM_CLASSES}')\n",
        "print('Category mappings:', CATEGORY_MAPPINGS)\n",
        "\n",
        "y_train['label'].value_counts().plot(kind='bar', title='Class distribution')\n",
        "plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "LABEL2IDX = {label: idx for idx, label in enumerate(sorted(y_train['label'].unique()))}\n",
        "IDX2LABEL = {idx: label for label, idx in LABEL2IDX.items()}\n",
        "print('Label mapping:', LABEL2IDX)\n",
        "\n",
        "\n",
        "def pivot_timeseries(df: pd.DataFrame) -> np.ndarray:\n",
        "    pivoted = df.set_index(['sample_index', 'time'])[FEATURE_COLUMNS].unstack(level=1)\n",
        "    times = sorted(df['time'].unique())\n",
        "    pivoted = pivoted.reindex(columns=pd.MultiIndex.from_product([FEATURE_COLUMNS, times]))\n",
        "    values = pivoted.to_numpy().reshape(-1, NUM_FEATURES, len(times)).transpose(0, 2, 1)\n",
        "    return values\n",
        "\n",
        "\n",
        "X_train_np = pivot_timeseries(X_train_raw)\n",
        "X_test_np = pivot_timeseries(X_test_raw)\n",
        "y_train_idx = y_train.set_index('sample_index').loc[pd.unique(X_train_raw['sample_index'])]['label'].map(LABEL2IDX).to_numpy()\n",
        "\n",
        "print(X_train_np.shape, y_train_idx.shape, X_test_np.shape)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def compute_normalization_stats(data: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:\n",
        "    # data shape: (N, T, F)\n",
        "    mean = data.reshape(-1, NUM_FEATURES).mean(axis=0)\n",
        "    std = data.reshape(-1, NUM_FEATURES).std(axis=0) + 1e-6\n",
        "    return mean, std\n",
        "\n",
        "\n",
        "def normalize(data: np.ndarray, mean: np.ndarray, std: np.ndarray) -> np.ndarray:\n",
        "    return (data - mean) / std\n",
        "\n",
        "\n",
        "feat_mean, feat_std = compute_normalization_stats(X_train_np)\n",
        "X_train_np = normalize(X_train_np, feat_mean, feat_std)\n",
        "X_test_np = normalize(X_test_np, feat_mean, feat_std)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def make_dataloader_from_arrays(\n",
        "    X: np.ndarray,\n",
        "    y: Optional[np.ndarray],\n",
        "    batch_size: int,\n",
        "    shuffle: bool,\n",
        "    seed: int = SEED,\n",
        "    num_workers: int = 0,\n",
        "    pin_memory: bool = False,\n",
        ") -> DataLoader:\n",
        "    dataset = TimeSeriesDataset(X, y)\n",
        "    generator = torch.Generator().manual_seed(seed) if shuffle else None\n",
        "    return DataLoader(\n",
        "        dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=shuffle,\n",
        "        drop_last=False,\n",
        "        num_workers=num_workers,\n",
        "        pin_memory=pin_memory,\n",
        "        generator=generator,\n",
        "    )\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class TimeSeriesDataset(Dataset):\n",
        "    def __init__(self, data: np.ndarray, labels: Optional[np.ndarray] = None):\n",
        "        self.data = torch.tensor(data, dtype=torch.float32)\n",
        "        self.labels = None if labels is None else torch.tensor(labels, dtype=torch.long)\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        return self.data.shape[0]\n",
        "\n",
        "    def __getitem__(self, idx: int):\n",
        "        if self.labels is None:\n",
        "            return self.data[idx]\n",
        "        return self.data[idx], self.labels[idx]\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def create_dataloaders(\n",
        "    X: np.ndarray,\n",
        "    y: np.ndarray,\n",
        "    valid_size: float = 0.1,\n",
        "    batch_size: int = 64,\n",
        "    seed: int = SEED,\n",
        "    num_workers: int = 0,\n",
        "    pin_memory: bool = False,\n",
        ") -> Tuple[DataLoader, DataLoader, Tuple[np.ndarray, ...], Tuple[np.ndarray, np.ndarray]]:\n",
        "    idx_all = np.arange(len(y))\n",
        "    X_train, X_valid, y_train, y_valid, train_idx, valid_idx = train_test_split(\n",
        "        X,\n",
        "        y,\n",
        "        idx_all,\n",
        "        test_size=valid_size,\n",
        "        random_state=seed,\n",
        "        stratify=y,\n",
        "    )\n",
        "\n",
        "    train_loader = make_dataloader_from_arrays(\n",
        "        X_train,\n",
        "        y_train,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=True,\n",
        "        seed=seed,\n",
        "        num_workers=num_workers,\n",
        "        pin_memory=pin_memory,\n",
        "    )\n",
        "    valid_loader = make_dataloader_from_arrays(\n",
        "        X_valid,\n",
        "        y_valid,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=False,\n",
        "        seed=seed,\n",
        "        num_workers=num_workers,\n",
        "        pin_memory=pin_memory,\n",
        "    )\n",
        "    return train_loader, valid_loader, (X_train, y_train, X_valid, y_valid), (train_idx, valid_idx)\n",
        "\n",
        "\n",
        "DEFAULT_BATCH_SIZE = 64\n",
        "train_loader, valid_loader, (X_train_split, y_train_split, X_valid_split, y_valid_split), _ = create_dataloaders(\n",
        "    X_train_np,\n",
        "    y_train_idx,\n",
        "    batch_size=DEFAULT_BATCH_SIZE,\n",
        ")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def label_smoothing_targets(num_classes: int, smoothing: float) -> torch.Tensor:\n",
        "    assert 0.0 <= smoothing < 1.0\n",
        "    return torch.full((num_classes,), smoothing / (num_classes - 1), dtype=torch.float32)\n",
        "\n",
        "\n",
        "def build_criterion(config: Dict, class_counts: Optional[np.ndarray] = None) -> nn.Module:\n",
        "    weight = None\n",
        "    if config.get('use_class_weights', False) and class_counts is not None:\n",
        "        total = class_counts.sum()\n",
        "        weight = torch.tensor([total / max(c, 1) for c in class_counts], dtype=torch.float32)\n",
        "    if config.get('use_focal_loss', False):\n",
        "        alpha = weight if weight is not None else None\n",
        "        return FocalLoss(alpha=alpha, gamma=config.get('focal_gamma', 2.0))\n",
        "    smoothing = config.get('label_smoothing', 0.0)\n",
        "    return nn.CrossEntropyLoss(\n",
        "        weight=weight,\n",
        "        label_smoothing=smoothing,\n",
        "    )\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class RecurrentBackbone(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        input_size: int,\n",
        "        hidden_size: int = 128,\n",
        "        num_layers: int = 2,\n",
        "        dropout: float = 0.2,\n",
        "        rnn_type: str = 'lstm',\n",
        "        bidirectional: bool = True,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        rnn_cls = {\n",
        "            'rnn': nn.RNN,\n",
        "            'gru': nn.GRU,\n",
        "            'lstm': nn.LSTM,\n",
        "        }[rnn_type.lower()]\n",
        "        self.rnn_type = rnn_type.lower()\n",
        "        self.bidirectional = bidirectional\n",
        "        self.rnn = rnn_cls(\n",
        "            input_size=input_size,\n",
        "            hidden_size=hidden_size,\n",
        "            num_layers=num_layers,\n",
        "            dropout=dropout if num_layers > 1 else 0.0,\n",
        "            batch_first=True,\n",
        "            bidirectional=bidirectional,\n",
        "        )\n",
        "        proj_input = hidden_size * (2 if bidirectional else 1)\n",
        "        self.head = nn.Sequential(\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(proj_input, hidden_size),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(hidden_size, NUM_CLASSES),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out, _ = self.rnn(x)\n",
        "        # use last time-step hidden state\n",
        "        last = out[:, -1, :]\n",
        "        return self.head(last)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def compute_classification_metrics(preds: np.ndarray, targets: np.ndarray) -> Dict[str, float]:\n",
        "    accuracy = float((preds == targets).mean())\n",
        "    precision, recall, f1, _ = precision_recall_fscore_support(\n",
        "        targets,\n",
        "        preds,\n",
        "        average='macro',\n",
        "        zero_division=0,\n",
        "    )\n",
        "    return {\n",
        "        'accuracy': accuracy,\n",
        "        'precision': float(precision),\n",
        "        'recall': float(recall),\n",
        "        'f1': float(f1),\n",
        "    }\n",
        "\n",
        "\n",
        "def train_one_epoch(\n",
        "    model: nn.Module,\n",
        "    loader: DataLoader,\n",
        "    criterion: nn.Module,\n",
        "    optimizer: torch.optim.Optimizer,\n",
        "    scaler: GradScaler,\n",
        "    max_grad_norm: float = 5.0,\n",
        ") -> Tuple[float, Dict[str, float]]:\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    preds_all, targets_all = [], []\n",
        "\n",
        "    for inputs, targets in loader:\n",
        "        inputs = inputs.to(DEVICE)\n",
        "        targets = targets.to(DEVICE)\n",
        "        optimizer.zero_grad(set_to_none=True)\n",
        "\n",
        "        with autocast_context():\n",
        "            logits = model(inputs)\n",
        "            loss = criterion(logits, targets)\n",
        "\n",
        "        scaler.scale(loss).backward()\n",
        "        if max_grad_norm is not None:\n",
        "            scaler.unscale_(optimizer)\n",
        "            nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "\n",
        "        running_loss += loss.item() * inputs.size(0)\n",
        "        preds_all.append(torch.argmax(logits.detach(), dim=1).cpu())\n",
        "        targets_all.append(targets.cpu())\n",
        "\n",
        "    preds_np = torch.cat(preds_all).numpy()\n",
        "    targets_np = torch.cat(targets_all).numpy()\n",
        "    metrics = compute_classification_metrics(preds_np, targets_np)\n",
        "    avg_loss = running_loss / len(loader.dataset)\n",
        "    return avg_loss, metrics\n",
        "\n",
        "\n",
        "def evaluate_epoch(\n",
        "    model: nn.Module,\n",
        "    loader: DataLoader,\n",
        "    criterion: nn.Module,\n",
        ") -> Tuple[float, Dict[str, float], np.ndarray, np.ndarray]:\n",
        "    model.eval()\n",
        "    running_loss = 0.0\n",
        "    preds_all, targets_all = [], []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, targets in loader:\n",
        "            inputs = inputs.to(DEVICE)\n",
        "            targets = targets.to(DEVICE)\n",
        "            with autocast_context():\n",
        "                logits = model(inputs)\n",
        "                loss = criterion(logits, targets)\n",
        "\n",
        "            running_loss += loss.item() * inputs.size(0)\n",
        "            preds_all.append(torch.argmax(logits, dim=1).cpu())\n",
        "            targets_all.append(targets.cpu())\n",
        "\n",
        "    preds_np = torch.cat(preds_all).numpy()\n",
        "    targets_np = torch.cat(targets_all).numpy()\n",
        "    metrics = compute_classification_metrics(preds_np, targets_np)\n",
        "    avg_loss = running_loss / len(loader.dataset)\n",
        "    return avg_loss, metrics, preds_np, targets_np\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def fit_model(\n",
        "    config: Dict,\n",
        "    train_loader: DataLoader,\n",
        "    valid_loader: DataLoader,\n",
        "    run_name: str,\n",
        "    criterion: Optional[nn.Module] = None,\n",
        "    tensorboard: bool = True,\n",
        ") -> Tuple[nn.Module, Dict[str, List[float]], Dict]:\n",
        "    model = RecurrentBackbone(\n",
        "        input_size=NUM_FEATURES,\n",
        "        hidden_size=config['hidden_size'],\n",
        "        num_layers=config['num_layers'],\n",
        "        dropout=config['dropout'],\n",
        "        rnn_type=config['rnn_type'],\n",
        "        bidirectional=config.get('bidirectional', False),\n",
        "    ).to(DEVICE)\n",
        "\n",
        "    if criterion is None:\n",
        "        criterion = build_criterion(config)\n",
        "    if hasattr(criterion, 'to'):\n",
        "        criterion = criterion.to(DEVICE)\n",
        "\n",
        "    optimizer = torch.optim.AdamW(\n",
        "        model.parameters(),\n",
        "        lr=config['lr'],\n",
        "        weight_decay=config.get('weight_decay', 0.0),\n",
        "    )\n",
        "\n",
        "    scheduler = None\n",
        "    if config.get('scheduler', 'plateau') == 'plateau':\n",
        "        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
        "            optimizer,\n",
        "            mode='max',\n",
        "            factor=config.get('scheduler_factor', 0.5),\n",
        "            patience=config.get('scheduler_patience', 3),\n",
        "            threshold=config.get('scheduler_threshold', 1e-3),\n",
        "            cooldown=config.get('scheduler_cooldown', 0),\n",
        "        )\n",
        "\n",
        "    scaler = create_grad_scaler()\n",
        "\n",
        "    history: Dict[str, List[float]] = {\n",
        "        'train_loss': [],\n",
        "        'train_accuracy': [],\n",
        "        'train_f1': [],\n",
        "        'train_precision': [],\n",
        "        'train_recall': [],\n",
        "        'valid_loss': [],\n",
        "        'valid_accuracy': [],\n",
        "        'valid_f1': [],\n",
        "        'valid_precision': [],\n",
        "        'valid_recall': [],\n",
        "        'valid_f1_ema': [],\n",
        "        'lr': [],\n",
        "    }\n",
        "\n",
        "    run_log_dir = (LOG_DIR / run_name).resolve()\n",
        "    writer = SummaryWriter(run_log_dir.as_posix()) if tensorboard else None\n",
        "\n",
        "    metric_key = config.get('metric_for_best', 'macro_f1')\n",
        "    patience = config.get('patience', 10)\n",
        "    min_improvement = config.get('min_improvement', 0.0)\n",
        "    warmup_epochs = config.get('warmup_epochs', 0)\n",
        "    base_lr = config['lr']\n",
        "    ema_alpha = config.get('ema_alpha', 0.1)\n",
        "\n",
        "    best_ema = -np.inf\n",
        "    best_metric_value = -np.inf\n",
        "    ema_value = None\n",
        "    patience_counter = 0\n",
        "    checkpoint_path = (CHECKPOINT_DIR / f'{run_name}.pt').resolve()\n",
        "    best_state: Optional[Dict] = None\n",
        "\n",
        "    for epoch in range(1, config['epochs'] + 1):\n",
        "        train_loss, train_metrics = train_one_epoch(\n",
        "            model,\n",
        "            train_loader,\n",
        "            criterion,\n",
        "            optimizer,\n",
        "            scaler,\n",
        "            max_grad_norm=config.get('max_grad_norm', 5.0),\n",
        "        )\n",
        "        valid_loss, valid_metrics, preds, targets = evaluate_epoch(\n",
        "            model,\n",
        "            valid_loader,\n",
        "            criterion,\n",
        "        )\n",
        "\n",
        "        metric_value = valid_metrics['f1'] if metric_key in {'macro_f1', 'f1'} else valid_metrics.get('accuracy', 0.0)\n",
        "        ema_value = metric_value if ema_value is None else (ema_alpha * metric_value + (1 - ema_alpha) * ema_value)\n",
        "\n",
        "        if warmup_epochs > 0 and epoch <= warmup_epochs:\n",
        "            lr_now = base_lr * (epoch / warmup_epochs)\n",
        "            for group in optimizer.param_groups:\n",
        "                group['lr'] = lr_now\n",
        "        elif scheduler is not None:\n",
        "            scheduler.step(metric_value)\n",
        "\n",
        "        current_lr = optimizer.param_groups[0]['lr']\n",
        "\n",
        "        history['train_loss'].append(train_loss)\n",
        "        history['train_accuracy'].append(train_metrics['accuracy'])\n",
        "        history['train_f1'].append(train_metrics['f1'])\n",
        "        history['train_precision'].append(train_metrics['precision'])\n",
        "        history['train_recall'].append(train_metrics['recall'])\n",
        "        history['valid_loss'].append(valid_loss)\n",
        "        history['valid_accuracy'].append(valid_metrics['accuracy'])\n",
        "        history['valid_f1'].append(valid_metrics['f1'])\n",
        "        history['valid_precision'].append(valid_metrics['precision'])\n",
        "        history['valid_recall'].append(valid_metrics['recall'])\n",
        "        history['valid_f1_ema'].append(ema_value)\n",
        "        history['lr'].append(current_lr)\n",
        "\n",
        "        if writer is not None:\n",
        "            writer.add_scalar('Loss/train', train_loss, epoch)\n",
        "            writer.add_scalar('Loss/valid', valid_loss, epoch)\n",
        "            writer.add_scalar('F1/macro', valid_metrics['f1'], epoch)\n",
        "            writer.add_scalar('F1/macro_ema', ema_value, epoch)\n",
        "            writer.add_scalar('Accuracy/train', train_metrics['accuracy'], epoch)\n",
        "            writer.add_scalar('Accuracy/valid', valid_metrics['accuracy'], epoch)\n",
        "            writer.add_scalar('LearningRate', current_lr, epoch)\n",
        "            try:\n",
        "                rep = classification_report(targets, preds, output_dict=True, zero_division=0)\n",
        "                for cls_name, stats in rep.items():\n",
        "                    if isinstance(stats, dict) and 'f1-score' in stats:\n",
        "                        writer.add_scalar(f'F1_class/{cls_name}', stats['f1-score'], epoch)\n",
        "                cm = confusion_matrix(targets, preds)\n",
        "                fig, ax = plt.subplots()\n",
        "                ax.imshow(cm, interpolation='nearest', cmap='Blues')\n",
        "                ax.set_title('Confusion Matrix')\n",
        "                ax.set_xlabel('Predicted')\n",
        "                ax.set_ylabel('True')\n",
        "                for (i, j), val in np.ndenumerate(cm):\n",
        "                    ax.text(j, i, int(val), ha='center', va='center')\n",
        "                writer.add_figure('ConfusionMatrix', fig, epoch)\n",
        "                plt.close(fig)\n",
        "            except Exception:\n",
        "                pass\n",
        "\n",
        "        msg = (\n",
        "            f\"Epoch {epoch:03d} | \"\n",
        "            f\"train_loss={train_loss:.4f} acc={train_metrics['accuracy']:.3f} f1={train_metrics['f1']:.3f} | \"\n",
        "            f\"valid_loss={valid_loss:.4f} acc={valid_metrics['accuracy']:.3f} f1={valid_metrics['f1']:.3f} | \"\n",
        "            f\"lr={current_lr:.2e}\"\n",
        "        )\n",
        "        print(msg)\n",
        "\n",
        "        improved = ema_value > best_ema + min_improvement\n",
        "        if improved:\n",
        "            best_ema = ema_value\n",
        "            best_metric_value = metric_value\n",
        "            patience_counter = 0\n",
        "            best_state = {\n",
        "                'epoch': epoch,\n",
        "                'model_state': {k: v.cpu() for k, v in model.state_dict().items()},\n",
        "                'optimizer_state': optimizer.state_dict(),\n",
        "                'metrics': valid_metrics,\n",
        "                'train_metrics': train_metrics,\n",
        "                'preds': preds,\n",
        "                'targets': targets,\n",
        "                'val_macro_f1': metric_value,\n",
        "                'val_macro_f1_ema': ema_value,\n",
        "            }\n",
        "            torch.save(best_state['model_state'], checkpoint_path)\n",
        "        else:\n",
        "            patience_counter += 1\n",
        "            if patience_counter >= patience:\n",
        "                print(\n",
        "                    f\"Early stopping triggered at epoch {epoch}. Best EMA metric={best_ema:.4f}\"\n",
        "                )\n",
        "                break\n",
        "\n",
        "    if writer is not None:\n",
        "        writer.close()\n",
        "\n",
        "    if best_state is None:\n",
        "        best_state = {\n",
        "            'epoch': epoch,\n",
        "            'model_state': {k: v.cpu() for k, v in model.state_dict().items()},\n",
        "            'optimizer_state': optimizer.state_dict(),\n",
        "            'metrics': valid_metrics,\n",
        "            'train_metrics': train_metrics,\n",
        "            'preds': preds,\n",
        "            'targets': targets,\n",
        "            'val_macro_f1': metric_value,\n",
        "            'val_macro_f1_ema': ema_value,\n",
        "        }\n",
        "        torch.save(best_state['model_state'], checkpoint_path)\n",
        "\n",
        "    model.load_state_dict(best_state['model_state'])\n",
        "    best_state.update(\n",
        "        {\n",
        "            'run_name': run_name,\n",
        "            'config': copy.deepcopy(config),\n",
        "            'history': history,\n",
        "            'checkpoint_path': checkpoint_path,\n",
        "            'best_f1': best_metric_value,\n",
        "        }\n",
        "    )\n",
        "    return model, history, best_state\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def run_cross_validation(\n",
        "    config: Dict,\n",
        "    n_splits: int = 5,\n",
        "    shuffle: bool = True,\n",
        "    random_state: int = SEED,\n",
        ") -> List[Dict]:\n",
        "    skf = StratifiedKFold(n_splits=n_splits, shuffle=shuffle, random_state=random_state)\n",
        "    fold_results: List[Dict] = []\n",
        "\n",
        "    print(f\"\\n>>> Starting {n_splits}-fold CV for {config['run_name']} ({config['rnn_type'].upper()})\")\n",
        "    for fold_idx, (train_idx, valid_idx) in enumerate(skf.split(X_train_np, y_train_idx), start=1):\n",
        "        fold_config = copy.deepcopy(config)\n",
        "        fold_config['run_name'] = f\"{config['run_name']}_fold{fold_idx}\"\n",
        "\n",
        "        X_tr, y_tr = X_train_np[train_idx], y_train_idx[train_idx]\n",
        "        X_val, y_val = X_train_np[valid_idx], y_train_idx[valid_idx]\n",
        "\n",
        "        train_loader = make_dataloader_from_arrays(X_tr, y_tr, batch_size=fold_config['batch_size'], shuffle=True)\n",
        "        valid_loader = make_dataloader_from_arrays(X_val, y_val, batch_size=fold_config['batch_size'], shuffle=False)\n",
        "\n",
        "        model, history, best_state = fit_model(\n",
        "            fold_config,\n",
        "            train_loader,\n",
        "            valid_loader,\n",
        "            run_name=fold_config['run_name'],\n",
        "            tensorboard=fold_config.get('tensorboard', True),\n",
        "        )\n",
        "        best_state['fold'] = fold_idx\n",
        "        best_state['model'] = model\n",
        "        best_state['history'] = history\n",
        "        fold_results.append(best_state)\n",
        "\n",
        "        metrics = best_state['metrics']\n",
        "        print(\n",
        "            f\"Fold {fold_idx}/{n_splits} | best F1={metrics['f1']:.4f} | \"\n",
        "            f\"Accuracy={metrics['accuracy']:.4f}\"\n",
        "        )\n",
        "\n",
        "    return fold_results\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def summarize_cv_results(cv_results: List[Dict]) -> pd.DataFrame:\n",
        "    rows = []\n",
        "    for res in cv_results:\n",
        "        cfg = res['config']\n",
        "        metrics = res['metrics']\n",
        "        rows.append({\n",
        "            'run_name': res['run_name'],\n",
        "            'fold': res['fold'],\n",
        "            'rnn_type': cfg['rnn_type'],\n",
        "            'bidirectional': cfg.get('bidirectional', False),\n",
        "            'hidden_size': cfg['hidden_size'],\n",
        "            'num_layers': cfg['num_layers'],\n",
        "            'dropout': cfg['dropout'],\n",
        "            'f1': metrics['f1'],\n",
        "            'accuracy': metrics['accuracy'],\n",
        "            'precision': metrics['precision'],\n",
        "            'recall': metrics['recall'],\n",
        "            'checkpoint': str(res['checkpoint_path']),\n",
        "            'log_dir': str(LOG_DIR / res['run_name']),\n",
        "        })\n",
        "\n",
        "    df = pd.DataFrame(rows)\n",
        "    agg = df.groupby(['rnn_type', 'bidirectional'])[['f1', 'accuracy', 'precision', 'recall']].agg(['mean', 'std'])\n",
        "    agg.columns = ['_'.join(col) for col in agg.columns]\n",
        "    agg = agg.reset_index()\n",
        "    return df, agg\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "BASE_CONFIG = {\n",
        "    'hidden_size': 256,\n",
        "    'num_layers': 2,\n",
        "    'dropout': 0.3,\n",
        "    'lr': 2e-3,\n",
        "    'weight_decay': 1e-5,\n",
        "    'epochs': 60,\n",
        "    'batch_size': 64,\n",
        "    'valid_size': 0.1,\n",
        "    'seed': SEED,\n",
        "    'patience': 8,\n",
        "    'min_improvement': 5e-4,\n",
        "    'metric_for_best': 'macro_f1',\n",
        "    'warmup_epochs': 5,\n",
        "    'scheduler': 'plateau',\n",
        "    'scheduler_factor': 0.5,\n",
        "    'scheduler_patience': 3,\n",
        "    'scheduler_threshold': 1e-3,\n",
        "    'scheduler_cooldown': 0,\n",
        "    'max_grad_norm': 5.0,\n",
        "    'num_workers': 2,\n",
        "    'pin_memory': False,\n",
        "    'ema_alpha': 0.1,\n",
        "    'tensorboard': True,\n",
        "    'label_smoothing': 0.05,\n",
        "    'use_focal_loss': False,\n",
        "    'use_class_weights': False,\n",
        "}\n",
        "\n",
        "\n",
        "def prepare_config(\n",
        "    name: str,\n",
        "    rnn_type: str,\n",
        "    bidirectional: bool,\n",
        "    overrides: Optional[Dict] = None,\n",
        ") -> Dict:\n",
        "    config = copy.deepcopy(BASE_CONFIG)\n",
        "    config.update({\n",
        "        'run_name': name,\n",
        "        'rnn_type': rnn_type,\n",
        "        'bidirectional': bidirectional,\n",
        "    })\n",
        "    if overrides:\n",
        "        config.update(overrides)\n",
        "    return config\n",
        "\n",
        "\n",
        "def run_configurations(section_name: str, configs: List[Dict]) -> List[Dict]:\n",
        "    results: List[Dict] = []\n",
        "    for cfg in configs:\n",
        "        print(\n",
        "            f\"\\n=== [{section_name}] Running: {cfg['run_name']} \"\n",
        "            f\"({cfg['rnn_type'].upper()} - {'BI' if cfg['bidirectional'] else 'UNI'}) ===\"\n",
        "        )\n",
        "        result = run_experiment(cfg)\n",
        "        results.append(result)\n",
        "        print(\n",
        "            f\"Best validation F1: {result['best_f1']:.4f} at epoch {result['epoch']} | \"\n",
        "            f\"Accuracy: {result['metrics']['accuracy']:.4f}\"\n",
        "        )\n",
        "    return results\n",
        "\n",
        "\n",
        "def run_experiment(config: Dict) -> Dict:\n",
        "    run_name = config.get('run_name') or f\"{config['rnn_type'].upper()}_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
        "    config = copy.deepcopy(config)\n",
        "    config['run_name'] = run_name\n",
        "\n",
        "    set_seed(config.get('seed', SEED))\n",
        "\n",
        "    train_loader, valid_loader, (X_tr, y_tr, X_val, y_val), (train_idx, valid_idx) = create_dataloaders(\n",
        "        X_train_np,\n",
        "        y_train_idx,\n",
        "        valid_size=config.get('valid_size', 0.1),\n",
        "        batch_size=config['batch_size'],\n",
        "        seed=config.get('seed', SEED),\n",
        "        num_workers=config.get('num_workers', 4),\n",
        "        pin_memory=config.get('pin_memory', True),\n",
        "    )\n",
        "\n",
        "    class_counts = np.bincount(y_tr, minlength=NUM_CLASSES)\n",
        "    criterion = build_criterion(config, class_counts=class_counts)\n",
        "    if hasattr(criterion, 'to'):\n",
        "        criterion = criterion.to(DEVICE)\n",
        "\n",
        "    model, history, best_state = fit_model(\n",
        "        config,\n",
        "        train_loader,\n",
        "        valid_loader,\n",
        "        run_name=run_name,\n",
        "        criterion=criterion,\n",
        "        tensorboard=config.get('tensorboard', True),\n",
        "    )\n",
        "\n",
        "    best_state['data_split'] = {\n",
        "        'X_train': X_tr,\n",
        "        'y_train': y_tr,\n",
        "        'X_valid': X_val,\n",
        "        'y_valid': y_val,\n",
        "    }\n",
        "    best_state['split_idx'] = {\n",
        "        'train_idx': train_idx,\n",
        "        'valid_idx': valid_idx,\n",
        "    }\n",
        "    best_state['feature_names'] = FEATURE_COLUMNS\n",
        "    best_state['label2idx'] = LABEL2IDX\n",
        "    best_state['idx2label'] = IDX2LABEL\n",
        "    best_state['scaler_mean'] = feat_mean\n",
        "    best_state['scaler_std'] = feat_std\n",
        "    best_state['model'] = model\n",
        "    best_state['history'] = history\n",
        "    return best_state\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Containers for experiment outputs\n",
        "rnn_results: List[Dict] = []\n",
        "gru_results: List[Dict] = []\n",
        "lstm_results: List[Dict] = []\n",
        "sweep_results: List[Dict] = []\n",
        "cv_results: List[Dict] = []\n",
        "auto_cv_results: List[Dict] = []\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- RNN Experiments ---\n",
        "RNN_CONFIGS = [\n",
        "    prepare_config('RNN_SINGLE', 'rnn', False),\n",
        "    prepare_config('RNN_BI', 'rnn', True),\n",
        "]\n",
        "rnn_results = run_configurations('RNN', RNN_CONFIGS)\n",
        "\n",
        "# --- GRU Experiments ---\n",
        "GRU_CONFIGS = [\n",
        "    prepare_config('GRU_SINGLE', 'gru', False, {'use_class_weights': True}),\n",
        "    prepare_config('GRU_BI', 'gru', True, {'use_class_weights': True}),\n",
        "]\n",
        "gru_results = run_configurations('GRU', GRU_CONFIGS)\n",
        "\n",
        "# --- LSTM Experiments ---\n",
        "LSTM_CONFIGS = [\n",
        "    prepare_config('LSTM_SINGLE', 'lstm', False, {'dropout': 0.4, 'epochs': 80}),\n",
        "    prepare_config('LSTM_BI', 'lstm', True, {'dropout': 0.4, 'epochs': 80}),\n",
        "]\n",
        "lstm_results = run_configurations('LSTM', LSTM_CONFIGS)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# (Cross-validation verrÃ  eseguita dopo aver costruito summary_table)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "SWEEP_PARAM_SPACE = {\n",
        "    'hidden_size': [192, 256, 320],\n",
        "    'num_layers': [2, 3],\n",
        "    'dropout': [0.30, 0.45],\n",
        "    'bidirectional': [True, False],\n",
        "    'lr': [1e-3, 2e-3],\n",
        "}\n",
        "MAX_SWEEP_RUNS = 12\n",
        "SWEEP_SEED = 42\n",
        "\n",
        "param_grid = list(product(\n",
        "    SWEEP_PARAM_SPACE['hidden_size'],\n",
        "    SWEEP_PARAM_SPACE['num_layers'],\n",
        "    SWEEP_PARAM_SPACE['dropout'],\n",
        "    SWEEP_PARAM_SPACE['bidirectional'],\n",
        "    SWEEP_PARAM_SPACE['lr'],\n",
        "))\n",
        "random.Random(SWEEP_SEED).shuffle(param_grid)\n",
        "param_grid = param_grid[:MAX_SWEEP_RUNS]\n",
        "\n",
        "sweep_results: List[Dict] = []\n",
        "for idx, (hidden_size, num_layers, dropout, bidirectional, lr) in enumerate(param_grid, start=1):\n",
        "    run_name = (\n",
        "        f\"GRU_SWEEP_{idx:02d}_HS{hidden_size}_L{num_layers}_DO{int(dropout*100)}_\"\n",
        "        f\"{'BI' if bidirectional else 'UNI'}_LR{lr:.0e}\"\n",
        "    )\n",
        "    cfg = prepare_config(\n",
        "        run_name,\n",
        "        'gru',\n",
        "        bidirectional,\n",
        "        overrides={\n",
        "            'hidden_size': hidden_size,\n",
        "            'num_layers': num_layers,\n",
        "            'dropout': dropout,\n",
        "            'lr': lr,\n",
        "        },\n",
        "    )\n",
        "    print(f\"\\n>>> Sweep run {idx}/{len(param_grid)} â€” {run_name}\")\n",
        "    result = run_experiment(cfg)\n",
        "    sweep_results.append(result)\n",
        "\n",
        "print(f\"\\nCompleted {len(sweep_results)} GRU sweep runs.\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def gather_results(source_names: List[str]) -> List[Dict]:\n",
        "    collected: List[Dict] = []\n",
        "    for name in source_names:\n",
        "        if name in globals():\n",
        "            value = globals()[name]\n",
        "            if isinstance(value, list) and value:\n",
        "                collected.extend(value)\n",
        "    return collected\n",
        "\n",
        "\n",
        "def build_summary_table(result_sources: List[str]) -> Tuple[pd.DataFrame, List[Dict]]:\n",
        "    collected_results = gather_results(result_sources)\n",
        "    if not collected_results:\n",
        "        raise RuntimeError('No experiment results available. Run the training/sweep cells first.')\n",
        "\n",
        "    summary_rows = []\n",
        "    for res in collected_results:\n",
        "        cfg = res['config']\n",
        "        metrics = res['metrics']\n",
        "        summary_rows.append({\n",
        "            'run_name': res['run_name'],\n",
        "            'fold': res.get('fold'),\n",
        "            'rnn_type': cfg['rnn_type'],\n",
        "            'bidirectional': cfg.get('bidirectional', False),\n",
        "            'hidden_size': cfg['hidden_size'],\n",
        "            'num_layers': cfg['num_layers'],\n",
        "            'dropout': cfg['dropout'],\n",
        "            'lr': cfg['lr'],\n",
        "            'weight_decay': cfg.get('weight_decay', 0.0),\n",
        "            'use_class_weights': cfg.get('use_class_weights', False),\n",
        "            'use_focal_loss': cfg.get('use_focal_loss', False),\n",
        "            'label_smoothing': cfg.get('label_smoothing', 0.0),\n",
        "            'best_epoch': res['epoch'],\n",
        "            'best_f1': res['best_f1'],\n",
        "            'accuracy': metrics['accuracy'],\n",
        "            'precision': metrics['precision'],\n",
        "            'recall': metrics['recall'],\n",
        "            'checkpoint': str(res['checkpoint_path']),\n",
        "            'log_dir': str(LOG_DIR / res['run_name']),\n",
        "            'is_cv': res.get('fold') is not None,\n",
        "        })\n",
        "\n",
        "    summary_df = pd.DataFrame(summary_rows)\n",
        "    summary_df = summary_df.sort_values(by='best_f1', ascending=False).reset_index(drop=True)\n",
        "    return summary_df, collected_results\n",
        "\n",
        "\n",
        "RESULT_SOURCES = ['rnn_results', 'gru_results', 'lstm_results', 'sweep_results', 'cv_results', 'auto_cv_results']\n",
        "summary_table, all_results = build_summary_table(RESULT_SOURCES)\n",
        "summary_table\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "for _, row in summary_table.iterrows():\n",
        "    print(\n",
        "        f\"Run: {row['run_name']} | Model: {row['rnn_type'].upper()} | \"\n",
        "        f\"Bidirectional: {row['bidirectional']} | F1: {row['best_f1']:.4f}\"\n",
        "    )\n",
        "    print(f\"  Logs: {row['log_dir']}\")\n",
        "\n",
        "# Per TensorBoard combinato (eseguire su Colab / locale):\n",
        "# %tensorboard --logdir \"{LOG_DIR.as_posix()}\"\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cross-validation sul modello migliore\n",
        "if summary_table.empty:\n",
        "    raise RuntimeError('summary_table is empty; eseguire prima gli esperimenti.')\n",
        "\n",
        "BEST_CV_SPLITS = 5\n",
        "best_row = summary_table.iloc[0]\n",
        "best_cv_cfg = prepare_config(\n",
        "    f\"{best_row['run_name']}_CV\",\n",
        "    best_row['rnn_type'],\n",
        "    best_row['bidirectional'],\n",
        "    overrides={\n",
        "        'hidden_size': best_row['hidden_size'],\n",
        "        'num_layers': best_row['num_layers'],\n",
        "        'dropout': best_row['dropout'],\n",
        "        'lr': best_row['lr'],\n",
        "        'weight_decay': best_row['weight_decay'],\n",
        "        'use_class_weights': best_row['use_class_weights'],\n",
        "        'use_focal_loss': best_row['use_focal_loss'],\n",
        "        'label_smoothing': best_row['label_smoothing'],\n",
        "    },\n",
        ")\n",
        "cv_results.extend(run_cross_validation(best_cv_cfg, n_splits=BEST_CV_SPLITS))\n",
        "summary_table, all_results = build_summary_table(RESULT_SOURCES)\n",
        "cv_folds_df, cv_summary = summarize_cv_results(cv_results)\n",
        "cv_summary\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "ENABLE_AUTO_CV = False\n",
        "AUTO_CV_TOP_K = 2\n",
        "AUTO_CV_SPLITS = 5\n",
        "\n",
        "if ENABLE_AUTO_CV:\n",
        "    auto_cv_results = []\n",
        "    for _, row in summary_table.head(AUTO_CV_TOP_K).iterrows():\n",
        "        cfg = prepare_config(\n",
        "            f\"{row['run_name']}_AUTO_CV\",\n",
        "            row['rnn_type'],\n",
        "            row['bidirectional'],\n",
        "            overrides={\n",
        "                'hidden_size': row['hidden_size'],\n",
        "                'num_layers': row['num_layers'],\n",
        "                'dropout': row['dropout'],\n",
        "                'lr': row['lr'],\n",
        "                'weight_decay': row['weight_decay'],\n",
        "                'use_class_weights': row['use_class_weights'],\n",
        "                'use_focal_loss': row['use_focal_loss'],\n",
        "                'label_smoothing': row['label_smoothing'],\n",
        "            },\n",
        "        )\n",
        "        cv_out = run_cross_validation(cfg, n_splits=AUTO_CV_SPLITS)\n",
        "        auto_cv_results.extend(cv_out)\n",
        "\n",
        "    globals()['auto_cv_results'] = auto_cv_results\n",
        "    print(f\"Completed auto CV for top {min(AUTO_CV_TOP_K, len(summary_table))} runs.\")\n",
        "    RESULT_SOURCES = ['rnn_results', 'gru_results', 'lstm_results', 'sweep_results', 'cv_results', 'auto_cv_results']\n",
        "    summary_table, all_results = build_summary_table(RESULT_SOURCES)\n",
        "    summary_table\n",
        "else:\n",
        "    print('Auto CV disabled. Set ENABLE_AUTO_CV=True to enable automatic cross-validation.')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if 'all_results' not in globals() or not all_results:\n",
        "    raise RuntimeError('No experiment results available. Run the training/sweep cells first.')\n",
        "\n",
        "best_run = max(all_results, key=lambda x: x['best_f1'])\n",
        "best_model = best_run['model']\n",
        "best_history = best_run['history']\n",
        "print(\n",
        "    f\"Selected best run: {best_run['run_name']} | \"\n",
        "    f\"F1={best_run['best_f1']:.4f} | Accuracy={best_run['metrics']['accuracy']:.4f}\"\n",
        ")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def plot_history(history: Dict[str, List[float]], title: str = 'Learning Curves'):\n",
        "    epochs = range(1, len(history['train_loss']) + 1)\n",
        "    fig, axs = plt.subplots(1, 3, figsize=(18, 4))\n",
        "\n",
        "    axs[0].plot(epochs, history['train_loss'], label='Train')\n",
        "    axs[0].plot(epochs, history['valid_loss'], label='Valid')\n",
        "    axs[0].set_title('Loss')\n",
        "    axs[0].set_xlabel('Epoch')\n",
        "    axs[0].legend()\n",
        "\n",
        "    axs[1].plot(epochs, history['train_accuracy'], label='Train')\n",
        "    axs[1].plot(epochs, history['valid_accuracy'], label='Valid')\n",
        "    axs[1].set_title('Accuracy')\n",
        "    axs[1].set_xlabel('Epoch')\n",
        "    axs[1].legend()\n",
        "\n",
        "    axs[2].plot(epochs, history['train_f1'], label='Train F1')\n",
        "    axs[2].plot(epochs, history['valid_f1'], label='Valid F1')\n",
        "    axs[2].set_title('Macro F1')\n",
        "    axs[2].set_xlabel('Epoch')\n",
        "    axs[2].legend()\n",
        "\n",
        "    plt.suptitle(title)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "plot_history(best_history, title=f\"Learning Curves â€” {best_run['run_name']}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "best_preds = best_run['preds']\n",
        "best_targets = best_run['targets']\n",
        "print(f\"Best validation macro F1: {best_run['best_f1']:.3f}\")\n",
        "print(\n",
        "    classification_report(\n",
        "        best_targets,\n",
        "        best_preds,\n",
        "        target_names=[IDX2LABEL[i] for i in range(NUM_CLASSES)],\n",
        "    )\n",
        ")\n",
        "\n",
        "cf = confusion_matrix(best_targets, best_preds)\n",
        "plt.figure(figsize=(6, 5))\n",
        "sns.heatmap(\n",
        "    cf,\n",
        "    annot=True,\n",
        "    fmt='d',\n",
        "    cmap='Blues',\n",
        "    xticklabels=[IDX2LABEL[i] for i in range(NUM_CLASSES)],\n",
        "    yticklabels=[IDX2LABEL[i] for i in range(NUM_CLASSES)],\n",
        ")\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('True')\n",
        "plt.title(f\"Validation Confusion Matrix â€” {best_run['run_name']}\")\n",
        "plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "test_loader = DataLoader(TimeSeriesDataset(X_test_np), batch_size=256, shuffle=False)\n",
        "\n",
        "best_model.eval()\n",
        "test_preds = []\n",
        "with torch.no_grad():\n",
        "    for inputs in test_loader:\n",
        "        inputs = inputs.to(DEVICE)\n",
        "        logits = best_model(inputs)\n",
        "        test_preds.append(torch.argmax(logits, dim=1).cpu().numpy())\n",
        "\n",
        "test_preds = np.concatenate(test_preds)\n",
        "test_labels = [IDX2LABEL[idx] for idx in test_preds]\n",
        "\n",
        "submission = pd.DataFrame({\n",
        "    'sample_index': pd.unique(X_test_raw['sample_index']),\n",
        "    'label': test_labels,\n",
        "})\n",
        "submission_filename = OUTPUT_DIR / f\"submission_{best_run['run_name'].lower()}.csv\"\n",
        "submission.to_csv(submission_filename, index=False)\n",
        "print(f\"Saved submission to {submission_filename}\")\n",
        "submission.head()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%tensorboard --logdir \"/content/drive/MyDrive/[2025-2026]\\ AN2DL/Challenge/outputs/logs\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Next Steps\n",
        "\n",
        "- Espandere `EXPERIMENT_CONFIGS` con ricerche random/grid su hidden size, depth, dropout, learning rate e scheduler per automatizzare l'hyperparameter tuning.\n",
        "- Utilizzare `run_cross_validation` su piÃ¹ configurazioni e confrontare le metriche aggregate in `cv_summary`, esportando i risultati (CSV/LaTeX) per il report finale.\n",
        "- Monitorare tutti i run con `%tensorboard --logdir outputs/logs`, salvando screenshot delle curve principali e confrontando tempi/risorse.\n",
        "- Integrare tecniche di regularizzazione avanzate (label smoothing, mixup temporale, stochastic weight averaging) o layer di attention/pooling.\n",
        "- Costruire ensemble sui checkpoint migliori (media delle probabilitÃ  o voting) prima della submission Kaggle definitiva.\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
